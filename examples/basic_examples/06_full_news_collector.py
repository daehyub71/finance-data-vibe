"""
examples/basic_examples/06_full_news_collector.py (í’ˆì§ˆ ê²€ì¦ ê°•í™” ë²„ì „)

ì „ì²´ ì¢…ëª© ë‰´ìŠ¤ ìˆ˜ì§‘ê¸° - ë‰´ìŠ¤ í’ˆì§ˆ ê²€ì¦ ì‹œìŠ¤í…œ í†µí•©
âœ… ìµœê·¼ 4ì¼ê°„ ë‰´ìŠ¤ ìˆ˜ì§‘ (í‰ì¼ + ì£¼ë§ í¬í•¨)
âœ… ë„¤ì´ë²„ ë‰´ìŠ¤ API í™œìš©
âœ… ì¢…ëª©ëª… + "ì£¼ê°€", "ì‹¤ì ", "ì¬ë¬´" í‚¤ì›Œë“œ ì¡°í•©
âœ… ì™„ì „í•œ ë³¸ë¬¸ ë‚´ìš© ì¶”ì¶œ
ğŸ†• ë‰´ìŠ¤ í’ˆì§ˆ ê²€ì¦ ì‹œìŠ¤í…œ (ìŠ¤íŒ¸/ì¤‘ë³µ/ì˜¤ë¥˜ ìë™ í•„í„°ë§)
ğŸ†• ì‹ ë¢°ë„ ì ìˆ˜ ìë™ ê³„ì‚°
ğŸ†• í•œê¸€ ì¸ì½”ë”© ë¬¸ì œ ì™„ì „ í•´ê²°
"""

import sys
import os
from pathlib import Path

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
project_root = Path(__file__).parent.parent.parent
sys.path.append(str(project_root))

# í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
from dotenv import load_dotenv
load_dotenv(project_root / '.env')

import requests
import sqlite3
import pandas as pd
import time
from datetime import datetime, timedelta
import logging
from typing import List, Dict, Optional, Tuple
from concurrent.futures import ThreadPoolExecutor, as_completed
import json
import re
from bs4 import BeautifulSoup
from tqdm import tqdm
import threading
import difflib
from collections import Counter
import unicodedata

# ë¡œê¹… ì„¤ì • (í•œê¸€ ì¸ì½”ë”© ì™„ì „ í•´ê²°)
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(project_root / 'data' / 'news_collection.log', encoding='utf-8'),
        logging.StreamHandler(sys.stdout)
    ]
)
logger = logging.getLogger(__name__)

class NewsQualityValidator:
    """
    ë‰´ìŠ¤ í’ˆì§ˆ ê²€ì¦ ì‹œìŠ¤í…œ
    
    ìŠ¤íŒ¸, ì¤‘ë³µ, ì˜¤ë¥˜ê°€ ìˆëŠ” ë‰´ìŠ¤ë¥¼ ìë™ìœ¼ë¡œ í•„í„°ë§í•˜ê³ 
    ê° ë‰´ìŠ¤ì— ì‹ ë¢°ë„ ì ìˆ˜ë¥¼ ë¶€ì—¬í•©ë‹ˆë‹¤.
    """
    
    def __init__(self):
        # ìŠ¤íŒ¸ íŒ¨í„´ ì •ì˜
        self.spam_patterns = [
            r'í´ë¦­.*ì¡°íšŒ',
            r'ë¬´ë£Œ.*ìƒë‹´',
            r'ì§€ê¸ˆ.*ì‹ ì²­',
            r'100%.*ìˆ˜ìµ',
            r'ê¸‰ë“±.*í™•ì‹¤',
            r'ëŒ€ë°•.*ì¢…ëª©',
            r'ë¬´ì¡°ê±´.*ìƒìŠ¹',
            r'íˆ¬ì.*ë³´ì¥',
            r'ìˆ˜ìµë¥ .*\d+%.*ë³´ì¥',
            r'ë‹¨íƒ€.*ìˆ˜ìµ',
            r'ë¡œë˜.*ì¢…ëª©'
        ]
        
        # ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ë‰´ìŠ¤ ì†ŒìŠ¤
        self.trusted_sources = {
            'ì—°í•©ë‰´ìŠ¤': 95,
            'í•œêµ­ê²½ì œ': 90,
            'ë§¤ì¼ê²½ì œ': 88,
            'ì¡°ì„ ì¼ë³´': 85,
            'ì¤‘ì•™ì¼ë³´': 85,
            'ë™ì•„ì¼ë³´': 85,
            'ë¨¸ë‹ˆíˆ¬ë°ì´': 82,
            'ì „ìì‹ ë¬¸': 80,
            'ì„œìš¸ê²½ì œ': 78,
            'ì´ë°ì¼ë¦¬': 75
        }
        
        # ì˜ì‹¬ìŠ¤ëŸ¬ìš´ í‚¤ì›Œë“œ
        self.suspicious_keywords = [
            '2ë§Œ4900ì›', '5ë§Œì›', '10ë§Œì›',  # ëª…ë°±í•œ ì˜¤ë¥˜
            '999999', '000000',  # ë”ë¯¸ ë°ì´í„°
            'í…ŒìŠ¤íŠ¸', 'test', 'TEST',
            'ê´‘ê³ ', 'í™ë³´', 'í˜‘ì°¬',
            'ì´ë²¤íŠ¸', 'í”„ë¡œëª¨ì…˜'
        ]
        
        # ì¤‘ë³µ ê²€ì¶œìš© ìºì‹œ (ë©”ëª¨ë¦¬ íš¨ìœ¨ì„± ê³ ë ¤)
        self.content_hashes = set()
        self.title_cache = {}
        
        logger.info("âœ… ë‰´ìŠ¤ í’ˆì§ˆ ê²€ì¦ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ")
    
    def validate_news(self, news_data: Dict) -> Tuple[bool, int, List[str]]:
        """
        ë‰´ìŠ¤ í’ˆì§ˆ ì¢…í•© ê²€ì¦
        
        Args:
            news_data: ë‰´ìŠ¤ ë°ì´í„° ë”•ì…”ë„ˆë¦¬
            
        Returns:
            Tuple[is_valid, quality_score, issues]: 
            - is_valid: í†µê³¼ ì—¬ë¶€ (70ì  ì´ìƒ)
            - quality_score: ì‹ ë¢°ë„ ì ìˆ˜ (0-100)
            - issues: ë°œê²¬ëœ ë¬¸ì œì  ë¦¬ìŠ¤íŠ¸
        """
        issues = []
        score = 100  # ë§Œì ì—ì„œ ì‹œì‘í•´ì„œ ë¬¸ì œ ë°œê²¬ ì‹œ ê°ì 
        
        title = news_data.get('title', '')
        content = news_data.get('content', '')
        source = news_data.get('source', '')
        
        # 1. ìŠ¤íŒ¸ ê²€ì‚¬ (30ì  ê°ì )
        if self._is_spam_content(title, content):
            issues.append("ìŠ¤íŒ¸ íŒ¨í„´ ê°ì§€")
            score -= 30
        
        # 2. ì¤‘ë³µ ê²€ì‚¬ (25ì  ê°ì )
        if self._is_duplicate_content(title, content):
            issues.append("ì¤‘ë³µ ì½˜í…ì¸ ")
            score -= 25
        
        # 3. ì˜ì‹¬ìŠ¤ëŸ¬ìš´ í‚¤ì›Œë“œ ê²€ì‚¬ (20ì  ê°ì )
        if self._has_suspicious_keywords(title, content):
            issues.append("ì˜ì‹¬ìŠ¤ëŸ¬ìš´ í‚¤ì›Œë“œ í¬í•¨")
            score -= 20
        
        # 4. ì†ŒìŠ¤ ì‹ ë¢°ë„ ê²€ì‚¬ (ì ìˆ˜ ì¡°ì •)
        source_score = self._get_source_credibility(source)
        if source_score < 50:
            issues.append("ì‹ ë¢°ë„ ë‚®ì€ ì†ŒìŠ¤")
            score = min(score, source_score + 20)
        
        # 5. ì½˜í…ì¸  í’ˆì§ˆ ê²€ì‚¬ (15ì  ê°ì )
        content_quality = self._assess_content_quality(title, content)
        if content_quality < 70:
            issues.append("ì½˜í…ì¸  í’ˆì§ˆ ë¶€ì¡±")
            score -= 15
        
        # 6. ì¸ì½”ë”© ì˜¤ë¥˜ ê²€ì‚¬ (10ì  ê°ì )
        if self._has_encoding_issues(title, content):
            issues.append("ì¸ì½”ë”© ì˜¤ë¥˜")
            score -= 10
        
        # ìµœì¢… ì ìˆ˜ ë²”ìœ„ ì¡°ì •
        score = max(0, min(100, score))
        
        # 70ì  ì´ìƒë§Œ í†µê³¼
        is_valid = score >= 70 and len(issues) <= 2
        
        if not is_valid:
            logger.debug(f"ë‰´ìŠ¤ í’ˆì§ˆ ê²€ì¦ ì‹¤íŒ¨: {title[:30]}... (ì ìˆ˜: {score}, ë¬¸ì œ: {issues})")
        
        return is_valid, score, issues
    
    def _is_spam_content(self, title: str, content: str) -> bool:
        """ìŠ¤íŒ¸ íŒ¨í„´ ê²€ì‚¬"""
        text_combined = f"{title} {content}".lower()
        
        for pattern in self.spam_patterns:
            if re.search(pattern, text_combined):
                return True
        
        # ê³¼ë„í•œ íŠ¹ìˆ˜ë¬¸ì ì‚¬ìš© (ìŠ¤íŒ¸ íŠ¹ì§•)
        special_char_ratio = len(re.findall(r'[!@#$%^&*()+=\[\]{}|\\:";\'<>?,./]', text_combined)) / max(len(text_combined), 1)
        if special_char_ratio > 0.1:  # 10% ì´ìƒ
            return True
        
        # ê³¼ë„í•œ ìˆ«ì ì‚¬ìš©
        number_ratio = len(re.findall(r'\d', text_combined)) / max(len(text_combined), 1)
        if number_ratio > 0.3:  # 30% ì´ìƒ
            return True
        
        return False
    
    def _is_duplicate_content(self, title: str, content: str) -> bool:
        """ì¤‘ë³µ ì½˜í…ì¸  ê²€ì‚¬"""
        # ì œëª© ê¸°ë°˜ ìœ ì‚¬ë„ ê²€ì‚¬
        title_normalized = self._normalize_text(title)
        
        for cached_title in self.title_cache.keys():
            similarity = difflib.SequenceMatcher(None, title_normalized, cached_title).ratio()
            if similarity > 0.85:  # 85% ì´ìƒ ìœ ì‚¬í•˜ë©´ ì¤‘ë³µ
                return True
        
        # ìºì‹œì— ì¶”ê°€ (ìµœëŒ€ 1000ê°œê¹Œì§€ë§Œ ìœ ì§€)
        if len(self.title_cache) > 1000:
            # ì˜¤ë˜ëœ í•­ëª© ì ˆë°˜ ì‚­ì œ
            items = list(self.title_cache.items())
            self.title_cache = dict(items[500:])
        
        self.title_cache[title_normalized] = True
        
        # ë‚´ìš© í•´ì‹œ ê¸°ë°˜ ì¤‘ë³µ ê²€ì‚¬
        content_hash = hash(self._normalize_text(content))
        if content_hash in self.content_hashes:
            return True
        
        self.content_hashes.add(content_hash)
        
        # ë©”ëª¨ë¦¬ ê´€ë¦¬
        if len(self.content_hashes) > 5000:
            # ì ˆë°˜ ì‚­ì œ
            self.content_hashes = set(list(self.content_hashes)[2500:])
        
        return False
    
    def _has_suspicious_keywords(self, title: str, content: str) -> bool:
        """ì˜ì‹¬ìŠ¤ëŸ¬ìš´ í‚¤ì›Œë“œ ê²€ì‚¬"""
        text_combined = f"{title} {content}".lower()
        
        for keyword in self.suspicious_keywords:
            if keyword.lower() in text_combined:
                return True
        
        return False
    
    def _get_source_credibility(self, source: str) -> int:
        """ì†ŒìŠ¤ ì‹ ë¢°ë„ ì ìˆ˜ ë°˜í™˜"""
        for trusted_source, score in self.trusted_sources.items():
            if trusted_source in source:
                return score
        
        # ì•Œë ¤ì§€ì§€ ì•Šì€ ì†ŒìŠ¤ëŠ” ì¤‘ê°„ ì ìˆ˜
        return 60
    
    def _assess_content_quality(self, title: str, content: str) -> int:
        """ì½˜í…ì¸  í’ˆì§ˆ í‰ê°€"""
        quality_score = 100
        
        # ì œëª© ê¸¸ì´ ê²€ì‚¬
        if len(title) < 10 or len(title) > 200:
            quality_score -= 15
        
        # ë‚´ìš© ê¸¸ì´ ê²€ì‚¬
        if len(content) < 50:
            quality_score -= 20
        elif len(content) > 10000:
            quality_score -= 10
        
        # ë¬¸ì¥ êµ¬ì¡° ê²€ì‚¬
        sentences = re.split(r'[.!?]', content)
        if len(sentences) < 2:
            quality_score -= 15
        
        # ì˜ë¯¸ ìˆëŠ” ë‹¨ì–´ ë¹„ìœ¨
        words = re.findall(r'[ê°€-í£]+', content)
        if len(words) < 10:
            quality_score -= 20
        
        return max(0, quality_score)
    
    def _has_encoding_issues(self, title: str, content: str) -> bool:
        """ì¸ì½”ë”© ì˜¤ë¥˜ ê²€ì‚¬"""
        text_combined = f"{title} {content}"
        
        # ê¹¨ì§„ ë¬¸ì íŒ¨í„´
        broken_patterns = [
            r'[^\w\sê°€-í£ã„±-ã…ã…-ã…£.,!?()[\]{}:;"\'-]',  # ë¹„ì •ìƒ ë¬¸ì
            r'(?:[ï¿½ï¿½]{2,})',  # ì—°ì†ëœ ê¹¨ì§„ ë¬¸ì
            r'(?:&[a-zA-Z]+;){3,}',  # ê³¼ë„í•œ HTML ì—”í‹°í‹°
        ]
        
        for pattern in broken_patterns:
            if re.search(pattern, text_combined):
                return True
        
        return False
    
    def _normalize_text(self, text: str) -> str:
        """í…ìŠ¤íŠ¸ ì •ê·œí™”"""
        if not text:
            return ""
        
        # ìœ ë‹ˆì½”ë“œ ì •ê·œí™”
        text = unicodedata.normalize('NFKC', text)
        
        # ê³µë°± ì •ë¦¬
        text = re.sub(r'\s+', ' ', text).strip()
        
        # íŠ¹ìˆ˜ë¬¸ì ì œê±° (ë¹„êµìš©)
        text = re.sub(r'[^\w\sê°€-í£]', '', text)
        
        return text.lower()

class BusinessDayCalculator:
    """ì˜ì—…ì¼ ë° ì£¼ë§ í¬í•¨ ë‚ ì§œ ê³„ì‚°ê¸°"""
    
    @staticmethod
    def get_recent_news_days(days_count: int = 4) -> List[str]:
        """ìµœê·¼ ë‰´ìŠ¤ ìˆ˜ì§‘ ëŒ€ìƒì¼ ê³„ì‚° (í‰ì¼ + ì£¼ë§ í¬í•¨)"""
        news_days = []
        current_date = datetime.now()
        
        days_checked = 0
        while len(news_days) < days_count and days_checked < 10:
            current_date -= timedelta(days=1)
            days_checked += 1
            
            # ëª¨ë“  ìš”ì¼ í¬í•¨ (ì›”~ì¼)
            news_days.append(current_date.strftime('%Y-%m-%d'))
                
        logger.info(f"[ë‰´ìŠ¤ìˆ˜ì§‘ì¼] ìµœê·¼ {days_count}ì¼: {', '.join(news_days)}")
        return news_days

class NewsAPIManager:
    """ë„¤ì´ë²„ ë‰´ìŠ¤ API ê´€ë¦¬ì"""
    
    def __init__(self, client_id: str, client_secret: str):
        self.client_id = client_id
        self.client_secret = client_secret
        self.base_url = "https://openapi.naver.com/v1/search/news.json"
        self.headers = {
            'X-Naver-Client-Id': self.client_id,
            'X-Naver-Client-Secret': self.client_secret,
            'User-Agent': 'FinanceDataVibe/1.0'
        }
        
        # API í˜¸ì¶œ ì œí•œ ê´€ë¦¬
        self.api_calls_today = 0
        self.max_calls_per_day = 23000  # ì—¬ìœ ë¶„ 2000íšŒ
        self.last_call_time = time.time()
        self.min_interval = 0.12  # ì´ˆë‹¹ 8íšŒ ì œí•œ (ì•ˆì „í•˜ê²Œ)
        self.lock = threading.Lock()
        
    def rate_limit_check(self) -> bool:
        """API í˜¸ì¶œ ì œí•œ í™•ì¸"""
        with self.lock:
            current_time = time.time()
            
            if self.api_calls_today >= self.max_calls_per_day:
                logger.warning(f"âš ï¸ ì¼ì¼ API í˜¸ì¶œ ì œí•œ ë„ë‹¬: {self.api_calls_today:,}")
                return False
            
            time_since_last_call = current_time - self.last_call_time
            if time_since_last_call < self.min_interval:
                sleep_time = self.min_interval - time_since_last_call
                time.sleep(sleep_time)
            
            self.last_call_time = time.time()
            self.api_calls_today += 1
            
            return True
    
    def search_news(self, query: str, display: int = 100, sort: str = 'date') -> List[Dict]:
        """ë‰´ìŠ¤ ê²€ìƒ‰"""
        if not self.rate_limit_check():
            return []
        
        params = {
            'query': query,
            'display': min(display, 100),
            'sort': sort
        }
        
        try:
            response = requests.get(self.base_url, headers=self.headers, params=params, timeout=10)
            response.raise_for_status()
            
            data = response.json()
            items = data.get('items', [])
            
            # ìµœê·¼ ë‰´ìŠ¤ë§Œ í•„í„°ë§
            recent_items = self._filter_recent_news(items)
            
            return recent_items
            
        except requests.exceptions.RequestException as e:
            logger.error(f"âŒ ë‰´ìŠ¤ ê²€ìƒ‰ API ì˜¤ë¥˜ - ê²€ìƒ‰ì–´: {query}, ì˜¤ë¥˜: {e}")
            return []
        except Exception as e:
            logger.error(f"âŒ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜ - ê²€ìƒ‰ì–´: {query}, ì˜¤ë¥˜: {e}")
            return []
    
    def _filter_recent_news(self, items: List[Dict]) -> List[Dict]:
        """ìµœê·¼ 4ì¼ê°„ ë‰´ìŠ¤ë§Œ í•„í„°ë§"""
        news_days = BusinessDayCalculator.get_recent_news_days(4)
        recent_items = []
        
        for item in items:
            try:
                pub_date_str = item.get('pubDate', '')
                pub_date = self._parse_date(pub_date_str)
                
                if pub_date:
                    pub_date_formatted = pub_date.strftime('%Y-%m-%d')
                    if pub_date_formatted in news_days:
                        recent_items.append(item)
            except:
                continue
        
        return recent_items
    
    def _parse_date(self, date_str: str) -> Optional[datetime]:
        """ë‚ ì§œ ë¬¸ìì—´ì„ datetimeìœ¼ë¡œ ë³€í™˜"""
        try:
            dt = datetime.strptime(date_str, "%a, %d %b %Y %H:%M:%S %z")
            return dt.replace(tzinfo=None)
        except:
            return None

class EnhancedNewsContentExtractor:
    """ê°•í™”ëœ ë‰´ìŠ¤ ë³¸ë¬¸ ì¶”ì¶œê¸° (í’ˆì§ˆ ê²€ì¦ í†µí•©)"""
    
    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'ko-KR,ko;q=0.8,en-US;q=0.5,en;q=0.3',
            'Accept-Encoding': 'gzip, deflate, br',
            'DNT': '1',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1'
        })
    
    def extract_content(self, url: str) -> str:
        """ë‰´ìŠ¤ ê¸°ì‚¬ ë³¸ë¬¸ ì¶”ì¶œ (ê°•í™”ëœ ì •ì œ ê¸°ëŠ¥)"""
        try:
            response = self.session.get(url, timeout=15)
            response.raise_for_status()
            
            # ì¸ì½”ë”© ìë™ ê°ì§€ ë° ì„¤ì •
            response.encoding = response.apparent_encoding
            
            soup = BeautifulSoup(response.text, 'html.parser', from_encoding='utf-8')
            content = ""
            
            # ë„¤ì´ë²„ ë‰´ìŠ¤ ë³¸ë¬¸ ì¶”ì¶œ
            if 'news.naver.com' in url:
                content = self._extract_naver_content(soup)
            
            # ë‹¤ë¥¸ ë‰´ìŠ¤ ì‚¬ì´íŠ¸ ë³¸ë¬¸ ì¶”ì¶œ
            if not content:
                content = self._extract_general_content(soup)
            
            # ê°•í™”ëœ í…ìŠ¤íŠ¸ ì •ì œ
            content = self._advanced_text_cleaning(content)
            
            return content[:3000] if content else ""
            
        except Exception as e:
            logger.debug(f"ë³¸ë¬¸ ì¶”ì¶œ ì‹¤íŒ¨ - {url}: {e}")
            return ""
    
    def _extract_naver_content(self, soup: BeautifulSoup) -> str:
        """ë„¤ì´ë²„ ë‰´ìŠ¤ ë³¸ë¬¸ ì¶”ì¶œ"""
        selectors = [
            'div#newsct_article',
            'div.newsct_article', 
            'div#articleBodyContents',
            'div.article_body',
            'div.news_end',
            'div._article_body_contents'
        ]
        
        for selector in selectors:
            content_div = soup.select_one(selector)
            if content_div:
                # ë¶ˆí•„ìš”í•œ ìš”ì†Œ ì œê±°
                for elem in content_div.find_all(['script', 'style', 'ins', 'iframe', 'aside', 'nav', 'footer']):
                    elem.decompose()
                
                # ê´‘ê³ , ê´€ë ¨ê¸°ì‚¬ ë“± ì œê±°
                for elem in content_div.find_all(class_=re.compile(r'(ad|advertisement|related|recommend)')):
                    elem.decompose()
                
                text = content_div.get_text(separator=' ', strip=True)
                
                if len(text) > 100:
                    return text
        
        return ""
    
    def _extract_general_content(self, soup: BeautifulSoup) -> str:
        """ì¼ë°˜ ë‰´ìŠ¤ ì‚¬ì´íŠ¸ ë³¸ë¬¸ ì¶”ì¶œ"""
        selectors = [
            'div.article-content',
            'div.news-content',
            'div.content',
            'article',
            'div.post-content',
            'div.article_txt',
            'div.article-body',
            'div.news-article-content',
            'div.article-view-content'
        ]
        
        for selector in selectors:
            content = soup.select_one(selector)
            if content:
                for elem in content.find_all(['script', 'style', 'ins', 'iframe', 'nav', 'footer']):
                    elem.decompose()
                
                text = content.get_text(separator=' ', strip=True)
                
                if len(text) > 100:
                    return text
        
        # ë§ˆì§€ë§‰ ì‹œë„: ëª¨ë“  p íƒœê·¸
        paragraphs = soup.find_all('p')
        if paragraphs:
            text = ' '.join([p.get_text(strip=True) for p in paragraphs])
            if len(text) > 100:
                return text
        
        return ""
    
    def _advanced_text_cleaning(self, text: str) -> str:
        """ê°•í™”ëœ í…ìŠ¤íŠ¸ ì •ì œ (í•œê¸€ ì¤‘ë³µ ë° ì¸ì½”ë”© ë¬¸ì œ ì™„ì „ í•´ê²°)"""
        
        if not text:
            return ""
        
        # 1. ìœ ë‹ˆì½”ë“œ ì •ê·œí™”
        text = unicodedata.normalize('NFKC', text)
        
        # 2. HTML íƒœê·¸ ë° ì—”í‹°í‹° ì œê±°
        text = re.sub(r'<[^>]+>', ' ', text)
        text = re.sub(r'&[a-zA-Z0-9#]+;', ' ', text)
        
        # 3. ë¶ˆí•„ìš”í•œ ë¬¸êµ¬ ì œê±° (ê°•í™”)
        patterns_to_remove = [
            r'// flash ì˜¤ë¥˜ë¥¼ ìš°íšŒí•˜ê¸° ìœ„í•œ í•¨ìˆ˜ ì¶”ê°€.*',
            r'ë³¸\s*ê¸°ì‚¬ëŠ”.*?ì…ë‹ˆë‹¤',
            r'ì €ì‘ê¶Œì.*?ë¬´ë‹¨.*?ê¸ˆì§€',
            r'â“’.*?ë¬´ë‹¨.*?ê¸ˆì§€',
            r'Copyright.*?All.*?rights.*?reserved',
            r'ê¸°ì\s*=.*?ê¸°ì',
            r'^\s*\[.*?\]\s*',
            r'\s*\[.*?\]\s*$',
            r'ì´\s*ë©”ì¼.*?ë³´ë‚´ê¸°',
            r'ì¹´ì¹´ì˜¤í†¡.*?ê³µìœ ',
            r'í˜ì´ìŠ¤ë¶.*?ê³µìœ ',
            r'íŠ¸ìœ„í„°.*?ê³µìœ ',
            r'ë¬´ë‹¨ì „ì¬.*?ê¸ˆì§€',
            r'ë„¤ì´ë²„.*?ë¸”ë¡œê·¸',
            r'ê´€ë ¨.*?ë‰´ìŠ¤',
            r'ì´ì „.*?ê¸°ì‚¬',
            r'ë‹¤ìŒ.*?ê¸°ì‚¬',
            r'.*?êµ¬ë….*?ì•Œë¦¼',
            r'.*?íŒ”ë¡œìš°.*?',
            r'ê´‘ê³ .*?ë¬¸ì˜',
            r'ì œë³´.*?tip'
        ]
        
        for pattern in patterns_to_remove:
            text = re.sub(pattern, '', text, flags=re.IGNORECASE | re.DOTALL)
        
        # 4. íŠ¹ìˆ˜ ë¬¸ì ì •ë¦¬
        text = re.sub(r'[&\[\]{}()\*\+\?\|\^\$\\.~`!@#%=:;",<>]', ' ', text)
        
        # 5. ìˆ«ìì™€ í•œê¸€ ì‚¬ì´ ê³µë°±
        text = re.sub(r'(\d)([ê°€-í£])', r'\1 \2', text)
        text = re.sub(r'([ê°€-í£])(\d)', r'\1 \2', text)
        
        # 6. ì¤‘ë³µ ì œê±° (í•µì‹¬ ê°œì„ !)
        words = text.split()
        cleaned_words = []
        prev_word = ""
        
        for word in words:
            # ì—°ì†ëœ ê°™ì€ ë‹¨ì–´ ì œê±°
            if word != prev_word and len(word) > 0:
                cleaned_words.append(word)
            prev_word = word
        
        text = ' '.join(cleaned_words)
        
        # 7. ì¤‘ë³µ íŒ¨í„´ ì œê±° (ì •ê·œí‘œí˜„ì‹)
        text = re.sub(r'([ê°€-í£A-Za-z0-9]{2,})\1+', r'\1', text)
        
        # 8. ë°˜ë³µ êµ¬ë¬¸ ì œê±°
        def remove_repeating_patterns(text):
            for length in range(3, 15):
                pattern = f'(.{{{length}}})(\\1)+'
                text = re.sub(pattern, r'\1', text)
            return text
        
        text = remove_repeating_patterns(text)
        
        # 9. ì—¬ëŸ¬ ê³µë°±ì„ í•˜ë‚˜ë¡œ
        text = re.sub(r'\s+', ' ', text)
        
        # 10. ìµœì¢… ì •ë¦¬
        text = text.strip()
        
        return text

class StockNewsCollector:
    """ì£¼ì‹ ë‰´ìŠ¤ ìˆ˜ì§‘ê¸° ë©”ì¸ í´ë˜ìŠ¤ (í’ˆì§ˆ ê²€ì¦ í†µí•©)"""
    
    def __init__(self, client_id: str, client_secret: str):
        self.api_manager = NewsAPIManager(client_id, client_secret)
        self.content_extractor = EnhancedNewsContentExtractor()
        self.quality_validator = NewsQualityValidator()  # ğŸ†• í’ˆì§ˆ ê²€ì¦ ì‹œìŠ¤í…œ
        self.db_path = project_root / "finance_data.db"
        self.init_database()
        
        # í’ˆì§ˆ í†µê³„
        self.quality_stats = {
            'total_processed': 0,
            'quality_passed': 0,
            'quality_failed': 0,
            'spam_filtered': 0,
            'duplicate_filtered': 0,
            'low_quality_filtered': 0
        }
    
    def init_database(self):
        """ë°ì´í„°ë² ì´ìŠ¤ í…Œì´ë¸” ì´ˆê¸°í™” (í’ˆì§ˆ ê´€ë ¨ ì»¬ëŸ¼ ì¶”ê°€)"""
        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.cursor()
            
            # ê¸°ì¡´ í…Œì´ë¸” í™•ì¸
            cursor.execute("SELECT name FROM sqlite_master WHERE type='table'")
            existing_tables = [row[0] for row in cursor.fetchall()]
            
            # stock_info í…Œì´ë¸”ì´ ì—†ìœ¼ë©´ ìƒì„±
            if 'stock_info' not in existing_tables:
                logger.info("stock_info í…Œì´ë¸”ì„ ìƒì„±í•©ë‹ˆë‹¤...")
                cursor.execute('''
                    CREATE TABLE stock_info (
                        code TEXT PRIMARY KEY,
                        name TEXT NOT NULL,
                        market TEXT,
                        sector TEXT,
                        created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                    )
                ''')
                
                # ê¸°ë³¸ ì¢…ëª© ë°ì´í„° ì‚½ì…
                basic_stocks = [
                    ('005930', 'ì‚¼ì„±ì „ì', 'KOSPI', 'IT'),
                    ('000660', 'SKí•˜ì´ë‹‰ìŠ¤', 'KOSPI', 'IT'),
                    ('035420', 'NAVER', 'KOSPI', 'IT'),
                    ('005380', 'í˜„ëŒ€ì°¨', 'KOSPI', 'ìë™ì°¨'),
                    ('006400', 'ì‚¼ì„±SDI', 'KOSPI', 'í™”í•™'),
                    ('051910', 'LGí™”í•™', 'KOSPI', 'í™”í•™'),
                    ('096770', 'SKì´ë…¸ë² ì´ì…˜', 'KOSPI', 'í™”í•™'),
                    ('034730', 'SK', 'KOSPI', 'ì§€ì£¼íšŒì‚¬'),
                    ('003550', 'LG', 'KOSPI', 'ì§€ì£¼íšŒì‚¬'),
                    ('012330', 'í˜„ëŒ€ëª¨ë¹„ìŠ¤', 'KOSPI', 'ìë™ì°¨ë¶€í’ˆ'),
                    ('207940', 'ì‚¼ì„±ë°”ì´ì˜¤ë¡œì§ìŠ¤', 'KOSPI', 'ë°”ì´ì˜¤'),
                    ('373220', 'LGì—ë„ˆì§€ì†”ë£¨ì…˜', 'KOSPI', 'í™”í•™'),
                    ('000270', 'ê¸°ì•„', 'KOSPI', 'ìë™ì°¨'),
                    ('068270', 'ì…€íŠ¸ë¦¬ì˜¨', 'KOSPI', 'ë°”ì´ì˜¤'),
                    ('035720', 'ì¹´ì¹´ì˜¤', 'KOSPI', 'IT'),
                    ('018260', 'ì‚¼ì„±ì—ìŠ¤ë””ì—ìŠ¤', 'KOSPI', 'IT'),
                    ('036570', 'ì—”ì”¨ì†Œí”„íŠ¸', 'KOSPI', 'IT'),
                    ('066570', 'LGì „ì', 'KOSPI', 'ì „ì'),
                    ('105560', 'KBê¸ˆìœµ', 'KOSPI', 'ê¸ˆìœµ'),
                    ('055550', 'ì‹ í•œì§€ì£¼', 'KOSPI', 'ê¸ˆìœµ')
                ]
                
                for stock in basic_stocks:
                    cursor.execute('''
                        INSERT OR IGNORE INTO stock_info (code, name, market, sector)
                        VALUES (?, ?, ?, ?)
                    ''', stock)
                
                logger.info(f"{len(basic_stocks)}ê°œ ê¸°ë³¸ ì¢…ëª© ë°ì´í„° ìƒì„± ì™„ë£Œ")
            
            # ğŸ†• ê°•í™”ëœ news_articles í…Œì´ë¸” ìƒì„±
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS news_articles (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    stock_code TEXT NOT NULL,
                    stock_name TEXT NOT NULL,
                    title TEXT NOT NULL,
                    link TEXT NOT NULL UNIQUE,
                    description TEXT,
                    content TEXT,
                    pub_date TEXT,
                    source TEXT,
                    quality_score INTEGER DEFAULT 0,
                    quality_issues TEXT,
                    is_verified BOOLEAN DEFAULT 0,
                    sentiment_score REAL DEFAULT 0.0,
                    collected_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                )
            ''')
            
            # ì¸ë±ìŠ¤ ìƒì„±
            cursor.execute('CREATE INDEX IF NOT EXISTS idx_news_stock_code ON news_articles(stock_code)')
            cursor.execute('CREATE INDEX IF NOT EXISTS idx_news_quality_score ON news_articles(quality_score)')
            cursor.execute('CREATE INDEX IF NOT EXISTS idx_news_is_verified ON news_articles(is_verified)')
            cursor.execute('CREATE INDEX IF NOT EXISTS idx_news_collected_at ON news_articles(collected_at)')
            
            conn.commit()
    
    def get_all_stocks(self) -> List[Dict[str, str]]:
        """ì „ì²´ ì£¼ì‹ ì¢…ëª© ì¡°íšŒ"""
        with sqlite3.connect(self.db_path) as conn:
            df = pd.read_sql_query("""
                SELECT code, name 
                FROM stock_info 
                WHERE name NOT LIKE '%ìŠ¤íŒ©%'
                AND name NOT LIKE '%ë¦¬ì¸ %'
                AND name NOT LIKE '%ETF%'
                ORDER BY code
            """, conn)
            
        return df.to_dict('records')
    
    def get_existing_links_today(self) -> set:
        """ì˜¤ëŠ˜ ìˆ˜ì§‘ëœ ë‰´ìŠ¤ ë§í¬ë“¤ (ì¤‘ë³µ ë°©ì§€)"""
        with sqlite3.connect(self.db_path) as conn:
            df = pd.read_sql_query("""
                SELECT DISTINCT link 
                FROM news_articles 
                WHERE DATE(collected_at) = DATE('now')
            """, conn)
            
        return set(df['link'].tolist()) if not df.empty else set()
    
    def collect_stock_news(self, stock: Dict[str, str]) -> List[Dict]:
        """íŠ¹ì • ì¢…ëª©ì˜ ë‰´ìŠ¤ ìˆ˜ì§‘ (í’ˆì§ˆ ê²€ì¦ í†µí•©)"""
        stock_code = stock['code']
        stock_name = stock['name']
        
        collected_news = []
        existing_links = self.get_existing_links_today()
        
        # ê²€ìƒ‰ ì „ëµ: ì¢…ëª©ëª… + í‚¤ì›Œë“œ ì¡°í•©
        search_strategies = [
            stock_name,
            f"{stock_name} ì£¼ê°€",
            f"{stock_name} ì‹¤ì ",
            f"{stock_name} ì¬ë¬´"
        ]
        
        max_news_per_query = 30
        max_total_news = 50
        
        for query in search_strategies:
            if len(collected_news) >= max_total_news:
                break
            
            if self.api_manager.api_calls_today >= self.api_manager.max_calls_per_day:
                logger.warning("[ê²½ê³ ] API í˜¸ì¶œ ì œí•œ ë„ë‹¬, ìˆ˜ì§‘ ì¤‘ë‹¨")
                break
            
            news_items = self.api_manager.search_news(query, display=max_news_per_query)
            
            for item in news_items:
                if len(collected_news) >= max_total_news:
                    break
                
                if item['link'] in existing_links:
                    continue
                
                # ì¢…ëª© ê´€ë ¨ì„± ì²´í¬
                title = re.sub(r'<[^>]+>', '', item['title'])
                description = re.sub(r'<[^>]+>', '', item['description'])
                
                if self._is_relevant_news(title, description, stock_name, stock_code):
                    # ë³¸ë¬¸ ìˆ˜ì§‘
                    content = self.content_extractor.extract_content(item['link'])
                    
                    news_data = {
                        'stock_code': stock_code,
                        'stock_name': stock_name,
                        'title': title,
                        'link': item['link'],
                        'description': description,
                        'content': content,
                        'pub_date': item['pubDate'],
                        'source': self._extract_source(item.get('originallink', item['link']))
                    }
                    
                    # ğŸ†• ë‰´ìŠ¤ í’ˆì§ˆ ê²€ì¦
                    self.quality_stats['total_processed'] += 1
                    is_valid, quality_score, issues = self.quality_validator.validate_news(news_data)
                    
                    if is_valid:
                        news_data['quality_score'] = quality_score
                        news_data['quality_issues'] = ', '.join(issues) if issues else ''
                        news_data['is_verified'] = True
                        
                        collected_news.append(news_data)
                        existing_links.add(item['link'])
                        self.quality_stats['quality_passed'] += 1
                    else:
                        self.quality_stats['quality_failed'] += 1
                        
                        # ì‹¤íŒ¨ ì‚¬ìœ ë³„ í†µê³„ ì—…ë°ì´íŠ¸
                        if 'ìŠ¤íŒ¸ íŒ¨í„´ ê°ì§€' in issues:
                            self.quality_stats['spam_filtered'] += 1
                        if 'ì¤‘ë³µ ì½˜í…ì¸ ' in issues:
                            self.quality_stats['duplicate_filtered'] += 1
                        if 'ì½˜í…ì¸  í’ˆì§ˆ ë¶€ì¡±' in issues:
                            self.quality_stats['low_quality_filtered'] += 1
            
            time.sleep(0.1)
        
        if collected_news:
            quality_passed = len(collected_news)
            total_processed = self.quality_stats['total_processed']
            pass_rate = (quality_passed / max(total_processed, 1)) * 100
            
            logger.info(f"[ìˆ˜ì§‘ì™„ë£Œ] {stock_name}: {quality_passed}ê°œ ê³ í’ˆì§ˆ ë‰´ìŠ¤ ìˆ˜ì§‘ (í’ˆì§ˆ í†µê³¼ìœ¨: {pass_rate:.1f}%)")
        
        return collected_news
    
    def _is_relevant_news(self, title: str, description: str, stock_name: str, stock_code: str) -> bool:
        """ë‰´ìŠ¤ì˜ ì¢…ëª© ê´€ë ¨ì„± ì²´í¬"""
        # ì¢…ëª©ëª… ì§ì ‘ í¬í•¨
        if stock_name in title or stock_code in title:
            return True
        
        # ì„¤ëª…ì— ì¢…ëª©ëª… í¬í•¨
        if stock_name in description:
            return True
        
        # ì£¼ì‹ ê´€ë ¨ í‚¤ì›Œë“œ + ì¢…ëª©ëª… ì¼ë¶€
        stock_keywords = ['ì£¼ê°€', 'ì‹¤ì ', 'ì¬ë¬´', 'ë§¤ì¶œ', 'ì˜ì—…ì´ìµ', 'íˆ¬ì', 'ìƒì¥', 'ê³µì‹œ', 'ë°°ë‹¹']
        text_combined = f"{title} {description}".lower()
        
        if any(keyword in text_combined for keyword in stock_keywords):
            name_parts = stock_name.split()
            if any(part in text_combined for part in name_parts if len(part) > 1):
                return True
        
        return False
    
    def _extract_source(self, url: str) -> str:
        """ë‰´ìŠ¤ ì†ŒìŠ¤ ì¶”ì¶œ"""
        if not url:
            return 'Unknown'
        
        source_mapping = {
            'chosun.com': 'ì¡°ì„ ì¼ë³´',
            'donga.com': 'ë™ì•„ì¼ë³´',
            'joins.com': 'ì¤‘ì•™ì¼ë³´',
            'mk.co.kr': 'ë§¤ì¼ê²½ì œ',
            'hankyung.com': 'í•œêµ­ê²½ì œ',
            'yonhapnews.co.kr': 'ì—°í•©ë‰´ìŠ¤',
            'mt.co.kr': 'ë¨¸ë‹ˆíˆ¬ë°ì´',
            'etnews.com': 'ì „ìì‹ ë¬¸',
            'sedaily.com': 'ì„œìš¸ê²½ì œ',
            'edaily.co.kr': 'ì´ë°ì¼ë¦¬'
        }
        
        for domain, source in source_mapping.items():
            if domain in url:
                return source
        
        try:
            domain_parts = url.split('//')[1].split('/')[0].split('.')
            return domain_parts[-2] if len(domain_parts) > 1 else 'Unknown'
        except:
            return 'Unknown'
    
    def save_news_batch(self, news_list: List[Dict]) -> int:
        """ë‰´ìŠ¤ ë°°ì¹˜ ì €ì¥ (í’ˆì§ˆ ì •ë³´ í¬í•¨)"""
        if not news_list:
            return 0
        
        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.cursor()
            
            saved_count = 0
            for news in news_list:
                try:
                    cursor.execute('''
                        INSERT OR IGNORE INTO news_articles 
                        (stock_code, stock_name, title, link, description, content, pub_date, source,
                         quality_score, quality_issues, is_verified)
                        VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                    ''', (
                        news['stock_code'],
                        news['stock_name'],
                        news['title'],
                        news['link'],
                        news['description'],
                        news['content'],
                        news['pub_date'],
                        news['source'],
                        news.get('quality_score', 0),
                        news.get('quality_issues', ''),
                        news.get('is_verified', True)
                    ))
                    
                    if cursor.rowcount > 0:
                        saved_count += 1
                        
                except sqlite3.Error as e:
                    logger.error(f"ì €ì¥ ì‹¤íŒ¨ - {news['title']}: {e}")
            
            conn.commit()
            
        return saved_count
    
    def collect_all_stocks_news(self, max_workers: int = 3, batch_size: int = 20, test_mode: bool = False):
        """ì „ì²´ ì¢…ëª© ë‰´ìŠ¤ ìˆ˜ì§‘ (í’ˆì§ˆ ê²€ì¦ í†µí•©)"""
        
        news_days = BusinessDayCalculator.get_recent_news_days(4)
        stocks = self.get_all_stocks()
        
        if test_mode:
            stocks = stocks[:20]
            logger.info(f"[í…ŒìŠ¤íŠ¸ëª¨ë“œ] {len(stocks)}ê°œ ì¢…ëª©ìœ¼ë¡œ ì œí•œ")
        
        logger.info(f"[ì‹œì‘] ì´ {len(stocks)}ê°œ ì¢…ëª© ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹œì‘ (í’ˆì§ˆ ê²€ì¦ í™œì„±í™”)")
        logger.info(f"[ìˆ˜ì§‘ê¸°ê°„] ìµœê·¼ 4ì¼ê°„ ë‰´ìŠ¤ ìˆ˜ì§‘ (í‰ì¼ + ì£¼ë§ í¬í•¨)")
        
        total_collected = 0
        total_saved = 0
        
        with tqdm(total=len(stocks), desc="ê³ í’ˆì§ˆ ë‰´ìŠ¤ ìˆ˜ì§‘", unit="ì¢…ëª©") as pbar:
            
            for i in range(0, len(stocks), batch_size):
                batch = stocks[i:i + batch_size]
                batch_news = []
                
                if self.api_manager.api_calls_today >= self.api_manager.max_calls_per_day:
                    logger.warning("[ê²½ê³ ] ì¼ì¼ API í˜¸ì¶œ ì œí•œ ë„ë‹¬, ìˆ˜ì§‘ ì¤‘ë‹¨")
                    break
                
                logger.info(f"[ë°°ì¹˜ì²˜ë¦¬] ë°°ì¹˜ {i//batch_size + 1}/{(len(stocks)-1)//batch_size + 1} ì²˜ë¦¬ ì¤‘...")
                
                with ThreadPoolExecutor(max_workers=max_workers) as executor:
                    future_to_stock = {
                        executor.submit(self.collect_stock_news, stock): stock
                        for stock in batch
                    }
                    
                    for future in as_completed(future_to_stock):
                        stock = future_to_stock[future]
                        try:
                            news_list = future.result()
                            if news_list:
                                batch_news.extend(news_list)
                                total_collected += len(news_list)
                            
                            # í’ˆì§ˆ í†µê³„ ì—…ë°ì´íŠ¸
                            quality_rate = (self.quality_stats['quality_passed'] / 
                                          max(self.quality_stats['total_processed'], 1)) * 100
                            
                            pbar.set_postfix({
                                'APIí˜¸ì¶œ': f"{self.api_manager.api_calls_today:,}",
                                'ê³ í’ˆì§ˆ': f"{total_collected:,}",
                                'í’ˆì§ˆë¥ ': f"{quality_rate:.1f}%"
                            })
                            
                        except Exception as e:
                            logger.error(f"[ì˜¤ë¥˜] {stock['name']} ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                        
                        pbar.update(1)
                
                # ë°°ì¹˜ ì €ì¥
                if batch_news:
                    saved_count = self.save_news_batch(batch_news)
                    total_saved += saved_count
                    logger.info(f"[ë°°ì¹˜ì €ì¥] ë°°ì¹˜ ì €ì¥: {len(batch_news)}ê°œ ìˆ˜ì§‘ -> {saved_count}ê°œ ì‹ ê·œ ì €ì¥")
                
                # ë°°ì¹˜ ê°„ ëŒ€ê¸°
                if i + batch_size < len(stocks):
                    time.sleep(10)
        
        logger.info(f"[ì™„ë£Œ] ì „ì²´ ìˆ˜ì§‘ ì™„ë£Œ!")
        logger.info(f"[ê²°ê³¼] ìµœì¢… ê²°ê³¼: {total_collected:,}ê°œ ê³ í’ˆì§ˆ ë‰´ìŠ¤ ìˆ˜ì§‘, {total_saved:,}ê°œ ì €ì¥")
        logger.info(f"[APIí˜¸ì¶œ] API í˜¸ì¶œ ìˆ˜: {self.api_manager.api_calls_today:,}")
        
        self.print_collection_summary()
        self.print_quality_summary()  # ğŸ†• í’ˆì§ˆ ìš”ì•½ ì¶œë ¥
    
    def print_quality_summary(self):
        """ğŸ†• í’ˆì§ˆ ê²€ì¦ ê²°ê³¼ ìš”ì•½ ì¶œë ¥"""
        stats = self.quality_stats
        
        total_processed = stats['total_processed']
        if total_processed == 0:
            return
        
        quality_pass_rate = (stats['quality_passed'] / total_processed) * 100
        
        print(f"\n[í’ˆì§ˆê²€ì¦] ë‰´ìŠ¤ í’ˆì§ˆ ê²€ì¦ ê²°ê³¼:")
        print(f"  â€¢ ì´ ì²˜ë¦¬: {total_processed:,}ê°œ")
        print(f"  â€¢ í’ˆì§ˆ í†µê³¼: {stats['quality_passed']:,}ê°œ ({quality_pass_rate:.1f}%)")
        print(f"  â€¢ í’ˆì§ˆ ì‹¤íŒ¨: {stats['quality_failed']:,}ê°œ")
        print(f"\n[í•„í„°ë§] ì œê±°ëœ ë‰´ìŠ¤ ìœ í˜•:")
        print(f"  â€¢ ìŠ¤íŒ¸ ë‰´ìŠ¤: {stats['spam_filtered']:,}ê°œ")
        print(f"  â€¢ ì¤‘ë³µ ë‰´ìŠ¤: {stats['duplicate_filtered']:,}ê°œ")
        print(f"  â€¢ ì €í’ˆì§ˆ ë‰´ìŠ¤: {stats['low_quality_filtered']:,}ê°œ")
    
    def print_collection_summary(self):
        """ìˆ˜ì§‘ ê²°ê³¼ ìš”ì•½ ì¶œë ¥"""
        with sqlite3.connect(self.db_path) as conn:
            # ì˜¤ëŠ˜ ìˆ˜ì§‘ í†µê³„ (í’ˆì§ˆë³„)
            today_stats = pd.read_sql_query("""
                SELECT 
                    COUNT(*) as total_news,
                    COUNT(DISTINCT stock_code) as stocks_with_news,
                    COUNT(DISTINCT source) as news_sources,
                    AVG(LENGTH(content)) as avg_content_length,
                    AVG(quality_score) as avg_quality_score,
                    COUNT(CASE WHEN quality_score >= 80 THEN 1 END) as high_quality_count
                FROM news_articles 
                WHERE DATE(collected_at) = DATE('now')
            """, conn).iloc[0]
            
            # ì†ŒìŠ¤ë³„ í†µê³„
            source_stats = pd.read_sql_query("""
                SELECT source, COUNT(*) as count, AVG(quality_score) as avg_quality
                FROM news_articles 
                WHERE DATE(collected_at) = DATE('now')
                GROUP BY source
                ORDER BY avg_quality DESC, count DESC
                LIMIT 5
            """, conn)
            
            # ì¢…ëª©ë³„ ë‰´ìŠ¤ ìˆ˜ TOP 5
            stock_stats = pd.read_sql_query("""
                SELECT stock_name, COUNT(*) as news_count, AVG(quality_score) as avg_quality
                FROM news_articles 
                WHERE DATE(collected_at) = DATE('now')
                GROUP BY stock_code, stock_name
                ORDER BY news_count DESC
                LIMIT 5
            """, conn)
        
        print(f"\n[ìˆ˜ì§‘ìš”ì•½] ì˜¤ëŠ˜ ìˆ˜ì§‘ ìš”ì•½:")
        print(f"  â€¢ ì´ ë‰´ìŠ¤: {today_stats['total_news']:,}ê°œ")
        print(f"  â€¢ ë‰´ìŠ¤ ìˆëŠ” ì¢…ëª©: {today_stats['stocks_with_news']:,}ê°œ")
        print(f"  â€¢ ë‰´ìŠ¤ ì†ŒìŠ¤: {today_stats['news_sources']:,}ê°œ")
        print(f"  â€¢ í‰ê·  í’ˆì§ˆ ì ìˆ˜: {today_stats['avg_quality_score']:.1f}ì ")
        print(f"  â€¢ ê³ í’ˆì§ˆ ë‰´ìŠ¤ (80ì  ì´ìƒ): {today_stats['high_quality_count']:,}ê°œ")
        print(f"  â€¢ í‰ê·  ë³¸ë¬¸ ê¸¸ì´: {today_stats['avg_content_length']:.0f}ì")
        
        if not source_stats.empty:
            print(f"\n[ì†ŒìŠ¤ë³„í’ˆì§ˆ] ì†ŒìŠ¤ë³„ ë‰´ìŠ¤ í’ˆì§ˆ:")
            for _, row in source_stats.iterrows():
                print(f"  â€¢ {row['source']}: {row['count']}ê°œ (í’ˆì§ˆ: {row['avg_quality']:.1f}ì )")
        
        if not stock_stats.empty:
            print(f"\n[ì¸ê¸°ì¢…ëª©] ë‰´ìŠ¤ ë§ì€ ì¢…ëª© TOP 5:")
            for _, row in stock_stats.iterrows():
                print(f"  â€¢ {row['stock_name']}: {row['news_count']}ê°œ (í’ˆì§ˆ: {row['avg_quality']:.1f}ì )")

def get_api_credentials():
    """í™˜ê²½ë³€ìˆ˜ì—ì„œ ë„¤ì´ë²„ API ì¸ì¦ì •ë³´ ì¡°íšŒ"""
    client_id = os.getenv('NAVER_CLIENT_ID')
    client_secret = os.getenv('NAVER_CLIENT_SECRET')
    
    if not client_id or not client_secret:
        print("\n[í™˜ê²½ë³€ìˆ˜ ì„¤ì • í•„ìš”]")
        print("âŒ .env íŒŒì¼ì— ë„¤ì´ë²„ API ì¸ì¦ì •ë³´ë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš”:")
        print()
        print("# .env íŒŒì¼ì— ì¶”ê°€í•  ë‚´ìš©:")
        print("NAVER_CLIENT_ID=your_client_id_here")
        print("NAVER_CLIENT_SECRET=your_client_secret_here")
        print()
        
        # ìˆ˜ë™ ì…ë ¥ ì˜µì…˜ ì œê³µ
        choice = input("ìˆ˜ë™ìœ¼ë¡œ ì…ë ¥í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/N): ").strip().lower()
        if choice == 'y':
            client_id = input("ğŸ” Client ID: ").strip()
            client_secret = input("ğŸ” Client Secret: ").strip()
            
            if client_id and client_secret:
                # .env íŒŒì¼ì— ìë™ ì €ì¥ ì œì•ˆ
                save_choice = input("\nì´ ì •ë³´ë¥¼ .env íŒŒì¼ì— ì €ì¥í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/N): ").strip().lower()
                if save_choice == 'y':
                    save_to_env(client_id, client_secret)
                
                return client_id, client_secret
        
        return None, None
    
    print(f"[í™˜ê²½ë³€ìˆ˜] ë„¤ì´ë²„ API ì¸ì¦ì •ë³´ ë¡œë“œ ì™„ë£Œ")
    print(f"  â€¢ Client ID: {client_id[:10]}...")
    return client_id, client_secret

def save_to_env(client_id: str, client_secret: str):
    """API ì¸ì¦ì •ë³´ë¥¼ .env íŒŒì¼ì— ì €ì¥"""
    env_file = project_root / '.env'
    
    try:
        # ê¸°ì¡´ .env íŒŒì¼ ì½ê¸°
        env_content = ""
        if env_file.exists():
            with open(env_file, 'r', encoding='utf-8') as f:
                env_content = f.read()
        
        # ê¸°ì¡´ NAVER API ì„¤ì • ì œê±°
        lines = env_content.split('\n')
        filtered_lines = [line for line in lines if not line.startswith(('NAVER_CLIENT_ID', 'NAVER_CLIENT_SECRET'))]
        
        # ìƒˆë¡œìš´ API ì„¤ì • ì¶”ê°€
        filtered_lines.extend([
            '',
            '# ë„¤ì´ë²„ ë‰´ìŠ¤ API ì„¤ì •',
            f'NAVER_CLIENT_ID={client_id}',
            f'NAVER_CLIENT_SECRET={client_secret}'
        ])
        
        # .env íŒŒì¼ì— ì €ì¥
        with open(env_file, 'w', encoding='utf-8') as f:
            f.write('\n'.join(filtered_lines))
        
        print("âœ… .env íŒŒì¼ì— API ì¸ì¦ì •ë³´ ì €ì¥ ì™„ë£Œ")
        
    except Exception as e:
        print(f"âŒ .env íŒŒì¼ ì €ì¥ ì‹¤íŒ¨: {e}")

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    
    print("\n" + "="*70)
    print("ğŸ“° ê³ í’ˆì§ˆ ë‰´ìŠ¤ ìˆ˜ì§‘ê¸° (í’ˆì§ˆ ê²€ì¦ ì‹œìŠ¤í…œ í†µí•©)")
    print("="*70)
    print("âœ… ìµœê·¼ 4ì¼ê°„ ë‰´ìŠ¤ ëŒ€ìƒ (í‰ì¼ + ì£¼ë§ í¬í•¨)")
    print("âœ… ì¢…ëª©ëª… + 'ì£¼ê°€', 'ì‹¤ì ', 'ì¬ë¬´' í‚¤ì›Œë“œ ì¡°í•©")
    print("âœ… ì™„ì „í•œ ë³¸ë¬¸ ë‚´ìš© ì¶”ì¶œ")
    print("âœ… API í˜¸ì¶œ ì œí•œ ê´€ë¦¬ (25,000íšŒ/ì¼)")
    print("ğŸ†• ìŠ¤íŒ¸/ì¤‘ë³µ/ì˜¤ë¥˜ ë‰´ìŠ¤ ìë™ í•„í„°ë§")
    print("ğŸ†• ì‹ ë¢°ë„ ì ìˆ˜ ê¸°ë°˜ í’ˆì§ˆ ê´€ë¦¬")
    print("ğŸ†• í•œê¸€ ì¸ì½”ë”© ë¬¸ì œ ì™„ì „ í•´ê²°")
    
    # í™˜ê²½ë³€ìˆ˜ì—ì„œ API ì¸ì¦ ì •ë³´ ë¡œë“œ
    client_id, client_secret = get_api_credentials()
    
    if not client_id or not client_secret:
        print("âŒ API ì¸ì¦ ì •ë³´ê°€ í•„ìš”í•©ë‹ˆë‹¤.")
        return
    
    # ìˆ˜ì§‘ê¸° ì´ˆê¸°í™”
    collector = StockNewsCollector(client_id, client_secret)
    
    print("\nğŸ¯ ìˆ˜ì§‘ ëª¨ë“œ ì„ íƒ:")
    print("1. í…ŒìŠ¤íŠ¸ ëª¨ë“œ (20ê°œ ì¢…ëª©)")
    print("2. ì „ì²´ ëª¨ë“œ (ëª¨ë“  ì¢…ëª©)")
    print("3. í˜„ì¬ ìˆ˜ì§‘ í˜„í™© í™•ì¸")
    print("4. í’ˆì§ˆ í†µê³„ í™•ì¸")
    print("5. ì¢…ë£Œ")
    
    choice = input("\nì„ íƒ (1-5): ").strip()
    
    if choice == '1':
        print("\nğŸ§ª í…ŒìŠ¤íŠ¸ ëª¨ë“œë¡œ ê³ í’ˆì§ˆ ë‰´ìŠ¤ ìˆ˜ì§‘ì„ ì‹œì‘í•©ë‹ˆë‹¤...")
        collector.collect_all_stocks_news(test_mode=True)
        
    elif choice == '2':
        stocks = collector.get_all_stocks()
        print(f"\n[ì „ì²´ì •ë³´] ì „ì²´ ëŒ€ìƒ ì¢…ëª©: {len(stocks):,}ê°œ")
        print(f"[ì˜ˆìƒAPI] ì˜ˆìƒ API í˜¸ì¶œ: ì•½ {len(stocks) * 4:,}íšŒ")
        print(f"[í’ˆì§ˆê²€ì¦] ìë™ í’ˆì§ˆ í•„í„°ë§ í™œì„±í™”")
        
        confirm = input("âš ï¸ ì „ì²´ ì¢…ëª© ìˆ˜ì§‘ì€ ì‹œê°„ì´ ì˜¤ë˜ ê±¸ë¦½ë‹ˆë‹¤. ê³„ì†í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/N): ").strip().lower()
        if confirm == 'y':
            print("\nğŸš€ ì „ì²´ ëª¨ë“œë¡œ ê³ í’ˆì§ˆ ë‰´ìŠ¤ ìˆ˜ì§‘ì„ ì‹œì‘í•©ë‹ˆë‹¤...")
            collector.collect_all_stocks_news(test_mode=False)
        else:
            print("âŒ ìˆ˜ì§‘ì´ ì·¨ì†Œë˜ì—ˆìŠµë‹ˆë‹¤.")
            
    elif choice == '3':
        collector.print_collection_summary()
        
    elif choice == '4':
        collector.print_quality_summary()
        
    elif choice == '5':
        print("ğŸ‘‹ í”„ë¡œê·¸ë¨ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
    
    else:
        print("âŒ ì˜ëª»ëœ ì„ íƒì…ë‹ˆë‹¤.")

if __name__ == "__main__":
    main()