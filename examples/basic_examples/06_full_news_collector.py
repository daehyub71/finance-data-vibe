"""
examples/basic_examples/06_full_news_collector.py

ì „ì²´ ì¢…ëª© ë‰´ìŠ¤ ìˆ˜ì§‘ê¸° - ê¸°ì¡´ í”„ë¡œì íŠ¸ êµ¬ì¡°ì— ë§ì¶˜ ë²„ì „
âœ… ìµœê·¼ 2 ì˜ì—…ì¼ ë‰´ìŠ¤ ìˆ˜ì§‘
âœ… ë„¤ì´ë²„ ë‰´ìŠ¤ API í™œìš©
âœ… ì¢…ëª©ëª… + "ì£¼ê°€", "ì‹¤ì ", "ì¬ë¬´" í‚¤ì›Œë“œ ì¡°í•©
âœ… ì™„ì „í•œ ë³¸ë¬¸ ë‚´ìš© ì¶”ì¶œ
"""

import sys
import os
from pathlib import Path

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
project_root = Path(__file__).parent.parent.parent
sys.path.append(str(project_root))

# í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
from dotenv import load_dotenv
load_dotenv(project_root / '.env')

import requests
import sqlite3
import pandas as pd
import time
from datetime import datetime, timedelta
import logging
from typing import List, Dict, Optional
from concurrent.futures import ThreadPoolExecutor, as_completed
import json
import re
from bs4 import BeautifulSoup
from tqdm import tqdm
import threading

# ë¡œê¹… ì„¤ì • (Windows ì¸ì½”ë”© ë¬¸ì œ í•´ê²°)
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(project_root / 'data' / 'news_collection.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class BusinessDayCalculator:
    """ì˜ì—…ì¼ ë° ì£¼ë§ í¬í•¨ ë‚ ì§œ ê³„ì‚°ê¸°"""
    
    @staticmethod
    def get_recent_news_days(days_count: int = 4) -> List[str]:
        """ìµœê·¼ ë‰´ìŠ¤ ìˆ˜ì§‘ ëŒ€ìƒì¼ ê³„ì‚° (í‰ì¼ + ì£¼ë§ í¬í•¨)"""
        news_days = []
        current_date = datetime.now()
        
        days_checked = 0
        while len(news_days) < days_count and days_checked < 10:
            current_date -= timedelta(days=1)
            days_checked += 1
            
            # ëª¨ë“  ìš”ì¼ í¬í•¨ (ì›”~ì¼)
            news_days.append(current_date.strftime('%Y-%m-%d'))
                
        logger.info(f"[ë‰´ìŠ¤ìˆ˜ì§‘ì¼] ìµœê·¼ {days_count}ì¼: {', '.join(news_days)}")
        return news_days

class NewsAPIManager:
    """ë„¤ì´ë²„ ë‰´ìŠ¤ API ê´€ë¦¬ì"""
    
    def __init__(self, client_id: str, client_secret: str):
        self.client_id = client_id
        self.client_secret = client_secret
        self.base_url = "https://openapi.naver.com/v1/search/news.json"
        self.headers = {
            'X-Naver-Client-Id': self.client_id,
            'X-Naver-Client-Secret': self.client_secret
        }
        
        # API í˜¸ì¶œ ì œí•œ ê´€ë¦¬
        self.api_calls_today = 0
        self.max_calls_per_day = 23000  # ì—¬ìœ ë¶„ 2000íšŒ
        self.last_call_time = time.time()
        self.min_interval = 0.12  # ì´ˆë‹¹ 8íšŒ ì œí•œ (ì•ˆì „í•˜ê²Œ)
        self.lock = threading.Lock()
        
    def rate_limit_check(self) -> bool:
        """API í˜¸ì¶œ ì œí•œ í™•ì¸"""
        with self.lock:
            current_time = time.time()
            
            if self.api_calls_today >= self.max_calls_per_day:
                logger.warning(f"âš ï¸ ì¼ì¼ API í˜¸ì¶œ ì œí•œ ë„ë‹¬: {self.api_calls_today:,}")
                return False
            
            time_since_last_call = current_time - self.last_call_time
            if time_since_last_call < self.min_interval:
                sleep_time = self.min_interval - time_since_last_call
                time.sleep(sleep_time)
            
            self.last_call_time = time.time()
            self.api_calls_today += 1
            
            return True
    
    def search_news(self, query: str, display: int = 100, sort: str = 'date') -> List[Dict]:
        """ë‰´ìŠ¤ ê²€ìƒ‰"""
        if not self.rate_limit_check():
            return []
        
        params = {
            'query': query,
            'display': min(display, 100),
            'sort': sort
        }
        
        try:
            response = requests.get(self.base_url, headers=self.headers, params=params, timeout=10)
            response.raise_for_status()
            
            data = response.json()
            items = data.get('items', [])
            
            # ìµœê·¼ ì˜ì—…ì¼ í•„í„°ë§
            recent_items = self._filter_recent_news(items)
            
            return recent_items
            
        except requests.exceptions.RequestException as e:
            logger.error(f"âŒ ë‰´ìŠ¤ ê²€ìƒ‰ API ì˜¤ë¥˜ - ê²€ìƒ‰ì–´: {query}, ì˜¤ë¥˜: {e}")
            return []
        except Exception as e:
            logger.error(f"âŒ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜ - ê²€ìƒ‰ì–´: {query}, ì˜¤ë¥˜: {e}")
            return []
    
    def _filter_recent_news(self, items: List[Dict]) -> List[Dict]:
        """ìµœê·¼ 4ì¼ê°„ ë‰´ìŠ¤ë§Œ í•„í„°ë§ (í‰ì¼ + ì£¼ë§ í¬í•¨)"""
        news_days = BusinessDayCalculator.get_recent_news_days(4)
        recent_items = []
        
        for item in items:
            try:
                pub_date_str = item.get('pubDate', '')
                pub_date = self._parse_date(pub_date_str)
                
                if pub_date:
                    pub_date_formatted = pub_date.strftime('%Y-%m-%d')
                    if pub_date_formatted in news_days:
                        recent_items.append(item)
            except:
                continue
        
        return recent_items
    
    def _parse_date(self, date_str: str) -> Optional[datetime]:
        """ë‚ ì§œ ë¬¸ìì—´ì„ datetimeìœ¼ë¡œ ë³€í™˜"""
        try:
            dt = datetime.strptime(date_str, "%a, %d %b %Y %H:%M:%S %z")
            return dt.replace(tzinfo=None)
        except:
            return None

class NewsContentExtractor:
    """ë‰´ìŠ¤ ë³¸ë¬¸ ì¶”ì¶œê¸°"""
    
    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        })
    
    def extract_content(self, url: str) -> str:
        """ë‰´ìŠ¤ ê¸°ì‚¬ ë³¸ë¬¸ ì¶”ì¶œ"""
        try:
            response = self.session.get(url, timeout=15)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            content = ""
            
            # ë„¤ì´ë²„ ë‰´ìŠ¤ ë³¸ë¬¸ ì¶”ì¶œ
            if 'news.naver.com' in url:
                content = self._extract_naver_content(soup)
            
            # ë‹¤ë¥¸ ë‰´ìŠ¤ ì‚¬ì´íŠ¸ ë³¸ë¬¸ ì¶”ì¶œ
            if not content:
                content = self._extract_general_content(soup)
            
            return content[:3000] if content else ""
            
        except Exception as e:
            logger.debug(f"ë³¸ë¬¸ ì¶”ì¶œ ì‹¤íŒ¨ - {url}: {e}")
            return ""
    
    def _extract_naver_content(self, soup: BeautifulSoup) -> str:
        """ë„¤ì´ë²„ ë‰´ìŠ¤ ë³¸ë¬¸ ì¶”ì¶œ"""
        selectors = [
            'div#newsct_article',
            'div.newsct_article', 
            'div#articleBodyContents',
            'div.article_body',
            'div.news_end'
        ]
        
        for selector in selectors:
            content_div = soup.select_one(selector)
            if content_div:
                # ë¶ˆí•„ìš”í•œ ìš”ì†Œ ì œê±°
                for elem in content_div.find_all(['script', 'style', 'ins', 'iframe', 'aside']):
                    elem.decompose()
                
                text = content_div.get_text(separator=' ', strip=True)
                text = self._clean_text(text)
                
                if len(text) > 100:
                    return text
        
        return ""
    
    def _extract_general_content(self, soup: BeautifulSoup) -> str:
        """ì¼ë°˜ ë‰´ìŠ¤ ì‚¬ì´íŠ¸ ë³¸ë¬¸ ì¶”ì¶œ"""
        selectors = [
            'div.article-content',
            'div.news-content',
            'div.content',
            'article',
            'div.post-content',
            'div.article_txt',
            'div.article-body'
        ]
        
        for selector in selectors:
            content = soup.select_one(selector)
            if content:
                for elem in content.find_all(['script', 'style', 'ins', 'iframe']):
                    elem.decompose()
                
                text = content.get_text(separator=' ', strip=True)
                text = self._clean_text(text)
                
                if len(text) > 100:
                    return text
        
        # ë§ˆì§€ë§‰ ì‹œë„: ëª¨ë“  p íƒœê·¸
        paragraphs = soup.find_all('p')
        if paragraphs:
            text = ' '.join([p.get_text(strip=True) for p in paragraphs])
            text = self._clean_text(text)
            if len(text) > 100:
                return text
        
        return ""
    
    def _clean_text(self, text: str) -> str:
        """í…ìŠ¤íŠ¸ ì •ì œ"""
        # ë¶ˆí•„ìš”í•œ ë¬¸êµ¬ ì œê±°
        patterns_to_remove = [
            r'// flash ì˜¤ë¥˜ë¥¼ ìš°íšŒí•˜ê¸° ìœ„í•œ í•¨ìˆ˜ ì¶”ê°€.*',
            r'ë³¸ ê¸°ì‚¬ëŠ”.*?ì…ë‹ˆë‹¤',
            r'ì €ì‘ê¶Œì.*?ë¬´ë‹¨.*?ê¸ˆì§€',
            r'ê¸°ì\s*=.*?ê¸°ì',
            r'\[.*?\]',
            r'<.*?>',
            r'&[a-zA-Z]+;'
        ]
        
        for pattern in patterns_to_remove:
            text = re.sub(pattern, '', text, flags=re.IGNORECASE)
        
        # ì—¬ëŸ¬ ê³µë°±ì„ í•˜ë‚˜ë¡œ
        text = re.sub(r'\s+', ' ', text)
        
        return text.strip()

class StockNewsCollector:
    """ì£¼ì‹ ë‰´ìŠ¤ ìˆ˜ì§‘ê¸° ë©”ì¸ í´ë˜ìŠ¤"""
    
    def __init__(self, client_id: str, client_secret: str):
        self.api_manager = NewsAPIManager(client_id, client_secret)
        self.content_extractor = NewsContentExtractor()
        self.db_path = project_root / "finance_data.db"
        self.init_database()  # ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™” ì¶”ê°€
        
    def init_database(self):
        """ë°ì´í„°ë² ì´ìŠ¤ í…Œì´ë¸” ì´ˆê¸°í™”"""
        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.cursor()
            
            # ê¸°ì¡´ í…Œì´ë¸” í™•ì¸
            cursor.execute("SELECT name FROM sqlite_master WHERE type='table'")
            existing_tables = [row[0] for row in cursor.fetchall()]
            
            # stock_info í…Œì´ë¸”ì´ ì—†ìœ¼ë©´ ìƒì„±
            if 'stock_info' not in existing_tables:
                logger.info("stock_info í…Œì´ë¸”ì„ ìƒì„±í•©ë‹ˆë‹¤...")
                cursor.execute('''
                    CREATE TABLE stock_info (
                        code TEXT PRIMARY KEY,
                        name TEXT NOT NULL,
                        market TEXT,
                        sector TEXT,
                        created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                    )
                ''')
                
                # ê¸°ë³¸ ì¢…ëª© ë°ì´í„° ì‚½ì…
                basic_stocks = [
                    ('005930', 'ì‚¼ì„±ì „ì', 'KOSPI', 'IT'),
                    ('000660', 'SKí•˜ì´ë‹‰ìŠ¤', 'KOSPI', 'IT'),
                    ('035420', 'NAVER', 'KOSPI', 'IT'),
                    ('005380', 'í˜„ëŒ€ì°¨', 'KOSPI', 'ìë™ì°¨'),
                    ('006400', 'ì‚¼ì„±SDI', 'KOSPI', 'í™”í•™'),
                    ('051910', 'LGí™”í•™', 'KOSPI', 'í™”í•™'),
                    ('096770', 'SKì´ë…¸ë² ì´ì…˜', 'KOSPI', 'í™”í•™'),
                    ('034730', 'SK', 'KOSPI', 'ì§€ì£¼íšŒì‚¬'),
                    ('003550', 'LG', 'KOSPI', 'ì§€ì£¼íšŒì‚¬'),
                    ('012330', 'í˜„ëŒ€ëª¨ë¹„ìŠ¤', 'KOSPI', 'ìë™ì°¨ë¶€í’ˆ'),
                    ('207940', 'ì‚¼ì„±ë°”ì´ì˜¤ë¡œì§ìŠ¤', 'KOSPI', 'ë°”ì´ì˜¤'),
                    ('373220', 'LGì—ë„ˆì§€ì†”ë£¨ì…˜', 'KOSPI', 'í™”í•™'),
                    ('000270', 'ê¸°ì•„', 'KOSPI', 'ìë™ì°¨'),
                    ('068270', 'ì…€íŠ¸ë¦¬ì˜¨', 'KOSPI', 'ë°”ì´ì˜¤'),
                    ('035720', 'ì¹´ì¹´ì˜¤', 'KOSPI', 'IT'),
                    ('018260', 'ì‚¼ì„±ì—ìŠ¤ë””ì—ìŠ¤', 'KOSPI', 'IT'),
                    ('036570', 'ì—”ì”¨ì†Œí”„íŠ¸', 'KOSPI', 'IT'),
                    ('066570', 'LGì „ì', 'KOSPI', 'ì „ì'),
                    ('105560', 'KBê¸ˆìœµ', 'KOSPI', 'ê¸ˆìœµ'),
                    ('055550', 'ì‹ í•œì§€ì£¼', 'KOSPI', 'ê¸ˆìœµ')
                ]
                
                for stock in basic_stocks:
                    cursor.execute('''
                        INSERT OR IGNORE INTO stock_info (code, name, market, sector)
                        VALUES (?, ?, ?, ?)
                    ''', stock)
                
                logger.info(f"{len(basic_stocks)}ê°œ ê¸°ë³¸ ì¢…ëª© ë°ì´í„° ìƒì„± ì™„ë£Œ")
            
            # news_articles í…Œì´ë¸” ìƒì„± (ë‰´ìŠ¤ ì €ì¥ìš©)
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS news_articles (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    stock_code TEXT NOT NULL,
                    stock_name TEXT NOT NULL,
                    title TEXT NOT NULL,
                    link TEXT NOT NULL UNIQUE,
                    description TEXT,
                    content TEXT,
                    pub_date TEXT,
                    source TEXT,
                    sentiment_score REAL DEFAULT 0.0,
                    collected_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                )
            ''')
            
            # ì¸ë±ìŠ¤ ìƒì„±
            cursor.execute('CREATE INDEX IF NOT EXISTS idx_news_stock_code ON news_articles(stock_code)')
            cursor.execute('CREATE INDEX IF NOT EXISTS idx_news_pub_date ON news_articles(pub_date)')
            cursor.execute('CREATE INDEX IF NOT EXISTS idx_news_collected_at ON news_articles(collected_at)')
            
            conn.commit()
        
    def get_all_stocks(self) -> List[Dict[str, str]]:
        """ì „ì²´ ì£¼ì‹ ì¢…ëª© ì¡°íšŒ"""
        with sqlite3.connect(self.db_path) as conn:
            df = pd.read_sql_query("""
                SELECT code, name 
                FROM stock_info 
                WHERE name NOT LIKE '%ìŠ¤íŒ©%'
                AND name NOT LIKE '%ë¦¬ì¸ %'
                AND name NOT LIKE '%ETF%'
                ORDER BY code
            """, conn)
            
        return df.to_dict('records')
    
    def get_existing_links_today(self) -> set:
        """ì˜¤ëŠ˜ ìˆ˜ì§‘ëœ ë‰´ìŠ¤ ë§í¬ë“¤ (ì¤‘ë³µ ë°©ì§€)"""
        with sqlite3.connect(self.db_path) as conn:
            df = pd.read_sql_query("""
                SELECT DISTINCT link 
                FROM news_articles 
                WHERE DATE(collected_at) = DATE('now')
            """, conn)
            
        return set(df['link'].tolist()) if not df.empty else set()
    
    def collect_stock_news(self, stock: Dict[str, str]) -> List[Dict]:
        """íŠ¹ì • ì¢…ëª©ì˜ ë‰´ìŠ¤ ìˆ˜ì§‘"""
        stock_code = stock['code']
        stock_name = stock['name']
        
        collected_news = []
        existing_links = self.get_existing_links_today()
        
        # ê²€ìƒ‰ ì „ëµ: ì¢…ëª©ëª… + í‚¤ì›Œë“œ ì¡°í•©
        search_strategies = [
            stock_name,
            f"{stock_name} ì£¼ê°€",
            f"{stock_name} ì‹¤ì ",
            f"{stock_name} ì¬ë¬´"
        ]
        
        max_news_per_query = 30
        max_total_news = 50
        
        for query in search_strategies:
            if len(collected_news) >= max_total_news:
                break
            
            if self.api_manager.api_calls_today >= self.api_manager.max_calls_per_day:
                logger.warning("[ê²½ê³ ] API í˜¸ì¶œ ì œí•œ ë„ë‹¬, ìˆ˜ì§‘ ì¤‘ë‹¨")
                break
            
            news_items = self.api_manager.search_news(query, display=max_news_per_query)
            
            for item in news_items:
                if len(collected_news) >= max_total_news:
                    break
                
                if item['link'] in existing_links:
                    continue
                
                # ì¢…ëª© ê´€ë ¨ì„± ì²´í¬
                title = re.sub(r'<[^>]+>', '', item['title'])
                description = re.sub(r'<[^>]+>', '', item['description'])
                
                if self._is_relevant_news(title, description, stock_name, stock_code):
                    # ë³¸ë¬¸ ìˆ˜ì§‘
                    content = self.content_extractor.extract_content(item['link'])
                    
                    news_data = {
                        'stock_code': stock_code,
                        'stock_name': stock_name,
                        'title': title,
                        'link': item['link'],
                        'description': description,
                        'content': content,
                        'pub_date': item['pubDate'],
                        'source': self._extract_source(item.get('originallink', item['link']))
                    }
                    
                    collected_news.append(news_data)
                    existing_links.add(item['link'])
            
            time.sleep(0.1)
        
        if collected_news:
            logger.info(f"[ìˆ˜ì§‘ì™„ë£Œ] {stock_name}: {len(collected_news)}ê°œ ë‰´ìŠ¤ ìˆ˜ì§‘ ì™„ë£Œ")
        
        return collected_news
    
    def _is_relevant_news(self, title: str, description: str, stock_name: str, stock_code: str) -> bool:
        """ë‰´ìŠ¤ì˜ ì¢…ëª© ê´€ë ¨ì„± ì²´í¬"""
        # ì¢…ëª©ëª… ì§ì ‘ í¬í•¨
        if stock_name in title or stock_code in title:
            return True
        
        # ì„¤ëª…ì— ì¢…ëª©ëª… í¬í•¨
        if stock_name in description:
            return True
        
        # ì£¼ì‹ ê´€ë ¨ í‚¤ì›Œë“œ + ì¢…ëª©ëª… ì¼ë¶€
        stock_keywords = ['ì£¼ê°€', 'ì‹¤ì ', 'ì¬ë¬´', 'ë§¤ì¶œ', 'ì˜ì—…ì´ìµ', 'íˆ¬ì', 'ìƒì¥', 'ê³µì‹œ', 'ë°°ë‹¹']
        text_combined = f"{title} {description}".lower()
        
        if any(keyword in text_combined for keyword in stock_keywords):
            name_parts = stock_name.split()
            if any(part in text_combined for part in name_parts if len(part) > 1):
                return True
        
        return False
    
    def _extract_source(self, url: str) -> str:
        """ë‰´ìŠ¤ ì†ŒìŠ¤ ì¶”ì¶œ"""
        if not url:
            return 'Unknown'
        
        source_mapping = {
            'chosun.com': 'ì¡°ì„ ì¼ë³´',
            'donga.com': 'ë™ì•„ì¼ë³´',
            'joins.com': 'ì¤‘ì•™ì¼ë³´',
            'mk.co.kr': 'ë§¤ì¼ê²½ì œ',
            'hankyung.com': 'í•œêµ­ê²½ì œ',
            'yonhapnews.co.kr': 'ì—°í•©ë‰´ìŠ¤',
            'mt.co.kr': 'ë¨¸ë‹ˆíˆ¬ë°ì´',
            'etnews.com': 'ì „ìì‹ ë¬¸'
        }
        
        for domain, source in source_mapping.items():
            if domain in url:
                return source
        
        try:
            domain_parts = url.split('//')[1].split('/')[0].split('.')
            return domain_parts[-2] if len(domain_parts) > 1 else 'Unknown'
        except:
            return 'Unknown'
    
    def save_news_batch(self, news_list: List[Dict]) -> int:
        """ë‰´ìŠ¤ ë°°ì¹˜ ì €ì¥"""
        if not news_list:
            return 0
        
        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.cursor()
            
            saved_count = 0
            for news in news_list:
                try:
                    cursor.execute('''
                        INSERT OR IGNORE INTO news_articles 
                        (stock_code, stock_name, title, link, description, content, pub_date, source)
                        VALUES (?, ?, ?, ?, ?, ?, ?, ?)
                    ''', (
                        news['stock_code'],
                        news['stock_name'],
                        news['title'],
                        news['link'],
                        news['description'],
                        news['content'],
                        news['pub_date'],
                        news['source']
                    ))
                    
                    if cursor.rowcount > 0:
                        saved_count += 1
                        
                except sqlite3.Error as e:
                    logger.error(f"ì €ì¥ ì‹¤íŒ¨ - {news['title']}: {e}")
            
            conn.commit()
            
        return saved_count
    
    def collect_all_stocks_news(self, max_workers: int = 3, batch_size: int = 20, test_mode: bool = False):
        """ì „ì²´ ì¢…ëª© ë‰´ìŠ¤ ìˆ˜ì§‘"""
        
        news_days = BusinessDayCalculator.get_recent_news_days(4)
        stocks = self.get_all_stocks()
        
        if test_mode:
            stocks = stocks[:20]
            logger.info(f"[í…ŒìŠ¤íŠ¸ëª¨ë“œ] {len(stocks)}ê°œ ì¢…ëª©ìœ¼ë¡œ ì œí•œ")
        
        logger.info(f"[ì‹œì‘] ì´ {len(stocks)}ê°œ ì¢…ëª© ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹œì‘")
        logger.info(f"[ìˆ˜ì§‘ê¸°ê°„] ìµœê·¼ 4ì¼ê°„ ë‰´ìŠ¤ ìˆ˜ì§‘ (í‰ì¼ + ì£¼ë§ í¬í•¨)")
        
        total_collected = 0
        total_saved = 0
        
        with tqdm(total=len(stocks), desc="ë‰´ìŠ¤ ìˆ˜ì§‘ ì§„í–‰", unit="ì¢…ëª©") as pbar:
            
            for i in range(0, len(stocks), batch_size):
                batch = stocks[i:i + batch_size]
                batch_news = []
                
                if self.api_manager.api_calls_today >= self.api_manager.max_calls_per_day:
                    logger.warning("[ê²½ê³ ] ì¼ì¼ API í˜¸ì¶œ ì œí•œ ë„ë‹¬, ìˆ˜ì§‘ ì¤‘ë‹¨")
                    break
                
                logger.info(f"[ë°°ì¹˜ì²˜ë¦¬] ë°°ì¹˜ {i//batch_size + 1}/{(len(stocks)-1)//batch_size + 1} ì²˜ë¦¬ ì¤‘...")
                
                with ThreadPoolExecutor(max_workers=max_workers) as executor:
                    future_to_stock = {
                        executor.submit(self.collect_stock_news, stock): stock
                        for stock in batch
                    }
                    
                    for future in as_completed(future_to_stock):
                        stock = future_to_stock[future]
                        try:
                            news_list = future.result()
                            if news_list:
                                batch_news.extend(news_list)
                                total_collected += len(news_list)
                            
                            pbar.set_postfix({
                                'APIí˜¸ì¶œ': f"{self.api_manager.api_calls_today:,}",
                                'ìˆ˜ì§‘': f"{total_collected:,}",
                                'ì €ì¥': f"{total_saved:,}"
                            })
                            
                        except Exception as e:
                            logger.error(f"[ì˜¤ë¥˜] {stock['name']} ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                        
                        pbar.update(1)
                
                # ë°°ì¹˜ ì €ì¥
                if batch_news:
                    saved_count = self.save_news_batch(batch_news)
                    total_saved += saved_count
                    logger.info(f"[ë°°ì¹˜ì €ì¥] ë°°ì¹˜ ì €ì¥: {len(batch_news)}ê°œ ìˆ˜ì§‘ -> {saved_count}ê°œ ì‹ ê·œ ì €ì¥")
                
                # ë°°ì¹˜ ê°„ ëŒ€ê¸°
                if i + batch_size < len(stocks):
                    time.sleep(10)
        
        logger.info(f"[ì™„ë£Œ] ì „ì²´ ìˆ˜ì§‘ ì™„ë£Œ!")
        logger.info(f"[ê²°ê³¼] ìµœì¢… ê²°ê³¼: {total_collected:,}ê°œ ìˆ˜ì§‘, {total_saved:,}ê°œ ì €ì¥")
        logger.info(f"[APIí˜¸ì¶œ] API í˜¸ì¶œ ìˆ˜: {self.api_manager.api_calls_today:,}")
        
        self.print_collection_summary()
    
    def print_collection_summary(self):
        """ìˆ˜ì§‘ ê²°ê³¼ ìš”ì•½ ì¶œë ¥"""
        with sqlite3.connect(self.db_path) as conn:
            # ì˜¤ëŠ˜ ìˆ˜ì§‘ í†µê³„
            today_stats = pd.read_sql_query("""
                SELECT 
                    COUNT(*) as total_news,
                    COUNT(DISTINCT stock_code) as stocks_with_news,
                    COUNT(DISTINCT source) as news_sources,
                    AVG(LENGTH(content)) as avg_content_length
                FROM news_articles 
                WHERE DATE(collected_at) = DATE('now')
            """, conn).iloc[0]
            
            # ì†ŒìŠ¤ë³„ í†µê³„
            source_stats = pd.read_sql_query("""
                SELECT source, COUNT(*) as count
                FROM news_articles 
                WHERE DATE(collected_at) = DATE('now')
                GROUP BY source
                ORDER BY count DESC
                LIMIT 5
            """, conn)
            
            # ì¢…ëª©ë³„ ë‰´ìŠ¤ ìˆ˜ TOP 5
            stock_stats = pd.read_sql_query("""
                SELECT stock_name, COUNT(*) as news_count
                FROM news_articles 
                WHERE DATE(collected_at) = DATE('now')
                GROUP BY stock_code, stock_name
                ORDER BY news_count DESC
                LIMIT 5
            """, conn)
        
        print(f"\n[ìˆ˜ì§‘ìš”ì•½] ì˜¤ëŠ˜ ìˆ˜ì§‘ ìš”ì•½:")
        print(f"  â€¢ ì´ ë‰´ìŠ¤: {today_stats['total_news']:,}ê°œ")
        print(f"  â€¢ ë‰´ìŠ¤ ìˆëŠ” ì¢…ëª©: {today_stats['stocks_with_news']:,}ê°œ")
        print(f"  â€¢ ë‰´ìŠ¤ ì†ŒìŠ¤: {today_stats['news_sources']:,}ê°œ")
        print(f"  â€¢ í‰ê·  ë³¸ë¬¸ ê¸¸ì´: {today_stats['avg_content_length']:.0f}ì")
        
        if not source_stats.empty:
            print(f"\n[ì†ŒìŠ¤ë³„í†µê³„] ì†ŒìŠ¤ë³„ ë‰´ìŠ¤ ìˆ˜:")
            for _, row in source_stats.iterrows():
                print(f"  â€¢ {row['source']}: {row['count']}ê°œ")
        
        if not stock_stats.empty:
            print(f"\n[ì¸ê¸°ì¢…ëª©] ë‰´ìŠ¤ ë§ì€ ì¢…ëª© TOP 5:")
            for _, row in stock_stats.iterrows():
                print(f"  â€¢ {row['stock_name']}: {row['news_count']}ê°œ")

def get_api_credentials():
    """í™˜ê²½ë³€ìˆ˜ì—ì„œ ë„¤ì´ë²„ API ì¸ì¦ì •ë³´ ì¡°íšŒ"""
    client_id = os.getenv('NAVER_CLIENT_ID')
    client_secret = os.getenv('NAVER_CLIENT_SECRET')
    
    if not client_id or not client_secret:
        print("\n[í™˜ê²½ë³€ìˆ˜ ì„¤ì • í•„ìš”]")
        print("âŒ .env íŒŒì¼ì— ë„¤ì´ë²„ API ì¸ì¦ì •ë³´ë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš”:")
        print()
        print("# .env íŒŒì¼ì— ì¶”ê°€í•  ë‚´ìš©:")
        print("NAVER_CLIENT_ID=your_client_id_here")
        print("NAVER_CLIENT_SECRET=your_client_secret_here")
        print()
        
        # ìˆ˜ë™ ì…ë ¥ ì˜µì…˜ ì œê³µ
        choice = input("ìˆ˜ë™ìœ¼ë¡œ ì…ë ¥í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/N): ").strip().lower()
        if choice == 'y':
            client_id = input("ğŸ” Client ID: ").strip()
            client_secret = input("ğŸ” Client Secret: ").strip()
            
            if client_id and client_secret:
                # .env íŒŒì¼ì— ìë™ ì €ì¥ ì œì•ˆ
                save_choice = input("\nì´ ì •ë³´ë¥¼ .env íŒŒì¼ì— ì €ì¥í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/N): ").strip().lower()
                if save_choice == 'y':
                    save_to_env(client_id, client_secret)
                
                return client_id, client_secret
        
        return None, None
    
    print(f"[í™˜ê²½ë³€ìˆ˜] ë„¤ì´ë²„ API ì¸ì¦ì •ë³´ ë¡œë“œ ì™„ë£Œ")
    print(f"  â€¢ Client ID: {client_id[:10]}...")
    return client_id, client_secret

def save_to_env(client_id: str, client_secret: str):
    """API ì¸ì¦ì •ë³´ë¥¼ .env íŒŒì¼ì— ì €ì¥"""
    env_file = project_root / '.env'
    
    try:
        # ê¸°ì¡´ .env íŒŒì¼ ì½ê¸°
        env_content = ""
        if env_file.exists():
            with open(env_file, 'r', encoding='utf-8') as f:
                env_content = f.read()
        
        # ê¸°ì¡´ NAVER API ì„¤ì • ì œê±°
        lines = env_content.split('\n')
        filtered_lines = [line for line in lines if not line.startswith(('NAVER_CLIENT_ID', 'NAVER_CLIENT_SECRET'))]
        
        # ìƒˆë¡œìš´ API ì„¤ì • ì¶”ê°€
        filtered_lines.extend([
            '',
            '# ë„¤ì´ë²„ ë‰´ìŠ¤ API ì„¤ì •',
            f'NAVER_CLIENT_ID={client_id}',
            f'NAVER_CLIENT_SECRET={client_secret}'
        ])
        
        # .env íŒŒì¼ì— ì €ì¥
        with open(env_file, 'w', encoding='utf-8') as f:
            f.write('\n'.join(filtered_lines))
        
        print("âœ… .env íŒŒì¼ì— API ì¸ì¦ì •ë³´ ì €ì¥ ì™„ë£Œ")
        
    except Exception as e:
        print(f"âŒ .env íŒŒì¼ ì €ì¥ ì‹¤íŒ¨: {e}")

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    
    print("\n" + "="*70)
    print("ğŸ“° ì „ì²´ ì¢…ëª© ë‰´ìŠ¤ ìˆ˜ì§‘ê¸°")
    print("="*70)
    print("âœ… ìµœê·¼ 4ì¼ê°„ ë‰´ìŠ¤ ëŒ€ìƒ (í‰ì¼ + ì£¼ë§ í¬í•¨)")
    print("âœ… ì¢…ëª©ëª… + 'ì£¼ê°€', 'ì‹¤ì ', 'ì¬ë¬´' í‚¤ì›Œë“œ ì¡°í•©")
    print("âœ… ì™„ì „í•œ ë³¸ë¬¸ ë‚´ìš© ì¶”ì¶œ")
    print("âœ… API í˜¸ì¶œ ì œí•œ ê´€ë¦¬ (25,000íšŒ/ì¼)")
    
    # í™˜ê²½ë³€ìˆ˜ì—ì„œ API ì¸ì¦ ì •ë³´ ë¡œë“œ
    client_id, client_secret = get_api_credentials()
    
    if not client_id or not client_secret:
        print("âŒ API ì¸ì¦ ì •ë³´ê°€ í•„ìš”í•©ë‹ˆë‹¤.")
        return
    
    # ìˆ˜ì§‘ê¸° ì´ˆê¸°í™”
    collector = StockNewsCollector(client_id, client_secret)
    
    print("\nğŸ¯ ìˆ˜ì§‘ ëª¨ë“œ ì„ íƒ:")
    print("1. í…ŒìŠ¤íŠ¸ ëª¨ë“œ (20ê°œ ì¢…ëª©)")
    print("2. ì „ì²´ ëª¨ë“œ (ëª¨ë“  ì¢…ëª©)")
    print("3. í˜„ì¬ ìˆ˜ì§‘ í˜„í™© í™•ì¸")
    print("4. ì¢…ë£Œ")
    
    choice = input("\nì„ íƒ (1-4): ").strip()
    
    if choice == '1':
        print("\nğŸ§ª í…ŒìŠ¤íŠ¸ ëª¨ë“œë¡œ ë‰´ìŠ¤ ìˆ˜ì§‘ì„ ì‹œì‘í•©ë‹ˆë‹¤...")
        collector.collect_all_stocks_news(test_mode=True)
        
    elif choice == '2':
        stocks = collector.get_all_stocks()
        print(f"\n[ì „ì²´ì •ë³´] ì „ì²´ ëŒ€ìƒ ì¢…ëª©: {len(stocks):,}ê°œ")
        print(f"[ì˜ˆìƒAPI] ì˜ˆìƒ API í˜¸ì¶œ: ì•½ {len(stocks) * 4:,}íšŒ")
        
        confirm = input("âš ï¸ ì „ì²´ ì¢…ëª© ìˆ˜ì§‘ì€ ì‹œê°„ì´ ì˜¤ë˜ ê±¸ë¦½ë‹ˆë‹¤. ê³„ì†í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/N): ").strip().lower()
        if confirm == 'y':
            print("\nğŸš€ ì „ì²´ ëª¨ë“œë¡œ ë‰´ìŠ¤ ìˆ˜ì§‘ì„ ì‹œì‘í•©ë‹ˆë‹¤...")
            collector.collect_all_stocks_news(test_mode=False)
        else:
            print("âŒ ìˆ˜ì§‘ì´ ì·¨ì†Œë˜ì—ˆìŠµë‹ˆë‹¤.")
            
    elif choice == '3':
        collector.print_collection_summary()
        
    elif choice == '4':
        print("ğŸ‘‹ í”„ë¡œê·¸ë¨ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
    
    else:
        print("âŒ ì˜ëª»ëœ ì„ íƒì…ë‹ˆë‹¤.")

if __name__ == "__main__":
    main()