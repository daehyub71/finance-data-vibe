"""
examples/basic_examples/07_buffett_sentiment_analyzer.py

ì›ŒëŸ° ë²„í• ìŠ¤íƒ€ì¼ ë‰´ìŠ¤ ê°ì • ë¶„ì„ ì—”ì§„
âœ… ê°€ì¹˜íˆ¬ì ê´€ì  ê°ì • ì‚¬ì „ êµ¬ì¶•
âœ… í€ë”ë©˜í„¸ vs ë…¸ì´ì¦ˆ ë¶„ë¥˜
âœ… ì¼ë³„ ì¢…ëª©ë³„ ê°ì • ì§€ìˆ˜ ê³„ì‚°
âœ… ì¥ê¸°íˆ¬ì ê´€ë ¨ì„± ìŠ¤ì½”ì–´ë§

ë¹„ì¤‘: ë‰´ìŠ¤ê°ì •ë¶„ì„ 25% (ë³´ì¡° ì§€í‘œ ì—­í• )
"""

import sys
import os
from pathlib import Path
import sqlite3
import pandas as pd
import numpy as np
import re
from datetime import datetime, timedelta
from typing import Dict, List, Tuple, Optional
import json
from tqdm import tqdm
import logging
from collections import defaultdict, Counter

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
project_root = Path(__file__).parent.parent.parent
sys.path.append(str(project_root))

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class BuffettSentimentAnalyzer:
    """
    ì›ŒëŸ° ë²„í• ìŠ¤íƒ€ì¼ ë‰´ìŠ¤ ê°ì • ë¶„ì„ê¸°
    
    í•µì‹¬ ì² í•™:
    1. í€ë”ë©˜í„¸ ë‰´ìŠ¤ì— ë†’ì€ ê°€ì¤‘ì¹˜
    2. ë‹¨ê¸° ë…¸ì´ì¦ˆëŠ” ë‚®ì€ ê°€ì¤‘ì¹˜  
    3. ì¥ê¸° íˆ¬ì ê´€ë ¨ì„± ìš°ì„ 
    4. ê°ì • ì—­íˆ¬ì ì‹ í˜¸ í™œìš©
    """
    
    def __init__(self):
        self.db_path = project_root / "finance_data.db"
        
        # ë°ì´í„°ë² ì´ìŠ¤ ìŠ¤í‚¤ë§ˆ ê²€ì¦
        if not self._validate_database_schema():
            logger.error("âŒ ë°ì´í„°ë² ì´ìŠ¤ ìŠ¤í‚¤ë§ˆê°€ ë¶€ì ì ˆí•©ë‹ˆë‹¤!")
            logger.error("ğŸ”§ ë§ˆì´ê·¸ë ˆì´ì…˜ì„ ì‹¤í–‰í•˜ì„¸ìš”: python examples/basic_examples/08_db_migration_sentiment.py")
            raise RuntimeError("ë°ì´í„°ë² ì´ìŠ¤ ë§ˆì´ê·¸ë ˆì´ì…˜ í•„ìš”")
        
        # ì›ŒëŸ° ë²„í• ìŠ¤íƒ€ì¼ ê°ì • ì‚¬ì „ êµ¬ì¶•
        self._build_buffett_sentiment_dictionary()
        
        # ë‰´ìŠ¤ ì¹´í…Œê³ ë¦¬ë³„ ê°€ì¤‘ì¹˜ (ê°€ì¹˜íˆ¬ì ê´€ì )
        self.news_weights = {
            'fundamental': 1.0,      # í€ë”ë©˜í„¸ ë‰´ìŠ¤ (ìµœê³  ê°€ì¤‘ì¹˜)
            'business': 0.8,         # ì‚¬ì—… ê´€ë ¨ ë‰´ìŠ¤
            'financial': 0.9,        # ì¬ë¬´ ê´€ë ¨ ë‰´ìŠ¤
            'management': 0.7,       # ê²½ì˜ì§„ ê´€ë ¨ ë‰´ìŠ¤
            'market': 0.4,           # ì‹œì¥ ì¼ë°˜ ë‰´ìŠ¤
            'technical': 0.3,        # ê¸°ìˆ ì  ë¶„ì„ ë‰´ìŠ¤ (ë‚®ì€ ê°€ì¤‘ì¹˜)
            'noise': 0.1            # ë…¸ì´ì¦ˆì„± ë‰´ìŠ¤ (ìµœì € ê°€ì¤‘ì¹˜)
        }
        
        logger.info("âœ… ì›ŒëŸ° ë²„í• ìŠ¤íƒ€ì¼ ê°ì • ë¶„ì„ê¸° ì´ˆê¸°í™” ì™„ë£Œ")
    
    def _build_buffett_sentiment_dictionary(self):
        """ì›ŒëŸ° ë²„í• ìŠ¤íƒ€ì¼ ê°ì • ì‚¬ì „ êµ¬ì¶• (ê°€ì¹˜íˆ¬ì ê´€ì )"""
        
        # ğŸŸ¢ ê¸ì • ê°ì • ì‚¬ì „ (ê°€ì¹˜íˆ¬ì í˜¸ì¬)
        self.positive_words = {
            # ğŸ“Š í€ë”ë©˜í„¸ ê°•í™” (ê°€ì¥ ì¤‘ìš”)
            'fundamental_strong': {
                'words': [
                    'ì‹¤ì ê°œì„ ', 'ë§¤ì¶œì¦ê°€', 'ì˜ì—…ì´ìµ', 'ìˆœì´ìµì¦ê°€', 'ì´ìµë¥ ê°œì„ ',
                    'ROEìƒìŠ¹', 'ìê¸°ìë³¸ì´ìµë¥ ', 'ë¶€ì±„ê°ì†Œ', 'ì¬ë¬´ê±´ì „ì„±', 'ìœ ë™ì„±ê°œì„ ',
                    'ì˜ì—…í˜„ê¸ˆíë¦„', 'ì‰ì—¬í˜„ê¸ˆ', 'ìë³¸í™•ì¶©', 'ì¬ë¬´êµ¬ì¡°ê°œì„ ', 'ì‹ ìš©ë“±ê¸‰ìƒí–¥'
                ],
                'weight': 3.0  # ìµœê³  ê°€ì¤‘ì¹˜
            },
            
            # ğŸ­ ì‚¬ì—…ëª¨ë¸ ê°•í™”
            'business_strong': {
                'words': [
                    'ì‹ ì‚¬ì—…', 'ì‚¬ì—…í™•ì¥', 'ì‹œì¥ì ìœ ìœ¨', 'ê²½ìŸìš°ìœ„', 'ë¸Œëœë“œê°€ì¹˜',
                    'íŠ¹í—ˆì·¨ë“', 'ê¸°ìˆ ë ¥', 'ì—°êµ¬ê°œë°œ', 'í˜ì‹ ', 'ì°¨ë³„í™”',
                    'ì‹œì¥ì§„ì…', 'í•´ì™¸ì§„ì¶œ', 'ì‹ ê·œê³ ê°', 'ê³„ì•½ì²´ê²°', 'íŒŒíŠ¸ë„ˆì‹­'
                ],
                'weight': 2.5
            },
            
            # ğŸ’¼ ê²½ì˜ í’ˆì§ˆ
            'management_quality': {
                'words': [
                    'ê²½ì˜ì§„êµì²´', 'ì „ë¬¸ê²½ì˜', 'íˆ¬ëª…ê²½ì˜', 'ì§€ë°°êµ¬ì¡°ê°œì„ ', 'ESG',
                    'ë°°ë‹¹ì¦ì•¡', 'ë°°ë‹¹ì •ì±…', 'ì£¼ì£¼í™˜ì›', 'ìì‚¬ì£¼ë§¤ì…', 'ê°ì',
                    'êµ¬ì¡°ì¡°ì •ì™„ë£Œ', 'íš¨ìœ¨ì„±ê°œì„ ', 'ë¹„ìš©ì ˆê°', 'ìƒì‚°ì„±í–¥ìƒ'
                ],
                'weight': 2.0
            },
            
            # ğŸ“ˆ ì„±ì¥ ë™ë ¥
            'growth_drivers': {
                'words': [
                    'ë§¤ì¶œí™•ëŒ€', 'ìˆ˜ì£¼ì¦ê°€', 'ì£¼ë¬¸ê¸‰ì¦', 'ë°±ë¡œê·¸', 'íŒŒì´í”„ë¼ì¸',
                    'ì‹ ì œí’ˆì¶œì‹œ', 'ì œí’ˆí¬íŠ¸í´ë¦¬ì˜¤', 'ê³ ë¶€ê°€ê°€ì¹˜', 'í”„ë¦¬ë¯¸ì—„',
                    'ê¸€ë¡œë²Œì§„ì¶œ', 'ìˆ˜ì¶œì¦ê°€', 'ì‹œì¥í™•ëŒ€', 'ê³ ê°ê¸°ë°˜í™•ì¥'
                ],
                'weight': 2.0
            }
        }
        
        # ğŸ”´ ë¶€ì • ê°ì • ì‚¬ì „ (ê°€ì¹˜íˆ¬ì ì•…ì¬)
        self.negative_words = {
            # ğŸ“‰ í€ë”ë©˜í„¸ ì•…í™” (ê°€ì¥ ìœ„í—˜)
            'fundamental_weak': {
                'words': [
                    'ì‹¤ì ì•…í™”', 'ë§¤ì¶œê°ì†Œ', 'ì ìì „í™˜', 'ì†ì‹¤í™•ëŒ€', 'ì´ìµë¥ í•˜ë½',
                    'ROEí•˜ë½', 'ë¶€ì±„ì¦ê°€', 'ì¬ë¬´ì•…í™”', 'ìœ ë™ì„±ìœ„ê¸°', 'ìê¸ˆë‚œ',
                    'í˜„ê¸ˆíë¦„ì•…í™”', 'ì‹ ìš©ë“±ê¸‰í•˜í–¥', 'ë¶€ì‹¤', 'êµ¬ì¡°ì¡°ì •', 'ì •ë¦¬í•´ê³ '
                ],
                'weight': 3.0  # ìµœê³  ìœ„í—˜ ê°€ì¤‘ì¹˜
            },
            
            # ğŸ­ ì‚¬ì—…ëª¨ë¸ ìœ„ê¸°  
            'business_risk': {
                'words': [
                    'ì‹œì¥ì¶•ì†Œ', 'ì ìœ ìœ¨í•˜ë½', 'ê²½ìŸì‹¬í™”', 'ê°€ê²©ê²½ìŸ', 'ë§ˆì§„ì••ë°•',
                    'ê¸°ìˆ ë‚™í›„', 'íŠ¹í—ˆë¶„ìŸ', 'ì†Œì†¡', 'ê·œì œê°•í™”', 'ì œì¬',
                    'ê³ ê°ì´íƒˆ', 'ê³„ì•½í•´ì§€', 'ì‚¬ì—…ì² ìˆ˜', 'ê³µì¥íì‡„', 'ê°ì‚°'
                ],
                'weight': 2.5
            },
            
            # ğŸ’¼ ê²½ì˜ ë¦¬ìŠ¤í¬
            'management_risk': {
                'words': [
                    'ê²½ì˜ì§„ê°ˆë“±', 'ì§€ë°°êµ¬ì¡°', 'íš¡ë ¹', 'ë°°ì„', 'ë¶„ì‹íšŒê³„',
                    'ê°ì‚¬ì˜ê²¬', 'ì™¸ë¶€ê°ì‚¬', 'ê¸ˆìœµê°ë…ì›', 'ê²€ì°°ìˆ˜ì‚¬', 'ê¸°ì†Œ',
                    'ë°°ë‹¹ì¤‘ë‹¨', 'ë¬´ë°°', 'ì£¼ì£¼ê°ˆë“±', 'ê²½ì˜ê¶Œë¶„ìŸ'
                ],
                'weight': 2.0
            },
            
            # ğŸ“‰ ì‹œì¥ ë¦¬ìŠ¤í¬
            'market_risk': {
                'words': [
                    'ê²½ê¸°ì¹¨ì²´', 'ë¶ˆí™©', 'ê¸ˆë¦¬ì¸ìƒ', 'ì¸í”Œë ˆì´ì…˜', 'í™˜ìœ¨ê¸‰ë“±',
                    'ì›ìì¬ê°€ê²©', 'ìœ ê°€ê¸‰ë“±', 'ê³µê¸‰ë§ì°¨ì§ˆ', 'íŒ¬ë°ë¯¹', 'ì§€ì •í•™ì ë¦¬ìŠ¤í¬',
                    'ë¬´ì—­ë¶„ìŸ', 'ê´€ì„¸', 'ìˆ˜ì¶œê·œì œ', 'ê²½ì œì œì¬'
                ],
                'weight': 1.5
            }
        }
        
        # ğŸ”„ ì¤‘ë¦½ í‚¤ì›Œë“œ (ë…¸ì´ì¦ˆ í•„í„°ë§ìš©)
        self.neutral_noise_words = {
            'ì£¼ê°€', 'ì‹œì„¸', 'ì°¨íŠ¸', 'ê¸°ìˆ ì ', 'ì´í‰ì„ ', 'ê±°ë˜ëŸ‰', 
            'ë§¤ìˆ˜ì¶”ì²œ', 'ë§¤ë„ì¶”ì²œ', 'ëª©í‘œì£¼ê°€', 'ì£¼ì‹', 'ì¦ê¶Œ',
            'ë‹¨íƒ€', 'ìŠ¤ìœ™', 'ë°ì´íŠ¸ë ˆì´ë”©', 'ê¸‰ë“±ì£¼', 'ê¸‰ë½ì£¼'
        }
        
        logger.info("ğŸ“Š ì›ŒëŸ° ë²„í• ìŠ¤íƒ€ì¼ ê°ì • ì‚¬ì „ êµ¬ì¶• ì™„ë£Œ")
        logger.info(f"   ğŸŸ¢ ê¸ì • ì¹´í…Œê³ ë¦¬: {len(self.positive_words)}ê°œ")
        logger.info(f"   ğŸ”´ ë¶€ì • ì¹´í…Œê³ ë¦¬: {len(self.negative_words)}ê°œ")
    
    def _validate_database_schema(self) -> bool:
        """ë°ì´í„°ë² ì´ìŠ¤ ìŠ¤í‚¤ë§ˆ ê²€ì¦"""
        
        if not self.db_path.exists():
            logger.error(f"âŒ ë°ì´í„°ë² ì´ìŠ¤ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {self.db_path}")
            return False
        
        try:
            with sqlite3.connect(self.db_path) as conn:
                cursor = conn.cursor()
                
                # news_articles í…Œì´ë¸” ì¡´ì¬ í™•ì¸
                cursor.execute("SELECT name FROM sqlite_master WHERE type='table' AND name='news_articles'")
                if not cursor.fetchone():
                    logger.error("âŒ news_articles í…Œì´ë¸”ì´ ì—†ìŠµë‹ˆë‹¤")
                    return False
                
                # í•„ìš”í•œ ì»¬ëŸ¼ë“¤ í™•ì¸
                cursor.execute("PRAGMA table_info(news_articles)")
                existing_columns = [row[1] for row in cursor.fetchall()]
                
                required_columns = ['sentiment_score', 'sentiment_label', 'news_category', 'long_term_relevance']
                missing_columns = [col for col in required_columns if col not in existing_columns]
                
                if missing_columns:
                    logger.error(f"âŒ ëˆ„ë½ëœ ì»¬ëŸ¼: {missing_columns}")
                    return False
                
                logger.info("âœ… ë°ì´í„°ë² ì´ìŠ¤ ìŠ¤í‚¤ë§ˆ ê²€ì¦ ì™„ë£Œ")
                return True
                
        except Exception as e:
            logger.error(f"âŒ ë°ì´í„°ë² ì´ìŠ¤ ìŠ¤í‚¤ë§ˆ ê²€ì¦ ì‹¤íŒ¨: {e}")
            return False
    
    def categorize_news_content(self, title: str, content: str) -> str:
        """ë‰´ìŠ¤ ë‚´ìš©ì„ ê°€ì¹˜íˆ¬ì ê´€ì ìœ¼ë¡œ ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜"""
        
        text = f"{title} {content}".lower()
        
        # í€ë”ë©˜í„¸ ë‰´ìŠ¤ (ìµœìš°ì„ )
        fundamental_keywords = [
            'ì‹¤ì ', 'ë§¤ì¶œ', 'ì˜ì—…ì´ìµ', 'ìˆœì´ìµ', 'ì¬ë¬´ì œí‘œ', 'roe', 'ë¶€ì±„ë¹„ìœ¨',
            'ë°°ë‹¹', 'ê°ì‚¬ë³´ê³ ì„œ', 'ê³µì‹œ', 'ì‚¬ì—…ë³´ê³ ì„œ', 'ë¶„ê¸°ì‹¤ì ', 'ì—°ê°„ì‹¤ì '
        ]
        
        if any(keyword in text for keyword in fundamental_keywords):
            return 'fundamental'
        
        # ì‚¬ì—… ê´€ë ¨ ë‰´ìŠ¤
        business_keywords = [
            'ì‹ ì‚¬ì—…', 'ì‚¬ì—…í™•ì¥', 'ì¸ìˆ˜í•©ë³‘', 'ì „ëµì ì œíœ´', 'íŒŒíŠ¸ë„ˆì‹­',
            'ì‹ ì œí’ˆ', 'ì—°êµ¬ê°œë°œ', 'íŠ¹í—ˆ', 'ê¸°ìˆ ê°œë°œ', 'ì‹œì¥ì§„ì¶œ'
        ]
        
        if any(keyword in text for keyword in business_keywords):
            return 'business'
        
        # ì¬ë¬´ ê´€ë ¨ ë‰´ìŠ¤
        financial_keywords = [
            'ìê¸ˆì¡°ë‹¬', 'íˆ¬ììœ ì¹˜', 'ì±„ê¶Œë°œí–‰', 'ëŒ€ì¶œ', 'ì‹ ìš©ë“±ê¸‰',
            'ì¬ë¬´êµ¬ì¡°', 'í˜„ê¸ˆíë¦„', 'ìœ ë™ì„±', 'ìë³¸ê¸ˆ', 'ì¦ì'
        ]
        
        if any(keyword in text for keyword in financial_keywords):
            return 'financial'
        
        # ê²½ì˜ì§„ ê´€ë ¨ ë‰´ìŠ¤
        management_keywords = [
            'ëŒ€í‘œì´ì‚¬', 'ceo', 'ê²½ì˜ì§„', 'ì„ì›', 'ì‚¬ì¥', 'íšŒì¥',
            'ì´ì‚¬íšŒ', 'ì£¼ì£¼ì´íšŒ', 'ì§€ë°°êµ¬ì¡°', 'ê²½ì˜ê¶Œ', 'ìŠ¹ê³„'
        ]
        
        if any(keyword in text for keyword in management_keywords):
            return 'management'
        
        # ê¸°ìˆ ì /ì°¨íŠ¸ ë‰´ìŠ¤ (ë‚®ì€ ê°€ì¤‘ì¹˜)
        technical_keywords = [
            'ì°¨íŠ¸', 'ê¸°ìˆ ì ', 'ì´í‰ì„ ', 'ì§€ì§€ì„ ', 'ì €í•­ì„ ', 'ëŒíŒŒ',
            'ëª©í‘œì£¼ê°€', 'ë§¤ìˆ˜ì¶”ì²œ', 'ë§¤ë„ì¶”ì²œ', 'rsi', 'macd'
        ]
        
        if any(keyword in text for keyword in technical_keywords):
            return 'technical'
        
        # ì‹œì¥ ì¼ë°˜ ë‰´ìŠ¤
        market_keywords = [
            'ì½”ìŠ¤í”¼', 'ì½”ìŠ¤ë‹¥', 'ì¦ì‹œ', 'ì£¼ì‹ì‹œì¥', 'ê²½ê¸°', 'ê¸ˆë¦¬',
            'í™˜ìœ¨', 'ìœ ê°€', 'ì›ìì¬', 'ì¸í”Œë ˆì´ì…˜'
        ]
        
        if any(keyword in text for keyword in market_keywords):
            return 'market'
        
        # ê¸°ë³¸ê°’: ë…¸ì´ì¦ˆë¡œ ë¶„ë¥˜
        return 'noise'
    
    def calculate_buffett_sentiment_score(self, title: str, content: str, description: str = "") -> Dict:
        """ì›ŒëŸ° ë²„í• ìŠ¤íƒ€ì¼ ê°ì • ì ìˆ˜ ê³„ì‚°"""
        
        # ì „ì²´ í…ìŠ¤íŠ¸ ê²°í•© ë° ì „ì²˜ë¦¬
        full_text = f"{title} {description} {content}".lower()
        full_text = re.sub(r'[^\w\s]', ' ', full_text)  # íŠ¹ìˆ˜ë¬¸ì ì œê±°
        full_text = ' '.join(full_text.split())  # ê³µë°± ì •ë¦¬
        
        # ë‰´ìŠ¤ ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜
        news_category = self.categorize_news_content(title, content)
        category_weight = self.news_weights.get(news_category, 0.5)
        
        # ê°ì • ì ìˆ˜ ê³„ì‚°
        sentiment_scores = {
            'positive': 0.0,
            'negative': 0.0,
            'neutral': 0.0
        }
        
        detail_scores = {
            'positive_details': defaultdict(list),
            'negative_details': defaultdict(list)
        }
        
        # ê¸ì • ê°ì • ë¶„ì„
        for category, data in self.positive_words.items():
            for word in data['words']:
                if word in full_text:
                    score = data['weight'] * category_weight
                    sentiment_scores['positive'] += score
                    detail_scores['positive_details'][category].append({
                        'word': word,
                        'score': score
                    })
        
        # ë¶€ì • ê°ì • ë¶„ì„
        for category, data in self.negative_words.items():
            for word in data['words']:
                if word in full_text:
                    score = data['weight'] * category_weight
                    sentiment_scores['negative'] += score
                    detail_scores['negative_details'][category].append({
                        'word': word,
                        'score': score
                    })
        
        # ì¤‘ë¦½/ë…¸ì´ì¦ˆ ì²´í¬
        noise_count = sum(1 for word in self.neutral_noise_words if word in full_text)
        if noise_count > 0:
            sentiment_scores['neutral'] = noise_count * 0.1
        
        # ìµœì¢… ê°ì • ì ìˆ˜ ê³„ì‚° (-1.0 ~ 1.0)
        total_positive = sentiment_scores['positive']
        total_negative = sentiment_scores['negative']
        total_sentiment = total_positive + total_negative + sentiment_scores['neutral']
        
        if total_sentiment > 0:
            final_score = (total_positive - total_negative) / total_sentiment
        else:
            final_score = 0.0
        
        # ê°ì • ë¼ë²¨ ê²°ì • (ì›ŒëŸ° ë²„í• ê´€ì )
        if final_score > 0.3:
            sentiment_label = 'bullish'  # ì¥ê¸° ìƒìŠ¹ ì „ë§
        elif final_score < -0.3:
            sentiment_label = 'bearish'  # ì¥ê¸° í•˜ë½ ìœ„í—˜
        elif final_score > 0.1:
            sentiment_label = 'positive'  # ì†Œí­ ê¸ì •
        elif final_score < -0.1:
            sentiment_label = 'negative'  # ì†Œí­ ë¶€ì •
        else:
            sentiment_label = 'neutral'   # ì¤‘ë¦½
        
        # ì¥ê¸° íˆ¬ì ê´€ë ¨ì„± ì ìˆ˜ (0~100)
        long_term_relevance = self._calculate_long_term_relevance(
            news_category, total_positive, total_negative
        )
        
        return {
            'sentiment_score': round(final_score, 4),
            'sentiment_label': sentiment_label,
            'news_category': news_category,
            'category_weight': category_weight,
            'long_term_relevance': long_term_relevance,
            'positive_score': round(total_positive, 2),
            'negative_score': round(total_negative, 2),
            'detail_analysis': detail_scores
        }
    
    def _calculate_long_term_relevance(self, category: str, pos_score: float, neg_score: float) -> int:
        """ì¥ê¸° íˆ¬ì ê´€ë ¨ì„± ì ìˆ˜ ê³„ì‚° (0~100)"""
        
        # ì¹´í…Œê³ ë¦¬ë³„ ê¸°ë³¸ ì ìˆ˜
        base_scores = {
            'fundamental': 90,  # í€ë”ë©˜í„¸ì€ ì¥ê¸° íˆ¬ìì— ê°€ì¥ ì¤‘ìš”
            'business': 80,     # ì‚¬ì—… ëª¨ë¸ ë³€í™”ë„ ì¤‘ìš”
            'financial': 85,    # ì¬ë¬´ ìƒí™©ë„ ì¤‘ìš”
            'management': 70,   # ê²½ì˜ì§„ë„ ì¥ê¸°ì ìœ¼ë¡œ ì¤‘ìš”
            'market': 40,       # ì‹œì¥ ì¼ë°˜ì€ ë³´í†µ
            'technical': 20,    # ê¸°ìˆ ì  ë¶„ì„ì€ ì¥ê¸° íˆ¬ìì— ëœ ì¤‘ìš”
            'noise': 10        # ë…¸ì´ì¦ˆëŠ” ê±°ì˜ ë¬´ê´€
        }
        
        base_score = base_scores.get(category, 30)
        
        # ê°ì • ê°•ë„ ë³´ë„ˆìŠ¤/í˜ë„í‹°
        intensity = abs(pos_score - neg_score)
        if intensity > 5.0:
            base_score += 20  # ê°•í•œ ê°ì •ì€ ë” ì¤‘ìš”
        elif intensity > 2.0:
            base_score += 10
        elif intensity < 0.5:
            base_score -= 10  # ì•½í•œ ê°ì •ì€ ëœ ì¤‘ìš”
        
        return max(0, min(100, base_score))
    
    def analyze_news_batch(self, limit: int = None) -> pd.DataFrame:
        """ìˆ˜ì§‘ëœ ë‰´ìŠ¤ ë°°ì¹˜ ê°ì • ë¶„ì„"""
        
        logger.info("ğŸ” ì›ŒëŸ° ë²„í• ìŠ¤íƒ€ì¼ ë‰´ìŠ¤ ê°ì • ë¶„ì„ ì‹œì‘")
        
        try:
            with sqlite3.connect(self.db_path) as conn:
                # ê°ì • ë¶„ì„ì´ ì•ˆëœ ë‰´ìŠ¤ë“¤ ì¡°íšŒ
                query = """
                    SELECT id, stock_code, stock_name, title, content, description, 
                           pub_date, source, collected_at
                    FROM news_articles
                    WHERE sentiment_score IS NULL OR sentiment_score = 0.0
                    ORDER BY collected_at DESC
                """
                
                if limit:
                    query += f" LIMIT {limit}"
                
                news_df = pd.read_sql_query(query, conn)
                
                if news_df.empty:
                    logger.info("âœ… ëª¨ë“  ë‰´ìŠ¤ê°€ ì´ë¯¸ ê°ì • ë¶„ì„ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤")
                    return pd.DataFrame()
                
                logger.info(f"ğŸ“Š ê°ì • ë¶„ì„ ëŒ€ìƒ: {len(news_df):,}ê±´")
                
                # ê°ì • ë¶„ì„ ì‹¤í–‰
                results = []
                cursor = conn.cursor()
                
                progress_bar = tqdm(news_df.iterrows(), 
                                  total=len(news_df),
                                  desc="ğŸ” ì›ŒëŸ° ë²„í• ê°ì •ë¶„ì„",
                                  unit="ë‰´ìŠ¤")
                
                for idx, row in progress_bar:
                    try:
                        # ê°ì • ë¶„ì„ ìˆ˜í–‰
                        sentiment_result = self.calculate_buffett_sentiment_score(
                            title=row['title'] or "",
                            content=row['content'] or "",
                            description=row['description'] or ""
                        )
                        
                        # ê²°ê³¼ ì €ì¥
                        results.append({
                            'id': row['id'],
                            'stock_code': row['stock_code'],
                            'sentiment_score': sentiment_result['sentiment_score'],
                            'sentiment_label': sentiment_result['sentiment_label'],
                            'news_category': sentiment_result['news_category'],
                            'long_term_relevance': sentiment_result['long_term_relevance'],
                            **sentiment_result
                        })
                        
                        # DB ì—…ë°ì´íŠ¸
                        cursor.execute('''
                            UPDATE news_articles 
                            SET sentiment_score = ?, 
                                sentiment_label = ?,
                                news_category = ?,
                                long_term_relevance = ?
                            WHERE id = ?
                        ''', (
                            sentiment_result['sentiment_score'],
                            sentiment_result['sentiment_label'], 
                            sentiment_result['news_category'],
                            sentiment_result['long_term_relevance'],
                            row['id']
                        ))
                        
                        progress_bar.set_postfix({
                            'Label': sentiment_result['sentiment_label'][:4],
                            'Category': sentiment_result['news_category'][:4],
                            'Score': f"{sentiment_result['sentiment_score']:.2f}"
                        })
                        
                    except Exception as e:
                        logger.error(f"ë‰´ìŠ¤ ID {row['id']} ë¶„ì„ ì‹¤íŒ¨: {e}")
                        continue
                
                conn.commit()
                logger.info(f"âœ… ê°ì • ë¶„ì„ ì™„ë£Œ: {len(results):,}ê±´")
                
                return pd.DataFrame(results)
                
        except Exception as e:
            logger.error(f"âŒ ë°°ì¹˜ ê°ì • ë¶„ì„ ì‹¤íŒ¨: {e}")
            return pd.DataFrame()
    
    def calculate_daily_sentiment_index(self, days: int = 30) -> pd.DataFrame:
        """ì¼ë³„ ì¢…ëª©ë³„ ì›ŒëŸ° ë²„í• ê°ì • ì§€ìˆ˜ ê³„ì‚°"""
        
        logger.info(f"ğŸ“ˆ ìµœê·¼ {days}ì¼ ì¼ë³„ ê°ì • ì§€ìˆ˜ ê³„ì‚° ì¤‘...")
        
        try:
            with sqlite3.connect(self.db_path) as conn:
                # ìµœê·¼ Nì¼ê°„ ê°ì • ë¶„ì„ëœ ë‰´ìŠ¤ ì¡°íšŒ
                query = """
                    SELECT 
                        stock_code,
                        stock_name,
                        DATE(pub_date) as date,
                        sentiment_score,
                        sentiment_label,
                        news_category,
                        long_term_relevance,
                        COUNT(*) as news_count
                    FROM news_articles
                    WHERE sentiment_score IS NOT NULL
                    AND DATE(pub_date) >= DATE('now', '-{} days')
                    GROUP BY stock_code, stock_name, DATE(pub_date), 
                             sentiment_score, sentiment_label, news_category, long_term_relevance
                    ORDER BY stock_code, date DESC
                """.format(days)
                
                news_data = pd.read_sql_query(query, conn)
                
                if news_data.empty:
                    logger.warning("âŒ ê°ì • ë¶„ì„ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤")
                    return pd.DataFrame()
                
                # ì¼ë³„ ì¢…ëª©ë³„ ì§‘ê³„
                daily_sentiment = []
                
                for (stock_code, stock_name, date), group in news_data.groupby(['stock_code', 'stock_name', 'date']):
                    
                    # ì›ŒëŸ° ë²„í• ìŠ¤íƒ€ì¼ ê°€ì¤‘ í‰ê·  ê³„ì‚°
                    weighted_scores = []
                    category_counts = defaultdict(int)
                    
                    for _, row in group.iterrows():
                        # ì¥ê¸° íˆ¬ì ê´€ë ¨ì„±ìœ¼ë¡œ ê°€ì¤‘ì¹˜ ì ìš©
                        weight = row['long_term_relevance'] / 100.0
                        weighted_score = row['sentiment_score'] * weight * row['news_count']
                        weighted_scores.append(weighted_score)
                        category_counts[row['news_category']] += row['news_count']
                    
                    # ìµœì¢… ê°ì • ì§€ìˆ˜ ê³„ì‚°
                    if weighted_scores:
                        daily_score = np.mean(weighted_scores)
                    else:
                        daily_score = 0.0
                    
                    # ê°ì • ì§€ìˆ˜ë¥¼ 0~100 ë²”ìœ„ë¡œ ë³€í™˜ (50ì´ ì¤‘ë¦½)
                    sentiment_index = 50 + (daily_score * 25)  # -1~1 -> 25~75 ë²”ìœ„
                    sentiment_index = max(0, min(100, sentiment_index))
                    
                    # ì‹ ë¢°ë„ ê³„ì‚° (ë‰´ìŠ¤ ê°œìˆ˜ì™€ ì¹´í…Œê³ ë¦¬ ë‹¤ì–‘ì„± ê¸°ë°˜)
                    total_news = group['news_count'].sum()
                    category_diversity = len(category_counts)
                    confidence = min(100, total_news * 10 + category_diversity * 5)
                    
                    daily_sentiment.append({
                        'stock_code': stock_code,
                        'stock_name': stock_name,
                        'date': date,
                        'sentiment_index': round(sentiment_index, 2),
                        'sentiment_score': round(daily_score, 4),
                        'total_news': total_news,
                        'confidence': confidence,
                        'fundamental_news': category_counts.get('fundamental', 0),
                        'business_news': category_counts.get('business', 0),
                        'technical_news': category_counts.get('technical', 0),
                        'noise_news': category_counts.get('noise', 0)
                    })
                
                # ê²°ê³¼ë¥¼ DBì— ì €ì¥
                daily_df = pd.DataFrame(daily_sentiment)
                
                if not daily_df.empty:
                    cursor = conn.cursor()
                    
                    # ê¸°ì¡´ í…Œì´ë¸”ì´ ì—†ìœ¼ë©´ ìƒì„±
                    cursor.execute('''
                        CREATE TABLE IF NOT EXISTS daily_sentiment_index (
                            id INTEGER PRIMARY KEY AUTOINCREMENT,
                            stock_code TEXT NOT NULL,
                            stock_name TEXT NOT NULL,
                            date TEXT NOT NULL,
                            sentiment_index REAL NOT NULL,
                            sentiment_score REAL NOT NULL,
                            total_news INTEGER NOT NULL,
                            confidence INTEGER NOT NULL,
                            fundamental_news INTEGER DEFAULT 0,
                            business_news INTEGER DEFAULT 0,
                            technical_news INTEGER DEFAULT 0,
                            noise_news INTEGER DEFAULT 0,
                            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                            UNIQUE(stock_code, date)
                        )
                    ''')
                    
                    # ë°ì´í„° ì‚½ì…
                    for _, row in daily_df.iterrows():
                        cursor.execute('''
                            INSERT OR REPLACE INTO daily_sentiment_index
                            (stock_code, stock_name, date, sentiment_index, sentiment_score,
                             total_news, confidence, fundamental_news, business_news, 
                             technical_news, noise_news)
                            VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                        ''', (
                            row['stock_code'], row['stock_name'], row['date'],
                            row['sentiment_index'], row['sentiment_score'],
                            row['total_news'], row['confidence'],
                            row['fundamental_news'], row['business_news'],
                            row['technical_news'], row['noise_news']
                        ))
                    
                    conn.commit()
                    logger.info(f"âœ… ì¼ë³„ ê°ì • ì§€ìˆ˜ ê³„ì‚° ì™„ë£Œ: {len(daily_df):,}ê±´")
                
                return daily_df
                
        except Exception as e:
            logger.error(f"âŒ ì¼ë³„ ê°ì • ì§€ìˆ˜ ê³„ì‚° ì‹¤íŒ¨: {e}")
            return pd.DataFrame()
    
    def get_buffett_sentiment_summary(self) -> Dict:
        """ì›ŒëŸ° ë²„í• ìŠ¤íƒ€ì¼ ê°ì • ë¶„ì„ ê²°ê³¼ ìš”ì•½"""
        
        try:
            with sqlite3.connect(self.db_path) as conn:
                
                # ì „ì²´ ë‰´ìŠ¤ ê°ì • ë¶„í¬
                sentiment_dist = pd.read_sql_query("""
                    SELECT 
                        sentiment_label,
                        news_category,
                        COUNT(*) as count,
                        AVG(sentiment_score) as avg_score,
                        AVG(long_term_relevance) as avg_relevance
                    FROM news_articles
                    WHERE sentiment_score IS NOT NULL
                    GROUP BY sentiment_label, news_category
                    ORDER BY count DESC
                """, conn)
                
                # ì¢…ëª©ë³„ ê°ì • ì ìˆ˜ (í€ë”ë©˜í„¸ ë‰´ìŠ¤ ìœ„ì£¼)
                stock_sentiment = pd.read_sql_query("""
                    SELECT 
                        stock_code,
                        stock_name,
                        COUNT(*) as total_news,
                        COUNT(CASE WHEN news_category = 'fundamental' THEN 1 END) as fundamental_news,
                        AVG(sentiment_score) as avg_sentiment,
                        AVG(long_term_relevance) as avg_relevance,
                        AVG(CASE WHEN news_category = 'fundamental' THEN sentiment_score END) as fundamental_sentiment
                    FROM news_articles
                    WHERE sentiment_score IS NOT NULL
                    GROUP BY stock_code, stock_name
                    HAVING COUNT(*) >= 3
                    ORDER BY fundamental_sentiment DESC NULLS LAST, avg_sentiment DESC
                """, conn)
                
                # ìµœê·¼ 7ì¼ ê°ì • íŠ¸ë Œë“œ
                recent_trend = pd.read_sql_query("""
                    SELECT 
                        DATE(pub_date) as date,
                        COUNT(*) as total_news,
                        AVG(sentiment_score) as avg_sentiment,
                        COUNT(CASE WHEN news_category = 'fundamental' THEN 1 END) as fundamental_count
                    FROM news_articles
                    WHERE sentiment_score IS NOT NULL
                    AND DATE(pub_date) >= DATE('now', '-7 days')
                    GROUP BY DATE(pub_date)
                    ORDER BY date DESC
                """, conn)
                
                return {
                    'sentiment_distribution': sentiment_dist,
                    'stock_sentiment_ranking': stock_sentiment,
                    'recent_trend': recent_trend,
                    'analysis_timestamp': datetime.now().isoformat()
                }
                
        except Exception as e:
            logger.error(f"âŒ ê°ì • ë¶„ì„ ìš”ì•½ ì‹¤íŒ¨: {e}")
            return {}
    
    def get_investment_signals(self, top_n: int = 20) -> pd.DataFrame:
        """ì›ŒëŸ° ë²„í• ìŠ¤íƒ€ì¼ íˆ¬ì ì‹ í˜¸ ìƒì„±"""
        
        logger.info(f"ğŸ¯ ì›ŒëŸ° ë²„í• íˆ¬ì ì‹ í˜¸ ìƒì„± (ìƒìœ„ {top_n}ê°œ)")
        
        try:
            with sqlite3.connect(self.db_path) as conn:
                
                # ìµœê·¼ 7ì¼ê°„ í€ë”ë©˜í„¸ ë‰´ìŠ¤ ê¸°ë°˜ ì‹ í˜¸
                query = """
                    SELECT 
                        na.stock_code,
                        na.stock_name,
                        COUNT(*) as total_news,
                        COUNT(CASE WHEN na.news_category = 'fundamental' THEN 1 END) as fundamental_news,
                        AVG(na.sentiment_score) as avg_sentiment,
                        AVG(CASE WHEN na.news_category = 'fundamental' THEN na.sentiment_score END) as fundamental_sentiment,
                        AVG(na.long_term_relevance) as avg_relevance,
                        MAX(na.sentiment_score) as max_sentiment,
                        MIN(na.sentiment_score) as min_sentiment
                    FROM news_articles na
                    WHERE na.sentiment_score IS NOT NULL
                    AND DATE(na.pub_date) >= DATE('now', '-7 days')
                    GROUP BY na.stock_code, na.stock_name
                    HAVING fundamental_news >= 1  -- í€ë”ë©˜í„¸ ë‰´ìŠ¤ ìµœì†Œ 1ê°œ
                    ORDER BY fundamental_sentiment DESC NULLS LAST, avg_relevance DESC
                    LIMIT ?
                """
                
                signals_df = pd.read_sql_query(query, conn, params=(top_n * 2,))
                
                if signals_df.empty:
                    logger.warning("âŒ íˆ¬ì ì‹ í˜¸ ìƒì„±í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤")
                    return pd.DataFrame()
                
                # ì‹ í˜¸ ê°•ë„ ê³„ì‚°
                signals_df['signal_strength'] = (
                    signals_df['fundamental_sentiment'].fillna(0) * 0.7 +
                    signals_df['avg_sentiment'] * 0.3
                ) * (signals_df['avg_relevance'] / 100)
                
                # ì‹ í˜¸ íƒ€ì… ê²°ì •
                def get_signal_type(row):
                    fund_sent = row['fundamental_sentiment'] or 0
                    if fund_sent > 0.3:
                        return 'STRONG_BUY'
                    elif fund_sent > 0.1:
                        return 'BUY'
                    elif fund_sent < -0.3:
                        return 'STRONG_SELL'
                    elif fund_sent < -0.1:
                        return 'SELL'
                    else:
                        return 'HOLD'
                
                signals_df['signal_type'] = signals_df.apply(get_signal_type, axis=1)
                
                # ì‹ ë¢°ë„ ê³„ì‚°
                signals_df['confidence'] = np.minimum(
                    100,
                    signals_df['fundamental_news'] * 30 + 
                    signals_df['total_news'] * 5 +
                    signals_df['avg_relevance'] * 0.5
                )
                
                # ìƒìœ„ Nê°œ ì„ íƒ
                final_signals = signals_df.nlargest(top_n, 'signal_strength')
                
                logger.info(f"âœ… íˆ¬ì ì‹ í˜¸ ìƒì„± ì™„ë£Œ: {len(final_signals)}ê°œ")
                
                return final_signals[['stock_code', 'stock_name', 'signal_type', 
                                    'signal_strength', 'confidence', 'fundamental_sentiment',
                                    'fundamental_news', 'total_news', 'avg_relevance']]
                
        except Exception as e:
            logger.error(f"âŒ íˆ¬ì ì‹ í˜¸ ìƒì„± ì‹¤íŒ¨: {e}")
            return pd.DataFrame()


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    
    print("\n" + "="*80)
    print("ğŸ¯ ì›ŒëŸ° ë²„í• ìŠ¤íƒ€ì¼ ë‰´ìŠ¤ ê°ì • ë¶„ì„ ì—”ì§„")
    print("="*80)
    print("ğŸ“Š ê¸°ë³¸ë¶„ì„(45%) : ê¸°ìˆ ë¶„ì„(30%) : ë‰´ìŠ¤ê°ì •ë¶„ì„(25%)")
    print("ğŸ¯ ê°€ì¹˜íˆ¬ì ê´€ì  ê°ì • ë¶„ì„ ë° íˆ¬ì ì‹ í˜¸ ìƒì„±")
    print()
    
    # ê°ì • ë¶„ì„ê¸° ì´ˆê¸°í™”
    analyzer = BuffettSentimentAnalyzer()
    
    while True:
        print("\nğŸ” ì›ŒëŸ° ë²„í• ê°ì • ë¶„ì„ ë©”ë‰´:")
        print("1. ë‰´ìŠ¤ ê°ì • ë¶„ì„ ì‹¤í–‰ (ë°°ì¹˜ ì²˜ë¦¬)")
        print("2. ì¼ë³„ ê°ì • ì§€ìˆ˜ ê³„ì‚°")
        print("3. ê°ì • ë¶„ì„ ê²°ê³¼ ìš”ì•½")
        print("4. ì›ŒëŸ° ë²„í• íˆ¬ì ì‹ í˜¸ ìƒì„±")
        print("5. íŠ¹ì • ì¢…ëª© ê°ì • ë¶„ì„ ì¡°íšŒ")
        print("6. í…ŒìŠ¤íŠ¸: ë‹¨ì¼ ë‰´ìŠ¤ ë¶„ì„")
        print("0. ì¢…ë£Œ")
        
        choice = input("\nì„ íƒ (0-6): ").strip()
        
        if choice == '0':
            print("ğŸ‘‹ ì›ŒëŸ° ë²„í• ê°ì • ë¶„ì„ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
            break
            
        elif choice == '1':
            # ë‰´ìŠ¤ ê°ì • ë¶„ì„ ì‹¤í–‰
            limit = input("ë¶„ì„í•  ë‰´ìŠ¤ ìˆ˜ (ì „ì²´: Enter): ").strip()
            limit = int(limit) if limit.isdigit() else None
            
            print(f"\nğŸ” ë‰´ìŠ¤ ê°ì • ë¶„ì„ ì‹œì‘...")
            results = analyzer.analyze_news_batch(limit=limit)
            
            if not results.empty:
                print(f"\nğŸ“Š ë¶„ì„ ê²°ê³¼ ìš”ì•½:")
                print(f"   ì´ ë¶„ì„: {len(results):,}ê±´")
                print(f"   ê°ì • ë¶„í¬:")
                sentiment_counts = results['sentiment_label'].value_counts()
                for label, count in sentiment_counts.items():
                    print(f"     {label}: {count:,}ê±´")
                
                category_counts = results['news_category'].value_counts()
                print(f"   ì¹´í…Œê³ ë¦¬ ë¶„í¬:")
                for category, count in category_counts.items():
                    print(f"     {category}: {count:,}ê±´")
            
        elif choice == '2':
            # ì¼ë³„ ê°ì • ì§€ìˆ˜ ê³„ì‚°
            days = input("ê³„ì‚°í•  ê¸°ê°„ (ì¼, ê¸°ë³¸ê°’: 30): ").strip()
            days = int(days) if days.isdigit() else 30
            
            print(f"\nğŸ“ˆ ìµœê·¼ {days}ì¼ ê°ì • ì§€ìˆ˜ ê³„ì‚°...")
            daily_sentiment = analyzer.calculate_daily_sentiment_index(days=days)
            
            if not daily_sentiment.empty:
                print(f"\nğŸ“Š ì¼ë³„ ê°ì • ì§€ìˆ˜ ìš”ì•½:")
                print(f"   ì´ ë°ì´í„°: {len(daily_sentiment):,}ê±´")
                print(f"   í‰ê·  ê°ì • ì§€ìˆ˜: {daily_sentiment['sentiment_index'].mean():.2f}")
                print(f"   ê°€ì¥ ê¸ì •ì : {daily_sentiment['sentiment_index'].max():.2f}")
                print(f"   ê°€ì¥ ë¶€ì •ì : {daily_sentiment['sentiment_index'].min():.2f}")
                
                print(f"\nğŸ“ˆ ê°ì • ì§€ìˆ˜ ìƒìœ„ 5ê°œ:")
                top_sentiment = daily_sentiment.nlargest(5, 'sentiment_index')
                for _, row in top_sentiment.iterrows():
                    print(f"     {row['stock_name']} ({row['stock_code']}): {row['sentiment_index']:.1f} ({row['date']})")
        
        elif choice == '3':
            # ê°ì • ë¶„ì„ ê²°ê³¼ ìš”ì•½
            print("\nğŸ“Š ì›ŒëŸ° ë²„í• ê°ì • ë¶„ì„ ê²°ê³¼ ìš”ì•½ ìƒì„± ì¤‘...")
            summary = analyzer.get_buffett_sentiment_summary()
            
            if summary:
                # ê°ì • ë¶„í¬
                if not summary['sentiment_distribution'].empty:
                    print(f"\nğŸ¯ ê°ì • ë¶„í¬:")
                    sentiment_dist = summary['sentiment_distribution']
                    for _, row in sentiment_dist.head(10).iterrows():
                        print(f"   {row['sentiment_label']} ({row['news_category']}): {row['count']:,}ê±´ (í‰ê· ì ìˆ˜: {row['avg_score']:.3f})")
                
                # ì¢…ëª©ë³„ ìˆœìœ„
                if not summary['stock_sentiment_ranking'].empty:
                    print(f"\nğŸ† í€ë”ë©˜í„¸ ê°ì • ì ìˆ˜ ìƒìœ„ 10ê°œ:")
                    stock_ranking = summary['stock_sentiment_ranking']
                    for _, row in stock_ranking.head(10).iterrows():
                        fund_sent = row['fundamental_sentiment'] or 0
                        print(f"   {row['stock_name']} ({row['stock_code']}): {fund_sent:.3f} (í€ë”ë©˜í„¸ ë‰´ìŠ¤: {row['fundamental_news']}ê±´)")
                
                # ìµœê·¼ íŠ¸ë Œë“œ
                if not summary['recent_trend'].empty:
                    print(f"\nğŸ“ˆ ìµœê·¼ 7ì¼ ê°ì • íŠ¸ë Œë“œ:")
                    for _, row in summary['recent_trend'].iterrows():
                        print(f"   {row['date']}: í‰ê· ê°ì • {row['avg_sentiment']:.3f}, í€ë”ë©˜í„¸ ë‰´ìŠ¤ {row['fundamental_count']}ê±´")
        
        elif choice == '4':
            # íˆ¬ì ì‹ í˜¸ ìƒì„±
            top_n = input("ìƒì„±í•  ì‹ í˜¸ ìˆ˜ (ê¸°ë³¸ê°’: 20): ").strip()
            top_n = int(top_n) if top_n.isdigit() else 20
            
            print(f"\nğŸ¯ ì›ŒëŸ° ë²„í• íˆ¬ì ì‹ í˜¸ ìƒì„± ì¤‘ (ìƒìœ„ {top_n}ê°œ)...")
            signals = analyzer.get_investment_signals(top_n=top_n)
            
            if not signals.empty:
                print(f"\nğŸš€ ì›ŒëŸ° ë²„í• íˆ¬ì ì‹ í˜¸:")
                print("-" * 100)
                for _, row in signals.iterrows():
                    signal_emoji = {
                        'STRONG_BUY': 'ğŸš€',
                        'BUY': 'ğŸ“ˆ', 
                        'HOLD': 'â¸ï¸',
                        'SELL': 'ğŸ“‰',
                        'STRONG_SELL': 'ğŸ”»'
                    }.get(row['signal_type'], 'â“')
                    
                    print(f"{signal_emoji} {row['stock_name']} ({row['stock_code']})")
                    print(f"   ì‹ í˜¸: {row['signal_type']}")
                    print(f"   ì‹ í˜¸ê°•ë„: {row['signal_strength']:.3f}")
                    print(f"   ì‹ ë¢°ë„: {row['confidence']:.1f}%")
                    print(f"   í€ë”ë©˜í„¸ ê°ì •: {row['fundamental_sentiment']:.3f}")
                    print(f"   ë‰´ìŠ¤: í€ë”ë©˜í„¸ {row['fundamental_news']}ê±´ / ì „ì²´ {row['total_news']}ê±´")
                    print()
            
        elif choice == '5':
            # íŠ¹ì • ì¢…ëª© ì¡°íšŒ
            stock_code = input("ì¢…ëª©ì½”ë“œ ì…ë ¥ (ì˜ˆ: 005930): ").strip()
            
            if stock_code:
                try:
                    with sqlite3.connect(analyzer.db_path) as conn:
                        query = """
                            SELECT stock_name, title, sentiment_score, sentiment_label, 
                                   news_category, long_term_relevance, pub_date
                            FROM news_articles
                            WHERE stock_code = ? AND sentiment_score IS NOT NULL
                            ORDER BY pub_date DESC
                            LIMIT 10
                        """
                        
                        stock_news = pd.read_sql_query(query, conn, params=(stock_code,))
                        
                        if not stock_news.empty:
                            stock_name = stock_news.iloc[0]['stock_name']
                            print(f"\nğŸ“Š {stock_name} ({stock_code}) ìµœê·¼ ë‰´ìŠ¤ ê°ì • ë¶„ì„:")
                            print("-" * 80)
                            
                            for _, row in stock_news.iterrows():
                                print(f"ğŸ“° {row['title'][:50]}...")
                                print(f"   ê°ì •: {row['sentiment_label']} ({row['sentiment_score']:.3f})")
                                print(f"   ì¹´í…Œê³ ë¦¬: {row['news_category']}")
                                print(f"   ì¥ê¸° ê´€ë ¨ì„±: {row['long_term_relevance']}%")
                                print(f"   ë‚ ì§œ: {row['pub_date']}")
                                print()
                        else:
                            print(f"âŒ {stock_code} ì¢…ëª©ì˜ ê°ì • ë¶„ì„ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
                            
                except Exception as e:
                    print(f"âŒ ì¡°íšŒ ì‹¤íŒ¨: {e}")
        
        elif choice == '6':
            # í…ŒìŠ¤íŠ¸: ë‹¨ì¼ ë‰´ìŠ¤ ë¶„ì„
            print("\nğŸ§ª ë‹¨ì¼ ë‰´ìŠ¤ ê°ì • ë¶„ì„ í…ŒìŠ¤íŠ¸")
            title = input("ë‰´ìŠ¤ ì œëª©: ").strip()
            content = input("ë‰´ìŠ¤ ë‚´ìš©: ").strip()
            
            if title or content:
                result = analyzer.calculate_buffett_sentiment_score(title, content)
                
                print(f"\nğŸ“Š ë¶„ì„ ê²°ê³¼:")
                print(f"   ê°ì • ì ìˆ˜: {result['sentiment_score']}")
                print(f"   ê°ì • ë¼ë²¨: {result['sentiment_label']}")
                print(f"   ë‰´ìŠ¤ ì¹´í…Œê³ ë¦¬: {result['news_category']}")
                print(f"   ì¹´í…Œê³ ë¦¬ ê°€ì¤‘ì¹˜: {result['category_weight']}")
                print(f"   ì¥ê¸° íˆ¬ì ê´€ë ¨ì„±: {result['long_term_relevance']}%")
                print(f"   ê¸ì • ì ìˆ˜: {result['positive_score']}")
                print(f"   ë¶€ì • ì ìˆ˜: {result['negative_score']}")
        
        else:
            print("âŒ ì˜¬ë°”ë¥¸ ë²ˆí˜¸ë¥¼ ì„ íƒí•´ì£¼ì„¸ìš”.")


if __name__ == "__main__":
    main()