"""
ğŸ“° ë‰´ìŠ¤ ìˆ˜ì§‘ ë° ê°ì • ë¶„ì„ ì‹œìŠ¤í…œ (í’ˆì§ˆ ê²€ì¦ ê°•í™” ë²„ì „)

ì´ ëª¨ë“ˆì€ ì£¼ì‹ ê´€ë ¨ ë‰´ìŠ¤ë¥¼ ìˆ˜ì§‘í•˜ê³  ê°ì • ë¶„ì„ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.

ì£¼ìš” ê¸°ëŠ¥:
1. ë„¤ì´ë²„ ê¸ˆìœµ ë‰´ìŠ¤ í¬ë¡¤ë§
2. ì¢…ëª©ë³„ ë‰´ìŠ¤ ìˆ˜ì§‘
3. ë‰´ìŠ¤ í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬
4. ê°ì • ë¶„ì„ (ê¸ì •/ë¶€ì •/ì¤‘ë¦½)
5. ì¢…ëª©ë³„ ê°ì • ì§€ìˆ˜ ê³„ì‚°
6. ì‹œê³„ì—´ ê°ì • ì¶”ì´ ë¶„ì„

ğŸ†• í’ˆì§ˆ ê²€ì¦ ê°•í™”:
7. ìŠ¤íŒ¸/ì¤‘ë³µ/ì˜¤ë¥˜ ë‰´ìŠ¤ ìë™ í•„í„°ë§
8. ì‹ ë¢°ë„ ì ìˆ˜ ìë™ ê³„ì‚°
9. í•œê¸€ ì¸ì½”ë”© ë¬¸ì œ ì™„ì „ í•´ê²°
10. ë‰´ìŠ¤ í’ˆì§ˆ ë“±ê¸‰ ì‹œìŠ¤í…œ

ğŸ¯ ëª©í‘œ: ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ë‰´ìŠ¤ ê¸°ë°˜ íˆ¬ì ì‹ í˜¸ ìƒì„±
"""

import sys
from pathlib import Path
import requests
from bs4 import BeautifulSoup
import pandas as pd
import sqlite3
import time
from datetime import datetime, timedelta
import re
import json
from urllib.parse import urljoin, quote
from tqdm import tqdm
import random
from concurrent.futures import ThreadPoolExecutor, as_completed
import threading
import difflib
from collections import Counter
import unicodedata
from typing import List, Dict, Optional, Tuple
import logging

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
project_root = Path(__file__).parent.parent.parent
sys.path.append(str(project_root))

try:
    from config.settings import DATA_DIR
    import warnings
    warnings.filterwarnings('ignore')
except ImportError as e:
    print(f"âŒ íŒ¨í‚¤ì§€ ì„¤ì¹˜ í•„ìš”: {e}")
    exit(1)

# ë¡œê¹… ì„¤ì • (í•œê¸€ ì¸ì½”ë”© ì™„ì „ í•´ê²°)
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(project_root / 'data' / 'news_collection_enhanced.log', encoding='utf-8'),
        logging.StreamHandler(sys.stdout)
    ]
)
logger = logging.getLogger(__name__)

class NewsQualityValidator:
    """
    ë‰´ìŠ¤ í’ˆì§ˆ ê²€ì¦ ì‹œìŠ¤í…œ
    
    ìŠ¤íŒ¸, ì¤‘ë³µ, ì˜¤ë¥˜ê°€ ìˆëŠ” ë‰´ìŠ¤ë¥¼ ìë™ìœ¼ë¡œ í•„í„°ë§í•˜ê³ 
    ê° ë‰´ìŠ¤ì— ì‹ ë¢°ë„ ì ìˆ˜ë¥¼ ë¶€ì—¬í•©ë‹ˆë‹¤.
    """
    
    def __init__(self):
        # ìŠ¤íŒ¸ íŒ¨í„´ ì •ì˜ (í•œêµ­ ì£¼ì‹ ë‰´ìŠ¤ íŠ¹í™”)
        self.spam_patterns = [
            r'í´ë¦­.*ì¡°íšŒ',
            r'ë¬´ë£Œ.*ìƒë‹´',
            r'ì§€ê¸ˆ.*ì‹ ì²­',
            r'100%.*ìˆ˜ìµ',
            r'ê¸‰ë“±.*í™•ì‹¤',
            r'ëŒ€ë°•.*ì¢…ëª©',
            r'ë¬´ì¡°ê±´.*ìƒìŠ¹',
            r'íˆ¬ì.*ë³´ì¥',
            r'ìˆ˜ìµë¥ .*\d+%.*ë³´ì¥',
            r'ë‹¨íƒ€.*ìˆ˜ìµ',
            r'ë¡œë˜.*ì¢…ëª©',
            r'ë”°ìƒ.*í™•ì‹¤',
            r'ë¬´ë£Œ.*ì¶”ì²œ',
            r'VIP.*ì¢…ëª©'
        ]
        
        # ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ë‰´ìŠ¤ ì†ŒìŠ¤ (ë„¤ì´ë²„ ê¸ˆìœµ ê¸°ì¤€)
        self.trusted_sources = {
            'ì—°í•©ë‰´ìŠ¤': 95,
            'í•œêµ­ê²½ì œ': 90,
            'ë§¤ì¼ê²½ì œ': 88,
            'ì¡°ì„ ì¼ë³´': 85,
            'ì¤‘ì•™ì¼ë³´': 85,
            'ë™ì•„ì¼ë³´': 85,
            'ë¨¸ë‹ˆíˆ¬ë°ì´': 82,
            'ì „ìì‹ ë¬¸': 80,
            'ì„œìš¸ê²½ì œ': 78,
            'ì´ë°ì¼ë¦¬': 75,
            'íŒŒì´ë‚¸ì…œë‰´ìŠ¤': 78,
            'MBN': 72,
            'SBS Biz': 75
        }
        
        # ì˜ì‹¬ìŠ¤ëŸ¬ìš´ í‚¤ì›Œë“œ (ëª…ë°±í•œ ì˜¤ë¥˜ í¬í•¨)
        self.suspicious_keywords = [
            '2ë§Œ4900ì›', '5ë§Œì›', '10ë§Œì›',  # ëª…ë°±í•œ ì£¼ê°€ ì˜¤ë¥˜
            '999999', '000000', '123456',  # ë”ë¯¸ ë°ì´í„°
            'í…ŒìŠ¤íŠ¸', 'test', 'TEST',
            'ê´‘ê³ ', 'í™ë³´', 'í˜‘ì°¬',
            'ì´ë²¤íŠ¸', 'í”„ë¡œëª¨ì…˜',
            '888ì›', '777ì›', '666ì›',  # ì˜ì‹¬ìŠ¤ëŸ¬ìš´ íŒ¨í„´
            'â—‹â—‹â—‹ì›', 'XXXì›'  # ë§ˆìŠ¤í‚¹ëœ ë°ì´í„°
        ]
        
        # ì¤‘ë³µ ê²€ì¶œìš© ìºì‹œ
        self.content_hashes = set()
        self.title_cache = {}
        
        logger.info("âœ… ë‰´ìŠ¤ í’ˆì§ˆ ê²€ì¦ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ")
    
    def validate_news(self, news_data: Dict) -> Tuple[bool, int, List[str]]:
        """
        ë‰´ìŠ¤ í’ˆì§ˆ ì¢…í•© ê²€ì¦
        
        Args:
            news_data: ë‰´ìŠ¤ ë°ì´í„° ë”•ì…”ë„ˆë¦¬
            
        Returns:
            Tuple[is_valid, quality_score, issues]: 
            - is_valid: í†µê³¼ ì—¬ë¶€ (70ì  ì´ìƒ)
            - quality_score: ì‹ ë¢°ë„ ì ìˆ˜ (0-100)
            - issues: ë°œê²¬ëœ ë¬¸ì œì  ë¦¬ìŠ¤íŠ¸
        """
        issues = []
        score = 100  # ë§Œì ì—ì„œ ì‹œì‘í•´ì„œ ë¬¸ì œ ë°œê²¬ ì‹œ ê°ì 
        
        title = news_data.get('title', '')
        content = news_data.get('content', '')
        source = news_data.get('source', '')
        
        # 1. ìŠ¤íŒ¸ ê²€ì‚¬ (30ì  ê°ì )
        if self._is_spam_content(title, content):
            issues.append("ìŠ¤íŒ¸ íŒ¨í„´ ê°ì§€")
            score -= 30
        
        # 2. ì¤‘ë³µ ê²€ì‚¬ (25ì  ê°ì )
        if self._is_duplicate_content(title, content):
            issues.append("ì¤‘ë³µ ì½˜í…ì¸ ")
            score -= 25
        
        # 3. ì˜ì‹¬ìŠ¤ëŸ¬ìš´ í‚¤ì›Œë“œ ê²€ì‚¬ (20ì  ê°ì )
        if self._has_suspicious_keywords(title, content):
            issues.append("ì˜ì‹¬ìŠ¤ëŸ¬ìš´ í‚¤ì›Œë“œ í¬í•¨")
            score -= 20
        
        # 4. ì†ŒìŠ¤ ì‹ ë¢°ë„ ê²€ì‚¬ (ì ìˆ˜ ì¡°ì •)
        source_score = self._get_source_credibility(source)
        if source_score < 50:
            issues.append("ì‹ ë¢°ë„ ë‚®ì€ ì†ŒìŠ¤")
            score = min(score, source_score + 20)
        
        # 5. ì½˜í…ì¸  í’ˆì§ˆ ê²€ì‚¬ (15ì  ê°ì )
        content_quality = self._assess_content_quality(title, content)
        if content_quality < 70:
            issues.append("ì½˜í…ì¸  í’ˆì§ˆ ë¶€ì¡±")
            score -= 15
        
        # 6. ì¸ì½”ë”© ì˜¤ë¥˜ ê²€ì‚¬ (10ì  ê°ì )
        if self._has_encoding_issues(title, content):
            issues.append("ì¸ì½”ë”© ì˜¤ë¥˜")
            score -= 10
        
        # ìµœì¢… ì ìˆ˜ ë²”ìœ„ ì¡°ì •
        score = max(0, min(100, score))
        
        # 70ì  ì´ìƒë§Œ í†µê³¼
        is_valid = score >= 70 and len(issues) <= 2
        
        if not is_valid:
            logger.debug(f"ë‰´ìŠ¤ í’ˆì§ˆ ê²€ì¦ ì‹¤íŒ¨: {title[:30]}... (ì ìˆ˜: {score}, ë¬¸ì œ: {issues})")
        
        return is_valid, score, issues
    
    def _is_spam_content(self, title: str, content: str) -> bool:
        """ìŠ¤íŒ¸ íŒ¨í„´ ê²€ì‚¬"""
        text_combined = f"{title} {content}".lower()
        
        for pattern in self.spam_patterns:
            if re.search(pattern, text_combined):
                return True
        
        # ê³¼ë„í•œ íŠ¹ìˆ˜ë¬¸ì ì‚¬ìš© (ìŠ¤íŒ¸ íŠ¹ì§•)
        special_char_ratio = len(re.findall(r'[!@#$%^&*()+=\[\]{}|\\:";\'<>?,./]', text_combined)) / max(len(text_combined), 1)
        if special_char_ratio > 0.1:  # 10% ì´ìƒ
            return True
        
        # ê³¼ë„í•œ ìˆ«ì ì‚¬ìš©
        number_ratio = len(re.findall(r'\d', text_combined)) / max(len(text_combined), 1)
        if number_ratio > 0.3:  # 30% ì´ìƒ
            return True
        
        return False
    
    def _is_duplicate_content(self, title: str, content: str) -> bool:
        """ì¤‘ë³µ ì½˜í…ì¸  ê²€ì‚¬"""
        # ì œëª© ê¸°ë°˜ ìœ ì‚¬ë„ ê²€ì‚¬
        title_normalized = self._normalize_text(title)
        
        for cached_title in self.title_cache.keys():
            similarity = difflib.SequenceMatcher(None, title_normalized, cached_title).ratio()
            if similarity > 0.85:  # 85% ì´ìƒ ìœ ì‚¬í•˜ë©´ ì¤‘ë³µ
                return True
        
        # ìºì‹œì— ì¶”ê°€ (ìµœëŒ€ 1000ê°œê¹Œì§€ë§Œ ìœ ì§€)
        if len(self.title_cache) > 1000:
            # ì˜¤ë˜ëœ í•­ëª© ì ˆë°˜ ì‚­ì œ
            items = list(self.title_cache.items())
            self.title_cache = dict(items[500:])
        
        self.title_cache[title_normalized] = True
        
        # ë‚´ìš© í•´ì‹œ ê¸°ë°˜ ì¤‘ë³µ ê²€ì‚¬
        content_hash = hash(self._normalize_text(content))
        if content_hash in self.content_hashes:
            return True
        
        self.content_hashes.add(content_hash)
        
        # ë©”ëª¨ë¦¬ ê´€ë¦¬
        if len(self.content_hashes) > 5000:
            # ì ˆë°˜ ì‚­ì œ
            self.content_hashes = set(list(self.content_hashes)[2500:])
        
        return False
    
    def _has_suspicious_keywords(self, title: str, content: str) -> bool:
        """ì˜ì‹¬ìŠ¤ëŸ¬ìš´ í‚¤ì›Œë“œ ê²€ì‚¬"""
        text_combined = f"{title} {content}".lower()
        
        for keyword in self.suspicious_keywords:
            if keyword.lower() in text_combined:
                return True
        
        return False
    
    def _get_source_credibility(self, source: str) -> int:
        """ì†ŒìŠ¤ ì‹ ë¢°ë„ ì ìˆ˜ ë°˜í™˜"""
        for trusted_source, score in self.trusted_sources.items():
            if trusted_source in source:
                return score
        
        # ë„¤ì´ë²„ê¸ˆìœµ ì¶œì²˜ëŠ” ì¤‘ê°„ ì ìˆ˜
        if 'ë„¤ì´ë²„ê¸ˆìœµ' in source:
            return 65
        
        # ì•Œë ¤ì§€ì§€ ì•Šì€ ì†ŒìŠ¤ëŠ” ë‚®ì€ ì ìˆ˜
        return 50
    
    def _assess_content_quality(self, title: str, content: str) -> int:
        """ì½˜í…ì¸  í’ˆì§ˆ í‰ê°€"""
        quality_score = 100
        
        # ì œëª© ê¸¸ì´ ê²€ì‚¬
        if len(title) < 10 or len(title) > 200:
            quality_score -= 15
        
        # ë‚´ìš© ê¸¸ì´ ê²€ì‚¬
        if len(content) < 50:
            quality_score -= 25
        elif len(content) > 10000:
            quality_score -= 5
        
        # ë¬¸ì¥ êµ¬ì¡° ê²€ì‚¬
        sentences = re.split(r'[.!?]', content)
        if len(sentences) < 2:
            quality_score -= 20
        
        # ì˜ë¯¸ ìˆëŠ” ë‹¨ì–´ ë¹„ìœ¨
        words = re.findall(r'[ê°€-í£]+', content)
        if len(words) < 10:
            quality_score -= 25
        
        # ë°˜ë³µ êµ¬ë¬¸ ê²€ì‚¬
        word_freq = Counter(words)
        if word_freq.most_common(1) and word_freq.most_common(1)[0][1] > len(words) * 0.1:
            quality_score -= 15
        
        return max(0, quality_score)
    
    def _has_encoding_issues(self, title: str, content: str) -> bool:
        """ì¸ì½”ë”© ì˜¤ë¥˜ ê²€ì‚¬"""
        text_combined = f"{title} {content}"
        
        # ê¹¨ì§„ ë¬¸ì íŒ¨í„´
        broken_patterns = [
            r'[^\w\sê°€-í£ã„±-ã…ã…-ã…£.,!?()[\]{}:;"\'-]',  # ë¹„ì •ìƒ ë¬¸ì
            r'(?:[ï¿½ï¿½]{2,})',  # ì—°ì†ëœ ê¹¨ì§„ ë¬¸ì
            r'(?:&[a-zA-Z]+;){3,}',  # ê³¼ë„í•œ HTML ì—”í‹°í‹°
            r'[?]{3,}',  # ì—°ì†ëœ ë¬¼ìŒí‘œ (ê¹¨ì§„ ë¬¸ì)
        ]
        
        for pattern in broken_patterns:
            if re.search(pattern, text_combined):
                return True
        
        return False
    
    def _normalize_text(self, text: str) -> str:
        """í…ìŠ¤íŠ¸ ì •ê·œí™”"""
        if not text:
            return ""
        
        # ìœ ë‹ˆì½”ë“œ ì •ê·œí™”
        text = unicodedata.normalize('NFKC', text)
        
        # ê³µë°± ì •ë¦¬
        text = re.sub(r'\s+', ' ', text).strip()
        
        # íŠ¹ìˆ˜ë¬¸ì ì œê±° (ë¹„êµìš©)
        text = re.sub(r'[^\w\sê°€-í£]', '', text)
        
        return text.lower()


class EnhancedNewsCollector:
    """
    ê°•í™”ëœ ë‰´ìŠ¤ ìˆ˜ì§‘ê¸° (í’ˆì§ˆ ê²€ì¦ í†µí•©)
    
    ì£¼ì‹ ê´€ë ¨ ë‰´ìŠ¤ë¥¼ ë‹¤ì–‘í•œ ì†ŒìŠ¤ì—ì„œ ìˆ˜ì§‘í•˜ê³  ì €ì¥í•©ë‹ˆë‹¤.
    """
    
    def __init__(self):
        self.data_dir = Path(DATA_DIR)
        self.db_path = self.data_dir / 'news_data.db'
        
        # í’ˆì§ˆ ê²€ì¦ ì‹œìŠ¤í…œ í†µí•©
        self.quality_validator = NewsQualityValidator()
        
        # ìˆ˜ì§‘ í†µê³„
        self.stats = {
            'total_collected': 0,
            'success_count': 0,
            'fail_count': 0,
            'duplicate_count': 0,
            'quality_passed': 0,
            'quality_failed': 0,
            'spam_filtered': 0,
            'low_quality_filtered': 0
        }
        
        # HTTP ì„¸ì…˜ ì„¤ì •
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'ko-KR,ko;q=0.8,en-US;q=0.5,en;q=0.3',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
        })
        
        self.init_database()
        print("âœ… ê°•í™”ëœ ë‰´ìŠ¤ ìˆ˜ì§‘ê¸° ì´ˆê¸°í™” ì™„ë£Œ")
    
    def init_database(self):
        """ğŸ—„ï¸ ë‰´ìŠ¤ ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™” (í’ˆì§ˆ ê´€ë ¨ ì»¬ëŸ¼ ì¶”ê°€)"""
        print("ğŸ—„ï¸ ë‰´ìŠ¤ ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™” ì¤‘...")
        
        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.cursor()
            
            # ê°•í™”ëœ ë‰´ìŠ¤ ê¸°ì‚¬ í…Œì´ë¸”
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS news_articles (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    stock_code TEXT,
                    stock_name TEXT,
                    title TEXT NOT NULL,
                    content TEXT,
                    summary TEXT,
                    url TEXT UNIQUE,
                    source TEXT,
                    author TEXT,
                    published_date TEXT,
                    collected_date TEXT,
                    sentiment_score REAL,
                    sentiment_label TEXT,
                    keywords TEXT,
                    view_count INTEGER,
                    comment_count INTEGER,
                    quality_score INTEGER DEFAULT 0,
                    quality_issues TEXT,
                    is_verified BOOLEAN DEFAULT 0,
                    credibility_rating TEXT DEFAULT 'UNKNOWN'
                )
            ''')
            
            # ê°ì • ë¶„ì„ ê²°ê³¼ í…Œì´ë¸”
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS sentiment_analysis (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    stock_code TEXT NOT NULL,
                    date TEXT NOT NULL,
                    positive_count INTEGER DEFAULT 0,
                    negative_count INTEGER DEFAULT 0,
                    neutral_count INTEGER DEFAULT 0,
                    total_count INTEGER DEFAULT 0,
                    sentiment_score REAL DEFAULT 0,
                    sentiment_index REAL DEFAULT 50,
                    average_quality REAL DEFAULT 0,
                    verified_news_ratio REAL DEFAULT 0,
                    created_date TEXT,
                    UNIQUE(stock_code, date)
                )
            ''')
            
            # í‚¤ì›Œë“œ ì¶”ì¶œ ê²°ê³¼ í…Œì´ë¸”
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS news_keywords (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    stock_code TEXT,
                    keyword TEXT,
                    frequency INTEGER,
                    date TEXT,
                    sentiment_impact REAL,
                    quality_weighted_impact REAL,
                    created_date TEXT
                )
            ''')
            
            # í’ˆì§ˆ í•„í„°ë§ ë¡œê·¸ í…Œì´ë¸” (ìƒˆë¡œ ì¶”ê°€)
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS quality_filter_log (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    stock_code TEXT,
                    title TEXT,
                    url TEXT,
                    filter_reason TEXT,
                    quality_score INTEGER,
                    filtered_date TEXT
                )
            ''')
            
            # ì¸ë±ìŠ¤ ìƒì„±
            cursor.execute('CREATE INDEX IF NOT EXISTS idx_news_stock_code ON news_articles(stock_code)')
            cursor.execute('CREATE INDEX IF NOT EXISTS idx_news_published_date ON news_articles(published_date)')
            cursor.execute('CREATE INDEX IF NOT EXISTS idx_news_quality_score ON news_articles(quality_score)')
            cursor.execute('CREATE INDEX IF NOT EXISTS idx_news_is_verified ON news_articles(is_verified)')
            cursor.execute('CREATE INDEX IF NOT EXISTS idx_sentiment_stock_date ON sentiment_analysis(stock_code, date)')
            
            conn.commit()
        
        print("âœ… ê°•í™”ëœ ë‰´ìŠ¤ ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™” ì™„ë£Œ")
    
    def get_stock_list_from_db(self):
        """ğŸ“Š ì£¼ì‹ DBì—ì„œ ì¢…ëª© ë¦¬ìŠ¤íŠ¸ ê°€ì ¸ì˜¤ê¸°"""
        stock_db_path = self.data_dir / 'stock_data.db'
        
        if not stock_db_path.exists():
            print("âŒ ì£¼ì‹ ë°ì´í„°ë² ì´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤!")
            return []
        
        try:
            with sqlite3.connect(stock_db_path) as conn:
                query = """
                    SELECT DISTINCT symbol as stock_code, name as stock_name
                    FROM stock_info
                    WHERE symbol IS NOT NULL 
                    AND LENGTH(symbol) = 6
                    ORDER BY symbol
                """
                result = pd.read_sql_query(query, conn)
                return result.to_dict('records')
                
        except Exception as e:
            print(f"âŒ ì£¼ì‹ DB ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return []
    
    def collect_naver_finance_news(self, stock_code, stock_name, days=7, max_pages=5):
        """
        ğŸ“° ë„¤ì´ë²„ ê¸ˆìœµ ë‰´ìŠ¤ ìˆ˜ì§‘ (í’ˆì§ˆ ê²€ì¦ í†µí•©)
        
        Args:
            stock_code (str): ì¢…ëª©ì½”ë“œ
            stock_name (str): ì¢…ëª©ëª…
            days (int): ìˆ˜ì§‘í•  ì¼ìˆ˜
            max_pages (int): ìµœëŒ€ í˜ì´ì§€ ìˆ˜
            
        Returns:
            list: ìˆ˜ì§‘ëœ ê³ í’ˆì§ˆ ë‰´ìŠ¤ ë¦¬ìŠ¤íŠ¸
        """
        news_list = []
        
        try:
            # ë„¤ì´ë²„ ê¸ˆìœµ ì¢…ëª© ë‰´ìŠ¤ URL
            base_url = f"https://finance.naver.com/item/news_news.naver"
            
            for page in range(1, max_pages + 1):
                params = {
                    'code': stock_code,
                    'page': page
                }
                
                try:
                    response = self.session.get(base_url, params=params, timeout=10)
                    response.raise_for_status()
                    
                    # ì¸ì½”ë”© ìë™ ê°ì§€
                    response.encoding = response.apparent_encoding
                    
                    soup = BeautifulSoup(response.text, 'html.parser')
                    
                    # ë‰´ìŠ¤ ëª©ë¡ íŒŒì‹±
                    news_items = soup.select('.tb_cont tr')
                    
                    for item in news_items:
                        try:
                            # ì œëª©ê³¼ ë§í¬ ì¶”ì¶œ
                            title_elem = item.select_one('.title a')
                            if not title_elem:
                                continue
                            
                            title = title_elem.get_text(strip=True)
                            news_url = urljoin("https://finance.naver.com", title_elem.get('href'))
                            
                            # ë‚ ì§œ ì¶”ì¶œ
                            date_elem = item.select_one('.date')
                            if date_elem:
                                date_str = date_elem.get_text(strip=True)
                                published_date = self.parse_date(date_str)
                            else:
                                published_date = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
                            
                            # ì •ë³´ ì œê³µì ì¶”ì¶œ
                            info_elem = item.select_one('.info')
                            source = info_elem.get_text(strip=True) if info_elem else 'ë„¤ì´ë²„ê¸ˆìœµ'
                            
                            # ë‰´ìŠ¤ ìƒì„¸ ë‚´ìš© ìˆ˜ì§‘ (ê°•í™”ëœ ì¶”ì¶œ)
                            content, summary = self.get_enhanced_news_content(news_url)
                            
                            news_data = {
                                'stock_code': stock_code,
                                'stock_name': stock_name,
                                'title': title,
                                'content': content,
                                'summary': summary,
                                'url': news_url,
                                'source': f'ë„¤ì´ë²„ê¸ˆìœµ-{source}',
                                'published_date': published_date,
                                'collected_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
                            }
                            
                            # ğŸ†• í’ˆì§ˆ ê²€ì¦
                            is_valid, quality_score, issues = self.quality_validator.validate_news(news_data)
                            
                            if is_valid:
                                # í’ˆì§ˆ ì •ë³´ ì¶”ê°€
                                news_data['quality_score'] = quality_score
                                news_data['quality_issues'] = ', '.join(issues) if issues else ''
                                news_data['is_verified'] = True
                                news_data['credibility_rating'] = self._get_credibility_rating(quality_score)
                                
                                news_list.append(news_data)
                                self.stats['quality_passed'] += 1
                            else:
                                # í•„í„°ë§ëœ ë‰´ìŠ¤ ë¡œê·¸ ì €ì¥
                                self._log_filtered_news(stock_code, title, news_url, issues, quality_score)
                                self.stats['quality_failed'] += 1
                                
                                if 'ìŠ¤íŒ¸ íŒ¨í„´ ê°ì§€' in issues:
                                    self.stats['spam_filtered'] += 1
                                if 'ì½˜í…ì¸  í’ˆì§ˆ ë¶€ì¡±' in issues:
                                    self.stats['low_quality_filtered'] += 1
                            
                            # ìš”ì²­ ê°„ê²© ì¡°ì ˆ
                            time.sleep(random.uniform(0.5, 1.5))
                            
                        except Exception as e:
                            print(f"  âš ï¸ ë‰´ìŠ¤ í•­ëª© íŒŒì‹± ì‹¤íŒ¨: {e}")
                            continue
                    
                    # í˜ì´ì§€ ê°„ ê°„ê²©
                    time.sleep(random.uniform(1, 2))
                    
                except Exception as e:
                    print(f"  âŒ í˜ì´ì§€ {page} ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
                    continue
            
        except Exception as e:
            print(f"âŒ {stock_code}({stock_name}) ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
        
        return news_list
    
    def get_enhanced_news_content(self, url):
        """
        ğŸ“„ ê°•í™”ëœ ë‰´ìŠ¤ ìƒì„¸ ë‚´ìš© ì¶”ì¶œ
        
        Args:
            url (str): ë‰´ìŠ¤ URL
            
        Returns:
            tuple: (content, summary)
        """
        try:
            response = self.session.get(url, timeout=15)
            response.raise_for_status()
            
            # ì¸ì½”ë”© ìë™ ê°ì§€ ë° ì„¤ì •
            response.encoding = response.apparent_encoding
            
            soup = BeautifulSoup(response.text, 'html.parser', from_encoding='utf-8')
            
            # ë‹¤ì–‘í•œ ë‰´ìŠ¤ ì‚¬ì´íŠ¸ì˜ ë³¸ë¬¸ ì„ íƒì (í™•ì¥)
            content_selectors = [
                '.news_body',
                '.article_body', 
                '.news_content',
                '#news_body',
                '.news_text',
                '.article_content',
                '#articleBodyContents',
                '.newsct_article',
                '.article-view-content',
                '.news-article-content'
            ]
            
            content = ""
            for selector in content_selectors:
                content_elem = soup.select_one(selector)
                if content_elem:
                    # ê´‘ê³ , ê´€ë ¨ê¸°ì‚¬ ë“± ì œê±° (ê°•í™”)
                    for unwanted in content_elem.find_all(['script', 'style', 'iframe', 'ins', 'aside', 'nav', 'footer']):
                        unwanted.decompose()
                    
                    # ê´‘ê³  ê´€ë ¨ í´ë˜ìŠ¤ ì œê±°
                    for elem in content_elem.find_all(class_=re.compile(r'(ad|advertisement|related|recommend|banner)')):
                        elem.decompose()
                    
                    content = content_elem.get_text(separator=' ', strip=True)
                    break
            
            # ê°•í™”ëœ í…ìŠ¤íŠ¸ ì •ì œ
            content = self._advanced_text_cleaning(content)
            
            # ìš”ì•½ ìƒì„± (ì²« 2-3ë¬¸ì¥, í’ˆì§ˆ ê³ ë ¤)
            sentences = re.split(r'[.!?]\s+', content)
            meaningful_sentences = [s for s in sentences if len(s.strip()) > 20]
            summary = '. '.join(meaningful_sentences[:3])[:300] if meaningful_sentences else content[:200]
            
            return content, summary
            
        except Exception as e:
            logger.debug(f"ê°•í™”ëœ ë³¸ë¬¸ ì¶”ì¶œ ì‹¤íŒ¨ - {url}: {e}")
            return "", ""
    
    def _advanced_text_cleaning(self, text: str) -> str:
        """ê°•í™”ëœ í…ìŠ¤íŠ¸ ì •ì œ (ì¤‘ë³µ í•´ê²° ê°œì„ )"""
        if not text:
            return ""
        
        # 1. ìœ ë‹ˆì½”ë“œ ì •ê·œí™”
        text = unicodedata.normalize('NFKC', text)
        
        # 2. HTML íƒœê·¸ ë° ì—”í‹°í‹° ì œê±°
        text = re.sub(r'<[^>]+>', ' ', text)
        text = re.sub(r'&[a-zA-Z0-9#]+;', ' ', text)
        
        # 3. ë¶ˆí•„ìš”í•œ ë¬¸êµ¬ ì œê±° (í™•ì¥)
        patterns_to_remove = [
            r'// flash ì˜¤ë¥˜ë¥¼ ìš°íšŒí•˜ê¸° ìœ„í•œ í•¨ìˆ˜ ì¶”ê°€.*',
            r'ë³¸\s*ê¸°ì‚¬ëŠ”.*?ì…ë‹ˆë‹¤',
            r'ì €ì‘ê¶Œì.*?ë¬´ë‹¨.*?ê¸ˆì§€',
            r'â“’.*?ë¬´ë‹¨.*?ê¸ˆì§€',
            r'Copyright.*?All.*?rights.*?reserved',
            r'ê¸°ì\s*=.*?ê¸°ì',
            r'^\s*\[.*?\]\s*',
            r'\s*\[.*?\]\s*
            ,
            r'ì´\s*ë©”ì¼.*?ë³´ë‚´ê¸°',
            r'ì¹´ì¹´ì˜¤í†¡.*?ê³µìœ ',
            r'í˜ì´ìŠ¤ë¶.*?ê³µìœ ',
            r'íŠ¸ìœ„í„°.*?ê³µìœ ',
            r'ë¬´ë‹¨ì „ì¬.*?ê¸ˆì§€',
            r'ë„¤ì´ë²„.*?ë¸”ë¡œê·¸',
            r'ê´€ë ¨.*?ë‰´ìŠ¤',
            r'ì´ì „.*?ê¸°ì‚¬',
            r'ë‹¤ìŒ.*?ê¸°ì‚¬',
            r'.*?êµ¬ë….*?ì•Œë¦¼',
            r'.*?íŒ”ë¡œìš°.*?',
            r'ê´‘ê³ .*?ë¬¸ì˜',
            r'ì œë³´.*?tip',
            r'ë”ë³´ê¸°.*?í´ë¦­',
            r'ë™ì˜ìƒ.*?ë³´ê¸°'
        ]
        
        for pattern in patterns_to_remove:
            text = re.sub(pattern, '', text, flags=re.IGNORECASE | re.DOTALL)
        
        # 4. íŠ¹ìˆ˜ ë¬¸ì ì •ë¦¬
        text = re.sub(r'[&\[\]{}()\*\+\?\|\^\$\\.~`!@#%=:;",<>]', ' ', text)
        
        # 5. ìˆ«ìì™€ í•œê¸€ ì‚¬ì´ ê³µë°±
        text = re.sub(r'(\d)([ê°€-í£])', r'\1 \2', text)
        text = re.sub(r'([ê°€-í£])(\d)', r'\1 \2', text)
        
        # 6. ê°•í™”ëœ ì¤‘ë³µ ì œê±°
        words = text.split()
        cleaned_words = []
        prev_word = ""
        
        for word in words:
            # ì—°ì†ëœ ê°™ì€ ë‹¨ì–´ ì œê±°
            if word != prev_word and len(word.strip()) > 0:
                cleaned_words.append(word)
            prev_word = word
        
        text = ' '.join(cleaned_words)
        
        # 7. ì¤‘ë³µ íŒ¨í„´ ì œê±° (ì •ê·œí‘œí˜„ì‹, ê°œì„ )
        text = re.sub(r'([ê°€-í£A-Za-z0-9]{2,})\1{2,}', r'\1', text)  # 3ë²ˆ ì´ìƒ ë°˜ë³µ
        
        # 8. ë°˜ë³µ êµ¬ë¬¸ ì œê±° (ê°œì„ ëœ ì•Œê³ ë¦¬ì¦˜)
        def remove_repeating_patterns(text):
            for length in range(3, 20):  # ë” ê¸´ íŒ¨í„´ë„ ê°ì§€
                pattern = f'(.{{{length}}})(\\1)+'
                text = re.sub(pattern, r'\1', text)
            return text
        
        text = remove_repeating_patterns(text)
        
        # 9. ì—¬ëŸ¬ ê³µë°±ì„ í•˜ë‚˜ë¡œ
        text = re.sub(r'\s+', ' ', text)
        
        # 10. ìµœì¢… ì •ë¦¬
        text = text.strip()
        
        return text
    
    def _get_credibility_rating(self, quality_score: int) -> str:
        """í’ˆì§ˆ ì ìˆ˜ë¥¼ ë“±ê¸‰ìœ¼ë¡œ ë³€í™˜"""
        if quality_score >= 90:
            return 'EXCELLENT'
        elif quality_score >= 80:
            return 'GOOD'
        elif quality_score >= 70:
            return 'FAIR'
        elif quality_score >= 60:
            return 'POOR'
        else:
            return 'VERY_POOR'
    
    def _log_filtered_news(self, stock_code: str, title: str, url: str, issues: List[str], quality_score: int):
        """í•„í„°ë§ëœ ë‰´ìŠ¤ ë¡œê·¸ ì €ì¥"""
        try:
            with sqlite3.connect(self.db_path) as conn:
                cursor = conn.cursor()
                cursor.execute('''
                    INSERT INTO quality_filter_log 
                    (stock_code, title, url, filter_reason, quality_score, filtered_date)
                    VALUES (?, ?, ?, ?, ?, ?)
                ''', (
                    stock_code,
                    title[:200],  # ì œëª© ê¸¸ì´ ì œí•œ
                    url,
                    ', '.join(issues),
                    quality_score,
                    datetime.now().isoformat()
                ))
                conn.commit()
        except Exception as e:
            logger.error(f"í•„í„°ë§ ë¡œê·¸ ì €ì¥ ì‹¤íŒ¨: {e}")
    
    def parse_date(self, date_str):
        """ğŸ“… ë‚ ì§œ ë¬¸ìì—´ íŒŒì‹± (ê°œì„ )"""
        try:
            # "07.05" í˜•íƒœ
            if re.match(r'\d{2}\.\d{2}
            , date_str):
                current_year = datetime.now().year
                month, day = date_str.split('.')
                return f"{current_year}-{month}-{day} 00:00:00"
            
            # "07.05 15:30" í˜•íƒœ
            elif re.match(r'\d{2}\.\d{2} \d{2}:\d{2}', date_str):
                current_year = datetime.now().year
                date_part, time_part = date_str.split(' ')
                month, day = date_part.split('.')
                return f"{current_year}-{month}-{day} {time_part}:00"
            
            # "2024.07.05" í˜•íƒœ
            elif re.match(r'\d{4}\.\d{2}\.\d{2}', date_str):
                year, month, day = date_str.split('.')
                return f"{year}-{month}-{day} 00:00:00"
            
            # ê¸°íƒ€ í˜•íƒœëŠ” í˜„ì¬ ì‹œê°„ ë°˜í™˜
            else:
                return datetime.now().strftime('%Y-%m-%d %H:%M:%S')
                
        except:
            return datetime.now().strftime('%Y-%m-%d %H:%M:%S')
    
    def save_news_to_db(self, news_list):
        """ğŸ“š ë‰´ìŠ¤ ë°ì´í„°ë¥¼ DBì— ì €ì¥ (í’ˆì§ˆ ì •ë³´ í¬í•¨)"""
        if not news_list:
            return 0
        
        try:
            with sqlite3.connect(self.db_path) as conn:
                cursor = conn.cursor()
                
                saved_count = 0
                for news in news_list:
                    try:
                        cursor.execute('''
                            INSERT OR IGNORE INTO news_articles 
                            (stock_code, stock_name, title, content, summary, url, source, 
                             published_date, collected_date, quality_score, quality_issues, 
                             is_verified, credibility_rating)
                            VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                        ''', (
                            news.get('stock_code', ''),
                            news.get('stock_name', ''),
                            news.get('title', ''),
                            news.get('content', ''),
                            news.get('summary', ''),
                            news.get('url', ''),
                            news.get('source', ''),
                            news.get('published_date', ''),
                            news.get('collected_date', ''),
                            news.get('quality_score', 0),
                            news.get('quality_issues', ''),
                            news.get('is_verified', True),
                            news.get('credibility_rating', 'UNKNOWN')
                        ))
                        
                        if cursor.rowcount > 0:
                            saved_count += 1
                        else:
                            self.stats['duplicate_count'] += 1
                            
                    except Exception as e:
                        print(f"  âš ï¸ ë‰´ìŠ¤ ì €ì¥ ì‹¤íŒ¨: {e}")
                        continue
                
                conn.commit()
                return saved_count
                
        except Exception as e:
            print(f"âŒ ë‰´ìŠ¤ DB ì €ì¥ ì‹¤íŒ¨: {e}")
            return 0
    
    def collect_all_stock_news(self, days=7, max_stocks=None, max_workers=3):
        """
        ğŸš€ ëª¨ë“  ì¢…ëª©ì˜ ë‰´ìŠ¤ ìˆ˜ì§‘ (í’ˆì§ˆ ê²€ì¦ í†µí•©)
        
        Args:
            days (int): ìˆ˜ì§‘í•  ì¼ìˆ˜
            max_stocks (int): ìµœëŒ€ ì¢…ëª© ìˆ˜ (Noneì´ë©´ ì „ì²´)
            max_workers (int): ë™ì‹œ ì²˜ë¦¬ ìŠ¤ë ˆë“œ ìˆ˜
        """
        print("ğŸš€ ê°•í™”ëœ ì „ì²´ ì¢…ëª© ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹œì‘!")
        print("=" * 60)
        
        # ì¢…ëª© ë¦¬ìŠ¤íŠ¸ ê°€ì ¸ì˜¤ê¸°
        stock_list = self.get_stock_list_from_db()
        if not stock_list:
            print("âŒ ìˆ˜ì§‘í•  ì¢…ëª©ì´ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        if max_stocks:
            stock_list = stock_list[:max_stocks]
        
        print(f"ğŸ“Š ì´ {len(stock_list)}ê°œ ì¢…ëª© ë‰´ìŠ¤ ìˆ˜ì§‘ ì˜ˆì •")
        print(f"ğŸ“… ìˆ˜ì§‘ ê¸°ê°„: ìµœê·¼ {days}ì¼")
        print(f"ğŸ§µ ë™ì‹œ ì²˜ë¦¬: {max_workers}ê°œ ìŠ¤ë ˆë“œ")
        print(f"ğŸ›¡ï¸ í’ˆì§ˆ ê²€ì¦: í™œì„±í™” (70ì  ì´ìƒë§Œ ì €ì¥)")
        
        estimated_time = len(stock_list) * 30 / max_workers / 60  # ë¶„ ë‹¨ìœ„
        print(f"â±ï¸  ì˜ˆìƒ ì†Œìš”ì‹œê°„: ì•½ {estimated_time:.1f}ë¶„")
        
        confirm = input(f"\nê³ í’ˆì§ˆ ë‰´ìŠ¤ ìˆ˜ì§‘ì„ ì‹œì‘í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/N): ").strip().lower()
        if confirm != 'y':
            print("ğŸ‘‹ ìˆ˜ì§‘ì„ ì·¨ì†Œí–ˆìŠµë‹ˆë‹¤.")
            return
        
        # ë©€í‹°ìŠ¤ë ˆë”©ìœ¼ë¡œ ìˆ˜ì§‘
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            # ì‘ì—… ì œì¶œ
            future_to_stock = {}
            for stock in stock_list:
                future = executor.submit(
                    self.collect_stock_news_worker, 
                    stock['stock_code'], 
                    stock['stock_name'], 
                    days
                )
                future_to_stock[future] = stock
            
            # ì§„í–‰ë¥  í‘œì‹œ
            progress_bar = tqdm(
                as_completed(future_to_stock), 
                total=len(stock_list),
                desc="ğŸ“° ê³ í’ˆì§ˆ ë‰´ìŠ¤ ìˆ˜ì§‘",
                unit="ì¢…ëª©"
            )
            
            for future in progress_bar:
                stock = future_to_stock[future]
                stock_code = stock['stock_code']
                stock_name = stock['stock_name']
                
                try:
                    news_count = future.result()
                    self.stats['success_count'] += 1
                    self.stats['total_collected'] += news_count
                    
                    # í’ˆì§ˆ í†µê³¼ìœ¨ ê³„ì‚°
                    total_processed = self.stats['quality_passed'] + self.stats['quality_failed']
                    quality_rate = (self.stats['quality_passed'] / max(total_processed, 1)) * 100
                    
                    progress_bar.set_postfix({
                        'Current': f"{stock_code}({stock_name[:8]})",
                        'ê³ í’ˆì§ˆ': self.stats['total_collected'],
                        'í’ˆì§ˆë¥ ': f"{quality_rate:.1f}%"
                    })
                    
                except Exception as e:
                    self.stats['fail_count'] += 1
                    print(f"\nâŒ {stock_code}({stock_name}) ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
        
        # ìˆ˜ì§‘ ê²°ê³¼ ì¶œë ¥
        self.print_enhanced_collection_summary()
    
    def collect_stock_news_worker(self, stock_code, stock_name, days):
        """ğŸ“° ê°œë³„ ì¢…ëª© ë‰´ìŠ¤ ìˆ˜ì§‘ (í’ˆì§ˆ ê²€ì¦ ì›Œì»¤)"""
        try:
            # ë„¤ì´ë²„ ê¸ˆìœµ ë‰´ìŠ¤ ìˆ˜ì§‘ (í’ˆì§ˆ ê²€ì¦ í†µí•©)
            news_list = self.collect_naver_finance_news(stock_code, stock_name, days)
            
            # DB ì €ì¥
            saved_count = self.save_news_to_db(news_list)
            
            return saved_count
            
        except Exception as e:
            raise e
    
    def print_enhanced_collection_summary(self):
        """ğŸ“‹ ê°•í™”ëœ ìˆ˜ì§‘ ê²°ê³¼ ìš”ì•½ ì¶œë ¥"""
        print("\n" + "=" * 60)
        print("ğŸ“‹ ê°•í™”ëœ ë‰´ìŠ¤ ìˆ˜ì§‘ ì™„ë£Œ!")
        print("=" * 60)
        print(f"ğŸ“Š ì²˜ë¦¬ëœ ì¢…ëª©: {self.stats['success_count'] + self.stats['fail_count']:,}ê°œ")
        print(f"âœ… ì„±ê³µ: {self.stats['success_count']:,}ê°œ")
        print(f"âŒ ì‹¤íŒ¨: {self.stats['fail_count']:,}ê°œ")
        print(f"ğŸ“° ìˆ˜ì§‘ëœ ê³ í’ˆì§ˆ ë‰´ìŠ¤: {self.stats['total_collected']:,}ê±´")
        print(f"ğŸ”„ ì¤‘ë³µ ì œì™¸: {self.stats['duplicate_count']:,}ê±´")
        
        # í’ˆì§ˆ í†µê³„
        total_processed = self.stats['quality_passed'] + self.stats['quality_failed']
        if total_processed > 0:
            quality_rate = (self.stats['quality_passed'] / total_processed) * 100
            print(f"\nğŸ›¡ï¸ í’ˆì§ˆ ê²€ì¦ ê²°ê³¼:")
            print(f"   ğŸ“Š ì´ ì²˜ë¦¬: {total_processed:,}ê±´")
            print(f"   âœ… í’ˆì§ˆ í†µê³¼: {self.stats['quality_passed']:,}ê±´ ({quality_rate:.1f}%)")
            print(f"   âŒ í’ˆì§ˆ ì‹¤íŒ¨: {self.stats['quality_failed']:,}ê±´")
            print(f"   ğŸš« ìŠ¤íŒ¸ í•„í„°ë§: {self.stats['spam_filtered']:,}ê±´")
            print(f"   ğŸ“‰ ì €í’ˆì§ˆ í•„í„°ë§: {self.stats['low_quality_filtered']:,}ê±´")
        
        print(f"\nğŸ—„ï¸ ë°ì´í„° ì €ì¥: {self.db_path}")
        print("=" * 60)
    
    def query_db(self, query, params=None):
        """DB ì¿¼ë¦¬ ì‹¤í–‰"""
        try:
            with sqlite3.connect(self.db_path) as conn:
                if params:
                    return pd.read_sql_query(query, conn, params=params)
                else:
                    return pd.read_sql_query(query, conn)
        except Exception as e:
            print(f"âŒ ì¿¼ë¦¬ ì‹¤í–‰ ì‹¤íŒ¨: {e}")
            return pd.DataFrame()
    
    def get_enhanced_news_summary(self):
        """ğŸ“Š ê°•í™”ëœ ë‰´ìŠ¤ ìˆ˜ì§‘ í˜„í™© ìš”ì•½"""
        print("ğŸ“Š ê°•í™”ëœ ë‰´ìŠ¤ ìˆ˜ì§‘ í˜„í™©")
        print("=" * 50)
        
        # ì „ì²´ ë‰´ìŠ¤ ìˆ˜ (í’ˆì§ˆë³„)
        total_stats = self.query_db("""
            SELECT 
                COUNT(*) as total_news,
                COUNT(CASE WHEN is_verified = 1 THEN 1 END) as verified_news,
                AVG(quality_score) as avg_quality,
                COUNT(CASE WHEN quality_score >= 80 THEN 1 END) as high_quality_news
            FROM news_articles
        """).iloc[0]
        
        print(f"ğŸ“° ì „ì²´ ë‰´ìŠ¤: {total_stats['total_news']:,}ê±´")
        print(f"âœ… ê²€ì¦ëœ ë‰´ìŠ¤: {total_stats['verified_news']:,}ê±´")
        print(f"ğŸ“Š í‰ê·  í’ˆì§ˆ ì ìˆ˜: {total_stats['avg_quality']:.1f}ì ")
        print(f"ğŸ† ê³ í’ˆì§ˆ ë‰´ìŠ¤ (80ì  ì´ìƒ): {total_stats['high_quality_news']:,}ê±´")
        
        # ì¢…ëª©ë³„ ë‰´ìŠ¤ ìˆ˜ (í’ˆì§ˆë³„, ìƒìœ„ 10ê°œ)
        stock_news = self.query_db("""
            SELECT 
                stock_code, 
                stock_name, 
                COUNT(*) as news_count,
                AVG(quality_score) as avg_quality,
                COUNT(CASE WHEN is_verified = 1 THEN 1 END) as verified_count
            FROM news_articles
            GROUP BY stock_code, stock_name
            ORDER BY news_count DESC
            LIMIT 10
        """)
        
        if not stock_news.empty:
            print(f"\nğŸ“ˆ ì¢…ëª©ë³„ ë‰´ìŠ¤ (ìƒìœ„ 10ê°œ):")
            for _, row in stock_news.iterrows():
                print(f"   {row['stock_code']} ({row['stock_name']}): {row['news_count']}ê±´ "
                      f"(ê²€ì¦: {row['verified_count']}ê±´, í‰ê· í’ˆì§ˆ: {row['avg_quality']:.1f}ì )")
        
        # ì†ŒìŠ¤ë³„ ì‹ ë¢°ë„
        source_stats = self.query_db("""
            SELECT 
                source, 
                COUNT(*) as count,
                AVG(quality_score) as avg_quality,
                COUNT(CASE WHEN is_verified = 1 THEN 1 END) as verified_count
            FROM news_articles
            GROUP BY source
            ORDER BY avg_quality DESC, count DESC
            LIMIT 10
        """)
        
        if not source_stats.empty:
            print(f"\nğŸ“° ì†ŒìŠ¤ë³„ ì‹ ë¢°ë„ (ìƒìœ„ 10ê°œ):")
            for _, row in source_stats.iterrows():
                verification_rate = (row['verified_count'] / row['count']) * 100
                print(f"   {row['source']}: {row['count']}ê±´ "
                      f"(í‰ê· í’ˆì§ˆ: {row['avg_quality']:.1f}ì , ê²€ì¦ë¥ : {verification_rate:.1f}%)")
        
        # ì¼ë³„ ë‰´ìŠ¤ ìˆ˜ (ìµœê·¼ 7ì¼, í’ˆì§ˆë³„)
        daily_news = self.query_db("""
            SELECT 
                DATE(published_date) as date, 
                COUNT(*) as count,
                AVG(quality_score) as avg_quality,
                COUNT(CASE WHEN is_verified = 1 THEN 1 END) as verified_count
            FROM news_articles
            WHERE published_date >= DATE('now', '-7 days')
            GROUP BY DATE(published_date)
            ORDER BY date DESC
        """)
        
        if not daily_news.empty:
            print(f"\nğŸ“… ì¼ë³„ ë‰´ìŠ¤ (ìµœê·¼ 7ì¼):")
            for _, row in daily_news.iterrows():
                verification_rate = (row['verified_count'] / row['count']) * 100
                print(f"   {row['date']}: {row['count']}ê±´ "
                      f"(ê²€ì¦: {row['verified_count']}ê±´, í‰ê· í’ˆì§ˆ: {row['avg_quality']:.1f}ì )")
        
        # í•„í„°ë§ í†µê³„
        filter_stats = self.query_db("""
            SELECT 
                filter_reason,
                COUNT(*) as count
            FROM quality_filter_log
            GROUP BY filter_reason
            ORDER BY count DESC
        """)
        
        if not filter_stats.empty:
            print(f"\nğŸš« í•„í„°ë§ í†µê³„:")
            for _, row in filter_stats.iterrows():
                print(f"   {row['filter_reason']}: {row['count']}ê±´")
        
        print("=" * 50)


class EnhancedNewsSentimentAnalyzer:
    """
    ê°•í™”ëœ ë‰´ìŠ¤ ê°ì • ë¶„ì„ê¸° (í’ˆì§ˆ ê°€ì¤‘ì¹˜ ì ìš©)
    
    ìˆ˜ì§‘ëœ ê³ í’ˆì§ˆ ë‰´ìŠ¤ì— ëŒ€í•´ ê°ì • ë¶„ì„ì„ ìˆ˜í–‰í•˜ê³  
    í’ˆì§ˆ ì ìˆ˜ë¥¼ ê°€ì¤‘ì¹˜ë¡œ í•˜ì—¬ ë” ì •í™•í•œ ê°ì • ì§€ìˆ˜ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.
    """
    
    def __init__(self):
        self.data_dir = Path(DATA_DIR)
        self.db_path = self.data_dir / 'news_data.db'
        
        # ê°•í™”ëœ ê¸ˆìœµ ê°ì • ì‚¬ì „ (í•œêµ­ì–´)
        self.positive_words = {
            'ìƒìŠ¹', 'ê¸‰ë“±', 'í˜¸ì¬', 'ì„±ì¥', 'ì¦ê°€', 'í™•ëŒ€', 'ê°œì„ ', 'íšŒë³µ',
            'ëŒíŒŒ', 'ìƒí–¥', 'ê¸ì •', 'í˜¸í™©', 'í™œì„±', 'ë¶€ì–‘', 'íˆ¬ì', 'í™•ì¥',
            'ìˆ˜ìµ', 'ì´ìµ', 'ì‹¤ì ', 'ê°œë°œ', 'í˜ì‹ ', 'ì „ë§', 'ê¸°ëŒ€', 'ì¶”ì²œ',
            'ë§¤ìˆ˜', 'ê°•ì„¸', 'ë°˜ë“±', 'ì„ ë°©', 'ì–‘í˜¸', 'ìš°ìˆ˜', 'íƒ„íƒ„', 'ê²¬ì¡°',
            'í‘ì', 'ì¦ìµ', 'í˜¸ì¡°', 'ê°œì„ ', 'ì‹ ê³ ê°€', 'ìµœê³ ', 'ì„±ê³µ', 'ìš°ëŸ‰'
        }
        
        self.negative_words = {
            'í•˜ë½', 'ê¸‰ë½', 'ì•…ì¬', 'ê°ì†Œ', 'ì¶•ì†Œ', 'ì•…í™”', 'ì¹¨ì²´', 'ìœ„ê¸°',
            'ì†ì‹¤', 'ì ì', 'ë¶€ì§„', 'ë‘”í™”', 'ê²½ê³ ', 'ìš°ë ¤', 'ë¶ˆì•ˆ', 'ë¦¬ìŠ¤í¬',
            'íƒ€ê²©', 'ì¶©ê²©', 'ì••ë°•', 'ì œì¬', 'ê·œì œ', 'íŒŒì‚°', 'êµ¬ì¡°ì¡°ì •',
            'ë§¤ë„', 'ì•½ì„¸', 'ì¡°ì •', 'ë¶€ë‹´', 'ì·¨ì•½', 'ì•…ìˆœí™˜', 'ì¹¨ì²´', 'ì €ì¡°',
            'ì ì', 'ê°ìµ', 'ë¶€ì‹¤', 'ìœ„í—˜', 'ì‹ ì €ê°€', 'ìµœì €', 'ì‹¤íŒ¨', 'ë¶ˆëŸ‰'
        }
        
        print("âœ… ê°•í™”ëœ ë‰´ìŠ¤ ê°ì • ë¶„ì„ê¸° ì´ˆê¸°í™” ì™„ë£Œ")
    
    def calculate_weighted_sentiment_score(self, text, quality_score=100):
        """
        ğŸ“Š í’ˆì§ˆ ê°€ì¤‘ì¹˜ë¥¼ ì ìš©í•œ ê°ì • ì ìˆ˜ ê³„ì‚°
        
        Args:
            text (str): ë¶„ì„í•  í…ìŠ¤íŠ¸
            quality_score (int): ë‰´ìŠ¤ í’ˆì§ˆ ì ìˆ˜ (0-100)
            
        Returns:
            tuple: (sentiment_score, sentiment_label, weighted_score)
        """
        if not text:
            return 0.0, 'neutral', 0.0
        
        text = text.lower()
        
        # ê¸ì •/ë¶€ì • ë‹¨ì–´ ê°œìˆ˜ ê³„ì‚°
        positive_count = sum(1 for word in self.positive_words if word in text)
        negative_count = sum(1 for word in self.negative_words if word in text)
        
        # ì´ ê°ì • ë‹¨ì–´ ìˆ˜
        total_sentiment_words = positive_count + negative_count
        
        if total_sentiment_words == 0:
            return 0.0, 'neutral', 0.0
        
        # ê¸°ë³¸ ê°ì • ì ìˆ˜ ê³„ì‚° (-1.0 ~ 1.0)
        sentiment_score = (positive_count - negative_count) / total_sentiment_words
        
        # í’ˆì§ˆ ê°€ì¤‘ì¹˜ ì ìš© (í’ˆì§ˆì´ ë†’ì„ìˆ˜ë¡ ë” ì‹ ë¢°)
        quality_weight = quality_score / 100.0
        weighted_score = sentiment_score * quality_weight
        
        # ë¼ë²¨ ê²°ì • (ê°€ì¤‘ì¹˜ ì ìš©ëœ ì ìˆ˜ ê¸°ì¤€)
        if weighted_score > 0.15:
            sentiment_label = 'positive'
        elif weighted_score < -0.15:
            sentiment_label = 'negative'
        else:
            sentiment_label = 'neutral'
        
        return sentiment_score, sentiment_label, weighted_score
    
    def analyze_all_news_sentiment(self):
        """ğŸ” ëª¨ë“  ê³ í’ˆì§ˆ ë‰´ìŠ¤ì— ëŒ€í•´ ê°ì • ë¶„ì„ ìˆ˜í–‰"""
        print("ğŸ” ê°•í™”ëœ ë‰´ìŠ¤ ê°ì • ë¶„ì„ ì‹œì‘!")
        
        try:
            with sqlite3.connect(self.db_path) as conn:
                # í’ˆì§ˆ ê²€ì¦ëœ ë‰´ìŠ¤ ì¤‘ ê°ì • ë¶„ì„ì´ ì•ˆëœ ë‰´ìŠ¤ë“¤ ì¡°íšŒ
                query = """
                    SELECT id, title, content, summary, stock_code, quality_score
                    FROM news_articles
                    WHERE sentiment_score IS NULL AND is_verified = 1
                    ORDER BY quality_score DESC, id
                """
                news_to_analyze = pd.read_sql_query(query, conn)
                
                if news_to_analyze.empty:
                    print("âœ… ëª¨ë“  ê³ í’ˆì§ˆ ë‰´ìŠ¤ê°€ ì´ë¯¸ ê°ì • ë¶„ì„ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")
                    return
                
                print(f"ğŸ“Š ê°ì • ë¶„ì„ ëŒ€ìƒ: {len(news_to_analyze)}ê±´ (ê²€ì¦ëœ ë‰´ìŠ¤ë§Œ)")
                
                cursor = conn.cursor()
                analyzed_count = 0
                
                # ì§„í–‰ë¥  í‘œì‹œ
                progress_bar = tqdm(news_to_analyze.iterrows(), 
                                  total=len(news_to_analyze),
                                  desc="ğŸ” í’ˆì§ˆê°€ì¤‘ ê°ì •ë¶„ì„",
                                  unit="ë‰´ìŠ¤")
                
                for _, row in progress_bar:
                    try:
                        # ì œëª©ê³¼ ìš”ì•½ì„ í•©ì³ì„œ ë¶„ì„
                        text_to_analyze = f"{row['title']} {row['summary']}"
                        quality_score = row['quality_score']
                        
                        # í’ˆì§ˆ ê°€ì¤‘ì¹˜ ì ìš© ê°ì • ë¶„ì„ ìˆ˜í–‰
                        sentiment_score, sentiment_label, weighted_score = self.calculate_weighted_sentiment_score(
                            text_to_analyze, quality_score
                        )
                        
                        # DB ì—…ë°ì´íŠ¸
                        cursor.execute('''
                            UPDATE news_articles 
                            SET sentiment_score = ?, sentiment_label = ?
                            WHERE id = ?
                        ''', (weighted_score, sentiment_label, row['id']))
                        
                        analyzed_count += 1
                        
                        progress_bar.set_postfix({
                            'Analyzed': analyzed_count,
                            'Current': sentiment_label,
                            'Quality': f"{quality_score}ì "
                        })
                        
                    except Exception as e:
                        print(f"\nâš ï¸ ë‰´ìŠ¤ ID {row['id']} ê°ì • ë¶„ì„ ì‹¤íŒ¨: {e}")
                        continue
                
                conn.commit()
                
                print(f"\nâœ… ê°•í™”ëœ ê°ì • ë¶„ì„ ì™„ë£Œ: {analyzed_count}ê±´")
                
                # ê°ì • ë¶„ì„ ê²°ê³¼ ìš”ì•½
                self.summarize_enhanced_sentiment_results()
                
        except Exception as e:
            print(f"âŒ ê°•í™”ëœ ê°ì • ë¶„ì„ ì‹¤íŒ¨: {e}")
    
    def calculate_enhanced_daily_sentiment_index(self):
        """ğŸ“ˆ í’ˆì§ˆ ê°€ì¤‘ì¹˜ ì ìš©ëœ ì¼ë³„ ì¢…ëª©ë³„ ê°ì • ì§€ìˆ˜ ê³„ì‚°"""
        print("ğŸ“ˆ ê°•í™”ëœ ì¼ë³„ ê°ì • ì§€ìˆ˜ ê³„ì‚° ì¤‘...")
        
        try:
            with sqlite3.connect(self.db_path) as conn:
                cursor = conn.cursor()
                
                # ì¢…ëª©ë³„, ì¼ë³„ ê°ì • ë¶„ì„ ê²°ê³¼ ì§‘ê³„ (í’ˆì§ˆ ê°€ì¤‘ì¹˜ ì ìš©)
                query = """
                    SELECT 
                        stock_code,
                        DATE(published_date) as date,
                        SUM(CASE WHEN sentiment_label = 'positive' THEN 1 ELSE 0 END) as positive_count,
                        SUM(CASE WHEN sentiment_label = 'negative' THEN 1 ELSE 0 END) as negative_count,
                        SUM(CASE WHEN sentiment_label = 'neutral' THEN 1 ELSE 0 END) as neutral_count,
                        COUNT(*) as total_count,
                        AVG(sentiment_score) as avg_sentiment_score,
                        AVG(quality_score) as avg_quality_score,
                        COUNT(CASE WHEN is_verified = 1 THEN 1 END) * 1.0 / COUNT(*) as verified_ratio
                    FROM news_articles
                    WHERE sentiment_score IS NOT NULL
                    AND published_date >= DATE('now', '-30 days')
                    GROUP BY stock_code, DATE(published_date)
                    ORDER BY stock_code, date
                """
                
                results = pd.read_sql_query(query, conn)
                
                if results.empty:
                    print("âŒ ê°ì • ë¶„ì„ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
                    return
                
                # ê°•í™”ëœ ê°ì • ì§€ìˆ˜ ê³„ì‚° ë° ì €ì¥
                for _, row in results.iterrows():
                    # í’ˆì§ˆ ê°€ì¤‘ì¹˜ ì ìš©ëœ ê°ì • ì§€ìˆ˜ ê³„ì‚° (0~100, 50ì´ ì¤‘ë¦½)
                    if row['total_count'] > 0:
                        positive_ratio = row['positive_count'] / row['total_count']
                        negative_ratio = row['negative_count'] / row['total_count']
                        
                        # ê¸°ë³¸ ê°ì • ì§€ìˆ˜
                        base_sentiment_index = 50 + (positive_ratio - negative_ratio) * 50
                        
                        # í’ˆì§ˆ ê°€ì¤‘ì¹˜ ì ìš©
                        quality_weight = row['avg_quality_score'] / 100.0
                        enhanced_sentiment_index = 50 + (base_sentiment_index - 50) * quality_weight
                        
                        # ê²€ì¦ëœ ë‰´ìŠ¤ ë¹„ìœ¨ë„ ë°˜ì˜
                        verified_weight = row['verified_ratio']
                        final_sentiment_index = 50 + (enhanced_sentiment_index - 50) * verified_weight
                    else:
                        final_sentiment_index = 50
                    
                    # DBì— ì €ì¥
                    cursor.execute('''
                        INSERT OR REPLACE INTO sentiment_analysis
                        (stock_code, date, positive_count, negative_count, neutral_count,
                         total_count, sentiment_score, sentiment_index, average_quality, 
                         verified_news_ratio, created_date)
                        VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                    ''', (
                        row['stock_code'],
                        row['date'],
                        row['positive_count'],
                        row['negative_count'],
                        row['neutral_count'],
                        row['total_count'],
                        row['avg_sentiment_score'],
                        final_sentiment_index,
                        row['avg_quality_score'],
                        row['verified_ratio'],
                        datetime.now().isoformat()
                    ))
                
                conn.commit()
                print(f"âœ… {len(results)}ê±´ì˜ ê°•í™”ëœ ì¼ë³„ ê°ì • ì§€ìˆ˜ ê³„ì‚° ì™„ë£Œ")
                
        except Exception as e:
            print(f"âŒ ê°•í™”ëœ ê°ì • ì§€ìˆ˜ ê³„ì‚° ì‹¤íŒ¨: {e}")
    
    def summarize_enhanced_sentiment_results(self):
        """ğŸ“Š ê°•í™”ëœ ê°ì • ë¶„ì„ ê²°ê³¼ ìš”ì•½"""
        try:
            with sqlite3.connect(self.db_path) as conn:
                # ì „ì²´ ê°ì • ë¶„í¬ (ê²€ì¦ëœ ë‰´ìŠ¤ë§Œ)
                sentiment_dist = pd.read_sql_query("""
                    SELECT 
                        sentiment_label,
                        COUNT(*) as count,
                        AVG(quality_score) as avg_quality,
                        COUNT(*) * 100.0 / (SELECT COUNT(*) FROM news_articles WHERE sentiment_label IS NOT NULL AND is_verified = 1) as percentage
                    FROM news_articles
                    WHERE sentiment_label IS NOT NULL AND is_verified = 1
                    GROUP BY sentiment_label
                """, conn)
                
                print("\nğŸ“Š ê°•í™”ëœ ê°ì • ë¶„í¬ (ê²€ì¦ëœ ë‰´ìŠ¤ë§Œ):")
                for _, row in sentiment_dist.iterrows():
                    print(f"   {row['sentiment_label']}: {row['count']:,}ê±´ ({row['percentage']:.1f}%, í‰ê· í’ˆì§ˆ: {row['avg_quality']:.1f}ì )")
                
                # ì¢…ëª©ë³„ ê°ì • ì ìˆ˜ (ìƒìœ„/í•˜ìœ„ 5ê°œ, í’ˆì§ˆ ê°€ì¤‘ì¹˜ ì ìš©)
                stock_sentiment = pd.read_sql_query("""
                    SELECT 
                        stock_code,
                        stock_name,
                        AVG(sentiment_score) as avg_sentiment,
                        AVG(quality_score) as avg_quality,
                        COUNT(*) as news_count,
                        COUNT(CASE WHEN is_verified = 1 THEN 1 END) as verified_count
                    FROM news_articles
                    WHERE sentiment_score IS NOT NULL AND is_verified = 1
                    GROUP BY stock_code, stock_name
                    HAVING COUNT(*) >= 5
                    ORDER BY avg_sentiment DESC
                """, conn)
                
                if not stock_sentiment.empty:
                    print(f"\nğŸ“ˆ ì¢…ëª©ë³„ í‰ê·  ê°ì • ì ìˆ˜ (í’ˆì§ˆ ê°€ì¤‘ì¹˜ ì ìš©):")
                    print("   ğŸ” ìƒìœ„ 5ê°œ:")
                    for _, row in stock_sentiment.head().iterrows():
                        print(f"      {row['stock_code']} ({row['stock_name']}): {row['avg_sentiment']:.3f} "
                              f"(í’ˆì§ˆ: {row['avg_quality']:.1f}ì , ê²€ì¦: {row['verified_count']}/{row['news_count']}ê±´)")
                    
                    print("   ğŸ“‰ í•˜ìœ„ 5ê°œ:")
                    for _, row in stock_sentiment.tail().iterrows():
                        print(f"      {row['stock_code']} ({row['stock_name']}): {row['avg_sentiment']:.3f} "
                              f"(í’ˆì§ˆ: {row['avg_quality']:.1f}ì , ê²€ì¦: {row['verified_count']}/{row['news_count']}ê±´)")
                
        except Exception as e:
            print(f"âš ï¸ ê°•í™”ëœ ê°ì • ë¶„ì„ ìš”ì•½ ì‹¤íŒ¨: {e}")
    
    def get_enhanced_stock_sentiment_trend(self, stock_code, days=30):
        """ğŸ“ˆ íŠ¹ì • ì¢…ëª©ì˜ ê°•í™”ëœ ê°ì • ì¶”ì´ ì¡°íšŒ"""
        try:
            with sqlite3.connect(self.db_path) as conn:
                query = """
                    SELECT 
                        date, 
                        sentiment_index, 
                        total_count,
                        average_quality,
                        verified_news_ratio
                    FROM sentiment_analysis
                    WHERE stock_code = ?
                    AND date >= DATE('now', '-{} days')
                    ORDER BY date
                """.format(days)
                
                return pd.read_sql_query(query, conn, params=(stock_code,))
                
        except Exception as e:
            print(f"âŒ ê°•í™”ëœ ê°ì • ì¶”ì´ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return pd.DataFrame()


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    
    print("ğŸš€ Finance Data Vibe - ê°•í™”ëœ ë‰´ìŠ¤ ìˆ˜ì§‘ ë° ê°ì • ë¶„ì„ ì‹œìŠ¤í…œ")
    print("=" * 70)
    print("ğŸ†• í’ˆì§ˆ ê²€ì¦ ì‹œìŠ¤í…œ í†µí•© - ìŠ¤íŒ¸/ì¤‘ë³µ/ì˜¤ë¥˜ ìë™ í•„í„°ë§")
    print("ğŸ†• ì‹ ë¢°ë„ ì ìˆ˜ ê¸°ë°˜ í’ˆì§ˆ ê´€ë¦¬")
    print("ğŸ†• í•œê¸€ ì¸ì½”ë”© ë¬¸ì œ ì™„ì „ í•´ê²°")
    print("ğŸ†• í’ˆì§ˆ ê°€ì¤‘ì¹˜ ì ìš© ê°ì • ë¶„ì„")
    
    while True:
        print("\nğŸ“° ì›í•˜ëŠ” ê¸°ëŠ¥ì„ ì„ íƒí•˜ì„¸ìš”:")
        print("1. ì „ì²´ ì¢…ëª© ê³ í’ˆì§ˆ ë‰´ìŠ¤ ìˆ˜ì§‘")
        print("2. íŠ¹ì • ì¢…ëª© ë‰´ìŠ¤ ìˆ˜ì§‘")
        print("3. ê°•í™”ëœ ë‰´ìŠ¤ ê°ì • ë¶„ì„ ìˆ˜í–‰")
        print("4. í’ˆì§ˆ ê°€ì¤‘ì¹˜ ì ìš© ì¼ë³„ ê°ì • ì§€ìˆ˜ ê³„ì‚°")
        print("5. ê°•í™”ëœ ë‰´ìŠ¤ ìˆ˜ì§‘ í˜„í™© í™•ì¸")
        print("6. í’ˆì§ˆë³„ ê°ì • ë¶„ì„ ê²°ê³¼ í™•ì¸")
        print("7. í’ˆì§ˆ í•„í„°ë§ í†µê³„ í™•ì¸")
        print("0. ì¢…ë£Œ")
        
        choice = input("\nì„ íƒí•˜ì„¸ìš” (0-7): ").strip()
        
        if choice == '0':
            print("ğŸ‘‹ í”„ë¡œê·¸ë¨ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
            break
        
        elif choice == '1':
            # ì „ì²´ ì¢…ëª© ê³ í’ˆì§ˆ ë‰´ìŠ¤ ìˆ˜ì§‘
            collector = EnhancedNewsCollector()
            
            days = int(input("ìˆ˜ì§‘í•  ì¼ìˆ˜ë¥¼ ì…ë ¥í•˜ì„¸ìš” (ê¸°ë³¸ê°’: 7): ").strip() or "7")
            max_stocks = input("ìµœëŒ€ ì¢…ëª© ìˆ˜ (ì „ì²´: Enter): ").strip()
            max_stocks = int(max_stocks) if max_stocks else None
            
            print(f"\nğŸ›¡ï¸ í’ˆì§ˆ ê²€ì¦ í™œì„±í™”: 70ì  ì´ìƒë§Œ ì €ì¥")
            print(f"ğŸš« ìë™ í•„í„°ë§: ìŠ¤íŒ¸/ì¤‘ë³µ/ì˜¤ë¥˜ ë‰´ìŠ¤ ì œê±°")
            
            collector.collect_all_stock_news(days=days, max_stocks=max_stocks)
        
        elif choice == '2':
            # íŠ¹ì • ì¢…ëª© ë‰´ìŠ¤ ìˆ˜ì§‘
            collector = EnhancedNewsCollector()
            
            stock_code = input("ì¢…ëª©ì½”ë“œë¥¼ ì…ë ¥í•˜ì„¸ìš” (ì˜ˆ: 005930): ").strip()
            stock_name = input("ì¢…ëª©ëª…ì„ ì…ë ¥í•˜ì„¸ìš” (ì˜ˆ: ì‚¼ì„±ì „ì): ").strip()
            days = int(input("ìˆ˜ì§‘í•  ì¼ìˆ˜ë¥¼ ì…ë ¥í•˜ì„¸ìš” (ê¸°ë³¸ê°’: 7): ").strip() or "7")
            
            if stock_code and stock_name:
                news_list = collector.collect_naver_finance_news(stock_code, stock_name, days)
                saved_count = collector.save_news_to_db(news_list)
                
                if news_list:
                    avg_quality = sum(news.get('quality_score', 0) for news in news_list) / len(news_list)
                    print(f"âœ… {saved_count}ê±´ì˜ ê³ í’ˆì§ˆ ë‰´ìŠ¤ë¥¼ ìˆ˜ì§‘í–ˆìŠµë‹ˆë‹¤. (í‰ê·  í’ˆì§ˆ: {avg_quality:.1f}ì )")
                else:
                    print("âŒ í’ˆì§ˆ ê¸°ì¤€ì„ í†µê³¼í•œ ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤.")
            else:
                print("âŒ ì¢…ëª©ì½”ë“œì™€ ì¢…ëª©ëª…ì„ ëª¨ë‘ ì…ë ¥í•´ì£¼ì„¸ìš”.")
        
        elif choice == '3':
            # ê°•í™”ëœ ë‰´ìŠ¤ ê°ì • ë¶„ì„
            analyzer = EnhancedNewsSentimentAnalyzer()
            analyzer.analyze_all_news_sentiment()
        
        elif choice == '4':
            # í’ˆì§ˆ ê°€ì¤‘ì¹˜ ì ìš© ì¼ë³„ ê°ì • ì§€ìˆ˜ ê³„ì‚°
            analyzer = EnhancedNewsSentimentAnalyzer()
            analyzer.calculate_enhanced_daily_sentiment_index()
        
        elif choice == '5':
            # ê°•í™”ëœ ë‰´ìŠ¤ ìˆ˜ì§‘ í˜„í™©
            collector = EnhancedNewsCollector()
            collector.get_enhanced_news_summary()
        
        elif choice == '6':
            # í’ˆì§ˆë³„ ê°ì • ë¶„ì„ ê²°ê³¼
            analyzer = EnhancedNewsSentimentAnalyzer()
            analyzer.summarize_enhanced_sentiment_results()
        
        elif choice == '7':
            # í’ˆì§ˆ í•„í„°ë§ í†µê³„
            collector = EnhancedNewsCollector()
            
            # í•„í„°ë§ í†µê³„ ì¡°íšŒ
            filter_stats = collector.query_db("""
                SELECT 
                    filter_reason,
                    COUNT(*) as count,
                    AVG(quality_score) as avg_quality_score
                FROM quality_filter_log
                WHERE DATE(filtered_date) >= DATE('now', '-7 days')
                GROUP BY filter_reason
                ORDER BY count DESC
            """)
            
            if not filter_stats.empty:
                print("\nğŸš« ìµœê·¼ 7ì¼ í’ˆì§ˆ í•„í„°ë§ í†µê³„:")
                print("=" * 50)
                for _, row in filter_stats.iterrows():
                    print(f"   {row['filter_reason']}: {row['count']:,}ê±´ (í‰ê·  ì ìˆ˜: {row['avg_quality_score']:.1f}ì )")
                
                # ì „ì²´ í†µê³„
                total_filtered = filter_stats['count'].sum()
                total_saved = collector.query_db("""
                    SELECT COUNT(*) as count 
                    FROM news_articles 
                    WHERE DATE(collected_date) >= DATE('now', '-7 days')
                """).iloc[0]['count']
                
                filter_rate = (total_filtered / max(total_filtered + total_saved, 1)) * 100
                print(f"\nğŸ“Š í•„í„°ë§ íš¨ê³¼:")
                print(f"   ğŸš« í•„í„°ë§ëœ ë‰´ìŠ¤: {total_filtered:,}ê±´")
                print(f"   âœ… ì €ì¥ëœ ë‰´ìŠ¤: {total_saved:,}ê±´")
                print(f"   ğŸ“ˆ í•„í„°ë§ë¥ : {filter_rate:.1f}%")
            else:
                print("âŒ ìµœê·¼ í•„í„°ë§ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        
        else:
            print("âŒ ì˜¬ë°”ë¥¸ ë²ˆí˜¸ë¥¼ ì„ íƒí•´ì£¼ì„¸ìš”.")


if __name__ == "__main__":
    main()